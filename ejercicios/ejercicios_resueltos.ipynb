{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¿Cúal es el tercer mayor valor de esta lista?¿en que orden está el número 950?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi_lista = [ 2, 517, 7, 11, 528, 531, 19, 536, 31, 40, 552, 50, 565, 55, 568, 571, 572, 583, 584, 588, 78, 591, 80, 82, 596, 91, 605, 611, 99, 101, 102, 105, 110, 622, 114, 118, 635, 636, 129, 643, 131, 645, 136, 655, 661, 664, 156, 669, 672, 163, 167, 168, 170, 683, 171, 176, 688, 179, 697, 187, 700, 191, 703, 197, 709, 201, 713, 716, 204, 205, 719, 720, 209, 726, 728, 729, 219, 228, 741, 231, 745, 237, 238, 240, 759, 248, 251, 254, 771, 773, 775, 778, 266, 787, 276, 789, 279, 793, 285, 798, 799, 800, 803, 295, 818, 307, 306, 821, 822, 823, 310, 315, 323, 835, 839, 843, 331, 334, 854, 342, 343, 347, 860, 363, 878, 367, 882, 883, 373, 886, 887, 376, 889, 379, 387, 900, 903, 907, 399, 400, 913, 911, 915, 409, 925, 929, 421, 933, 939, 437, 950, 951, 949, 953, 446, 959, 962, 451, 452, 453, 969, 458, 970, 972, 463, 464, 978, 982, 986, 475, 988, 474, 482, 485, 486, 509, 510]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "982\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "print(sorted(mi_lista)[-3])\n",
    "print(mi_lista.index(950))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tercer_mayor = sorted(mi_lista)[-3]\n",
    "orden_950 = mi_lista.index(950)\n",
    "print(tercer_mayor)\n",
    "print(orden_950)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la lista numeros generar otra lista con los cuadrados de cada numero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuadrados = []\n",
    "numeros = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
    "for n in numeros:\n",
    "    cuadrados.append(n ** 2 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25, 36, 49, 64, 81, 100]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuadrados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Intersección de listas\n",
    "Se tienen dos listas ( l1 y l2 ), escribir una función que devuelva la intersección de ambas listas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1,3,5,6,7,10,12,15,22]\n",
    "l2 = [3,4,6,7,8,9,22,10,11,13,15]\n",
    "def intersection( a, b) :\n",
    "    intersec = []\n",
    "    for item in l1:\n",
    "        if item in l2:\n",
    "            intersec.append(item)\n",
    "    return intersec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 7, 10, 15, 22]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intersection(l1,l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Listas reversas\n",
    "Escribir una función que dadas dos listas, devuelva un valor de verdad que dice si una lista es inversa a la otra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "l1 = [1,2,3]\n",
    "l2 = [3,2,1]\n",
    "\n",
    "def isReverse(a, b) :\n",
    "    es_reversa = b == list(reversed(a))\n",
    "    return es_reversa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "reversa = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isReverse(a, b) :\n",
    "    a_reversa = []\n",
    "    for item in a:\n",
    "        a_reversa.insert(0, item)\n",
    "    print(a)\n",
    "    print(b)\n",
    "    return a_reversa == b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lista incluida en otra lista\n",
    "Escribir una función que tome dos listas l1 y l2, y que devuelve True si todos los elementso de l1 están en l2, sino quq devuelva False.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_all(l1, l2):\n",
    "    is_in = 0\n",
    "    for elemento in l1:\n",
    "        if elemento in l2:\n",
    "            is_in +=1\n",
    "    return is_in == len(l1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contains_all([2,3,4,9,'a'], [2,3,4,'a',9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recorrer diccionario con listas\n",
    "Construir un diccionario cuya clave sea un nombre de persona y los valores sean listas de nombres de ciudades que esa persona visitó. Crear una función que dado un diccionario como el que se pide antes y un nombre de ciudad, devuelva la lista de las personas que visitaron esa ciudad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "datos = {\n",
    "    'juan' : ['Berlin','Barcelona','Buenos Aires','Baradero','Boston'],\n",
    "    'pedro' : ['Asunción','Atenas','Alicante','Almería','Berlin'],\n",
    "    'pepe' : ['Asunción','Buenos Aires','Berlin']\n",
    "}\n",
    "\n",
    "def visitantes( dic, ciudad ):\n",
    "    visitantes = []\n",
    "    for nombre in dic:\n",
    "        if ciudad in dic[nombre]:\n",
    "            visitantes.append(nombre)\n",
    "    return visitantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pedro', 'pepe']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visitantes(datos, 'Asunción')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Invertir diccionario\n",
    "\n",
    "Defina una función que dado un diccionario permita crear otro pero que este tenga invertidas las claves y los valores. Asuma que no hay valores repetidos y estos pueden ser usados como clave.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': 1, 'b': 2, 'z': 3}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2, 'z': 3}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(reverseDict({1:'a', 2:'b', 3:'z'}))\n",
    "reverseDict2({1:'a', 2:'b', 3:'z'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverseDict(dic):\n",
    "    reverse_dict = {}\n",
    "    for clave, valor in dic.items():\n",
    "        reverse_dict[valor] = clave\n",
    "    return reverse_dict\n",
    "\n",
    "\n",
    "def reverseDict2(dic):\n",
    "    reverse_dict = {}\n",
    "    for clave in dic:\n",
    "        reverse_dict[ dic[clave] ] = clave\n",
    "    return reverse_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina una función que tenga como parametros un a partir de un diccionario de secuencias proteicas y una secuencia de interes, que imprima en pantalla los nombres las proteinas que posean la secuencia de interes, y que devuelva la cantidad de dichas proteinas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "secuencias = {\n",
    "    'bos taurus': \"MEESQAELNVEPPLSQETFSDLWNLLPENNLLSSELSAPVDDLLPYTDVATWLDECPNEA\",\n",
    "    'homo sapiens': \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGP\",\n",
    "    'mus musculus': 'MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEA',\n",
    "    'rattus norvegicus': 'MEDSQSDMSIELPLSQETFSCLWKLLPPDDILPTTATGSPNSMEDLFLPQDVAELLEGPE',\n",
    "    'macaca mulatta': 'MEEPQSDPSIEPPLSQETFSDLWKLLPENNVLSPLPSQAVDDLMLSPDDLAQWLTEDPGP',\n",
    "    'sus scrofa': 'MEESQSELGVEPPLSQETFSDLWKLLPENNLLSSELSLAAVNDLLLSPVTNWLDENPDDA',\n",
    "    \"cricetulus griseus\":'MEEPQSDLSIELPLSQETFSDLWKLLPPNNVLSTLPSSDSIEELFLSENVTGWLEDSGGA',\n",
    "    'canis lupus': \"MEESQSELNIDPPLSQETFSELWNLLPENNVLSSELCPAVDELLLPESVVNWLDEDSDDA\",\n",
    "    'felix catus': 'MQEPPLELTIEPPLSQETFSELWNLLPENNVLSSELSSAMNELPLSEDVANWLDEAPDDA'\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion(secuencias, target):\n",
    "    hits = []\n",
    "    for proteina in secuencias:\n",
    "        if target in secuencias[proteina]:\n",
    "            hits.append(proteina)\n",
    "            print(proteina)\n",
    "    return len(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ayuda, la salida esperada para dicha funcion con una secuencia de interes \"LSSEL\" sería:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos taurus\n",
      "sus scrofa\n",
      "canis lupus\n",
      "felix catus\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "hits = funcion(secuencias, 'LSSEL')\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modificar la funcion anterior para que: \n",
    "+ se puede utilizar solo con el argumento de la secuencia de interes.\n",
    "+ Si encuentra proteinas, imprima en pantalla \"Se ha encontrado la secuencia ... en los siguientes organismos:\" antes de enumerar los organismos (en los ... va la secuencia de interes)\n",
    "\n",
    "+ Si no encuentra proteinas, imprima en pantalla \"No se ha encontrado la secuencia de interes en las proteinas\"\n",
    "El resultado debería ser:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos taurus\n",
      "sus scrofa\n",
      "canis lupus\n",
      "felix catus\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "hits = funcion2('LSSEL')\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def funcion2(target, secuencias = 'default'):\n",
    "    \n",
    "    if secuencias == 'default':\n",
    "        secuencias = {\n",
    "        'bos taurus': \"MEESQAELNVEPPLSQETFSDLWNLLPENNLLSSELSAPVDDLLPYTDVATWLDECPNEA\",\n",
    "        'homo sapiens': \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGP\",\n",
    "        'mus musculus': 'MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEA',\n",
    "        'rattus norvegicus': 'MEDSQSDMSIELPLSQETFSCLWKLLPPDDILPTTATGSPNSMEDLFLPQDVAELLEGPE',\n",
    "        'macaca mulatta': 'MEEPQSDPSIEPPLSQETFSDLWKLLPENNVLSPLPSQAVDDLMLSPDDLAQWLTEDPGP',\n",
    "        'sus scrofa': 'MEESQSELGVEPPLSQETFSDLWKLLPENNLLSSELSLAAVNDLLLSPVTNWLDENPDDA',\n",
    "        \"cricetulus griseus\":'MEEPQSDLSIELPLSQETFSDLWKLLPPNNVLSTLPSSDSIEELFLSENVTGWLEDSGGA',\n",
    "        'canis lupus': \"MEESQSELNIDPPLSQETFSELWNLLPENNVLSSELCPAVDELLLPESVVNWLDEDSDDA\",\n",
    "        'felix catus': 'MQEPPLELTIEPPLSQETFSELWNLLPENNVLSSELSSAMNELPLSEDVANWLDEAPDDA'\n",
    "        }\n",
    "    \n",
    "    \n",
    "    \n",
    "    hits = []\n",
    "    for proteina in secuencias:\n",
    "        if target in secuencias[proteina]:\n",
    "            hits.append(proteina)\n",
    "    if len(hits) > 0:\n",
    "        print(\"Se ha encontrado la secuencia\",target, \"en los siguientes organismos:\")\n",
    "        for organismo in hits:\n",
    "            print(organismo)\n",
    "    else:\n",
    "        print(\"No se ha encontrado la secuencia de interes en las proteinas\" )\n",
    "    return len(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se ha encontrado la secuencia LSSEL en los siguientes organismos:\n",
      "bos taurus\n",
      "sus scrofa\n",
      "canis lupus\n",
      "felix catus\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "hits = funcion2('LSSEL')\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No se ha encontrado la secuencia de interes en las proteinas\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "hits = funcion2('LSSELXXXXXX')\n",
    "print(hits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defina una función que dado dos números, devuelva el mayor. En caso de ser iguales devuela el string \"Los dos son iguales\" y en caso de ser ambos 0 devuelva \"Los dos son 0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mayor(a,b):\n",
    "    if a > b:\n",
    "        return a\n",
    "    if b > a:\n",
    "        return b\n",
    "    if a == 0 and b == 0:\n",
    "        return 'Los dos son 0'\n",
    "    if a == b:\n",
    "        return 'Los dos son iguales'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Los dos son 0'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mayor(0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(-10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 9, 8, 7, 6, 5, 4, 3, 2, 1]"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(10, 0, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"a\".upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%hisotry` not found.\n"
     ]
    }
   ],
   "source": [
    "%hisotry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      " 4/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      " 4/3:\n",
      "import pandas as pd\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      " 4/4: db = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo_loc'])\n",
      " 4/5: db['gene_name'] = db.uniprot.map(tablas['gene_names_dict'])\n",
      " 4/6: db.mlo_loc = db.mlo_loc.map(tablas['translate'])\n",
      " 4/7:\n",
      "basesdatos = tablas['human_box1'].drop_duplicates([\"uniprot\",'web'])\n",
      "basesdatos['gene_name'] = basesdatos.uniprot.map(tablas['gene_names_dict'])\n",
      "ndbs = basesdatos.gene_name.value_counts().to_dict()\n",
      " 4/8:\n",
      "pivot = db.pivot_table(index='gene_name',columns='mlo_loc',aggfunc='size')\n",
      "pivot['suma'] = pivot.sum(1)\n",
      " 4/9: #pivot.index = pivot.index.map(lambda x: x + \" - \" + str(counts_grande[x]))\n",
      "4/10:\n",
      "# calcular numero exacto de MLOS\n",
      "\n",
      "db_grande = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo'])\n",
      "\n",
      "db_grande.mlo_loc = db_grande.mlo_loc.map(tablas['translate'])\n",
      "\n",
      "db_grande['gene_name'] = db_grande.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "counts_grande = db_grande.gene_name.value_counts().to_dict()\n",
      "\n",
      "db_grande.gene_name.value_counts().value_counts()\n",
      "4/11: pivot['ndbs'] = pivot.index.map(ndbs)\n",
      "4/12: columnas = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body', 'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD','Other', 'suma','ndbs']\n",
      "4/13: chico = pivot.query('suma > 1')#[mlos]\n",
      "4/14: chico = pivot.query('suma >= 2')\n",
      "4/15: chico = chico.sort_values('suma',ascending=False)[columnas]\n",
      "4/16:\n",
      "chico4 = chico.query('suma > 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico3 = chico.query('suma == 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico2 = chico.query('suma == 2').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "4/17:\n",
      "ax = sns.heatmap(chico3.drop(columns=['ndbs','suma']),cmap='Blues')\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "4/18: chico3omas = chico.query('suma >= 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "4/19:\n",
      "plt.subplots(figsize=(5,4))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('hetamap_4.svg')\n",
      "4/20:\n",
      "plt.subplots(figsize=(10,4))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('hetamap_4.svg')\n",
      "4/21:\n",
      "plt.subplots(figsize=(5,15))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('hetamap_4.svg')\n",
      "4/22:\n",
      "plt.subplots(figsize=(5,14))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('hetamap_4.svg')\n",
      "4/23:\n",
      "plt.subplots(figsize=(5,13))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('hetamap_4.svg')\n",
      "4/24:\n",
      "plt.subplots(figsize=(4,13))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('hetamap_4.svg')\n",
      "4/25:\n",
      "plt.subplots(figsize=(4,13))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('prot_mlo_masde3.svg')\n",
      "4/26: !ls\n",
      "4/27: !pwd\n",
      "4/28:\n",
      "plt.subplots(figsize=(4,13))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_masde3.svg')\n",
      "4/29: len(chico3mas)\n",
      "4/30: chico3mas\n",
      "4/31: chico3omas\n",
      "4/32: len(chico3omas)\n",
      "4/33:\n",
      "chico3omas = chico.query('suma >= 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico2omas = chico.query('suma >= 2').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "4/34:\n",
      "plt.subplots(figsize=(4,13))\n",
      "ax = sns.heatmap(chico2omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "4/35:\n",
      "plt.subplots(figsize=(4,20))\n",
      "ax = sns.heatmap(chico2omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "4/36:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "4/37:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "4/38: chico2omas.columns\n",
      "4/39: chico2omas.columns[:-1]\n",
      "4/40: chico2omas\n",
      "4/41: chico2omas.columns[:-1]\n",
      "4/42:\n",
      "orden = ['suma', 'Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD', 'Other',\n",
      "       ]\n",
      "4/43:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.sort_values(orden,ascending=False).drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "4/44:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.sort_values(orden,ascending=False).drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      " 6/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "import altair as alt\n",
      "import numpy as np\n",
      " 6/2:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      " 6/3:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_latest.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      " 6/4:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      " 6/5:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      " 6/6:\n",
      "print('Proteinas plaac que no tienen LC segun Seg')\n",
      "print(\", \".join([u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]))\n",
      "print(\", \".join([tablas['gene_names_dict'][uniprot] for uniprot in [u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]]))\n",
      " 6/7:\n",
      "# PLAAC\n",
      "plaac['gene_name'] = plaac.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "print('cantidad de proteinas con PLD:',len(plaac.uniprot.unique()))\n",
      " 6/8: plaac = tablas['human_dominios_box1'].query('source == \"plaac\"')\n",
      " 6/9:\n",
      "# PLAAC\n",
      "plaac['gene_name'] = plaac.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "print('cantidad de proteinas con PLD:',len(plaac.uniprot.unique()))\n",
      "6/10: plaac_control = pd.read_csv(\"/home/fernando/datos/plaac_swiss_prot_human.tsv\",sep='\\t')\n",
      "6/11: plaac_control[\"uniprot\"] = plaac_control.SEQid.str.split(\"|\").str[1]\n",
      "6/12: plaac_control = plaac_control[~plaac_control.COREscore.isna()]\n",
      "6/13:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      "6/14:\n",
      "print('Proteinas plaac que no tienen LC segun Seg')\n",
      "print(\", \".join([u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]))\n",
      "print(\", \".join([tablas['gene_names_dict'][uniprot] for uniprot in [u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]]))\n",
      "6/15: db.uniprot = db.uniprot.str.strip()\n",
      "6/16:\n",
      "base = sp.copy()\n",
      "base['clase'] = 'no_lc'\n",
      "6/17:\n",
      "lc = tablas['human_lc_box1'].query('method == \"SEG_intermediate\"').copy()\n",
      "\n",
      "db = tablas['human_box1'][['uniprot','mlo_loc']].drop_duplicates()\n",
      "6/18: lc_db = lc.merge(tablas['human_box1'][['uniprot','mlo_loc']], how='left').drop_duplicates()\n",
      "6/19: plaac = tablas['human_dominios_box1'].query('source == \"plaac\"')\n",
      "6/20:\n",
      "# PLAAC\n",
      "plaac['gene_name'] = plaac.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "print('cantidad de proteinas con PLD:',len(plaac.uniprot.unique()))\n",
      "6/21: plaac_control = pd.read_csv(\"/home/fernando/datos/plaac_swiss_prot_human.tsv\",sep='\\t')\n",
      "6/22: plaac_control[\"uniprot\"] = plaac_control.SEQid.str.split(\"|\").str[1]\n",
      "6/23: plaac_control = plaac_control[~plaac_control.COREscore.isna()]\n",
      "6/24:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      "6/25:\n",
      "print('Proteinas plaac que no tienen LC segun Seg')\n",
      "print(\", \".join([u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]))\n",
      "print(\", \".join([tablas['gene_names_dict'][uniprot] for uniprot in [u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]]))\n",
      "6/26: db.uniprot = db.uniprot.str.strip()\n",
      "6/27:\n",
      "base = sp.copy()\n",
      "base['clase'] = 'no_lc'\n",
      "6/28: base.loc[base.seq.map(lambda x: re.findall(r'RGG[A-Z]{0,4}RGG[A-Z]{0,4}RGG[A-Z]{0,4}',x)).astype(bool),'clase'] = 'featured'\n",
      "6/29:\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "base.loc[base.seq.map(lambda x: re.findall(r'RGG[A-Z]{0,4}RGG[A-Z]{0,4}RGG[A-Z]{0,4}',x)).astype(bool),'clase'] = 'featured'\n",
      "6/30:\n",
      "#from Bio import SeqIO\n",
      "import re\n",
      "base.loc[base.seq.map(lambda x: re.findall(r'RGG[A-Z]{0,4}RGG[A-Z]{0,4}RGG[A-Z]{0,4}',x)).astype(bool),'clase'] = 'featured'\n",
      "6/31: base.clase.value_counts()\n",
      "6/32: !conda install biopython\n",
      "6/33:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "#from Bio import SeqIO\n",
      "#import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "6/34:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "6/35:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "6/36:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "6/37: base.loc[base.uniprot.isin(lc_control.uniprot),'clase'] = 'lc'\n",
      "6/38: base.clase.value_counts()\n",
      "6/39:\n",
      "lce = lc.explode('description')\n",
      "lce[\"aas\"] = lce.description.str.replace(' rich region','')\n",
      "rs_uniprots = lce[lce.aas.str.contains('S') & lce.aas.str.contains('R')]\n",
      "base.loc[base.uniprot.isin(rs_uniprots.uniprot),'clase'] = 'featured'\n",
      "6/40: base.clase.value_counts()\n",
      "6/41:\n",
      "base = sp.copy()\n",
      "base['clase'] = 'no_lc'\n",
      "6/42:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "6/43: base.loc[base.uniprot.isin(lc_control.uniprot),'clase'] = 'lc'\n",
      "6/44: base.loc[base.uniprot.isin(plaac_control.uniprot), 'clase'] = 'featured'\n",
      "6/45: base.clase.value_counts()\n",
      "6/46:\n",
      "#from Bio import SeqIO\n",
      "import re\n",
      "base.loc[base.seq.map(lambda x: re.findall(r'RGG[A-Z]{0,4}RGG[A-Z]{0,4}RGG[A-Z]{0,4}',x)).astype(bool),'clase'] = 'featured'\n",
      "6/47: base.clase.value_counts()\n",
      "6/48: base\n",
      "6/49: base1 = base[base.uniprot.isin(tablas['human_box1'].uniprot)]\n",
      "6/50: base1.clase.value_counts()\n",
      "6/51: base = base1\n",
      "6/52: base['clase'] = 'no_lc'\n",
      "6/53: base.loc[base.uniprot.isin(plaac_control.uniprot), 'clase'] = 'featured'\n",
      "6/54: base1.clase.value_counts()\n",
      "6/55:\n",
      "#from Bio import SeqIO\n",
      "import re\n",
      "base.loc[base.seq.map(lambda x: re.findall(r'RGG[A-Z]{0,4}RGG[A-Z]{0,4}RGG[A-Z]{0,4}',x)).astype(bool),'clase'] = 'featured'\n",
      "6/56: base1.clase.value_counts()\n",
      "6/57:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "6/58:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_latest.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "6/59:\n",
      "def max_polisize(seq):\n",
      "    # Calcula mayor largo de secuencia de aminoacidos iguales\n",
      "    last = seq[0]\n",
      "    c = 0\n",
      "    zonas = []\n",
      "\n",
      "    for char in seq[1:]:\n",
      "        if char == last:\n",
      "            c +=1\n",
      "        else:\n",
      "            zonas.append(c)\n",
      "            c = 0\n",
      "        last = char\n",
      "    zonas.append(c)\n",
      "    return max(zonas)\n",
      "6/60:\n",
      "lc = tablas['human_lc_box1'].query('method == \"SEG_intermediate\"').copy()\n",
      "\n",
      "db = tablas['human_box1'][['uniprot','mlo_loc']].drop_duplicates()\n",
      "6/61: lc_db = lc.merge(tablas['human_box1'][['uniprot','mlo_loc']], how='left').drop_duplicates()\n",
      "6/62: plaac = tablas['human_dominios_box1'].query('source == \"plaac\"')\n",
      "6/63:\n",
      "# PLAAC\n",
      "plaac['gene_name'] = plaac.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "print('cantidad de proteinas con PLD:',len(plaac.uniprot.unique()))\n",
      "6/64: plaac_control = pd.read_csv(\"/home/fernando/datos/plaac_swiss_prot_human.tsv\",sep='\\t')\n",
      "6/65: plaac_control[\"uniprot\"] = plaac_control.SEQid.str.split(\"|\").str[1]\n",
      "6/66: plaac_control = plaac_control[~plaac_control.COREscore.isna()]\n",
      "6/67:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      "6/68:\n",
      "print('Proteinas plaac que no tienen LC segun Seg')\n",
      "print(\", \".join([u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]))\n",
      "print(\", \".join([tablas['gene_names_dict'][uniprot] for uniprot in [u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]]))\n",
      "6/69: db.uniprot = db.uniprot.str.strip()\n",
      "6/70:\n",
      "base = sp.copy()\n",
      "base['clase'] = 'no_lc'\n",
      "6/71:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "6/72: base.loc[base.uniprot.isin(lc_control.uniprot),'clase'] = 'lc'\n",
      "6/73:\n",
      "#from Bio import SeqIO\n",
      "#import re\n",
      "#base.loc[base.seq.map(lambda x: re.findall(r'RGG[A-Z]{0,4}RGG[A-Z]{0,4}RGG[A-Z]{0,4}',x)).astype(bool),'clase'] = 'featured'\n",
      "6/74: base.loc[base.uniprot.isin(plaac_control.uniprot), 'clase'] = 'featured'\n",
      "6/75:\n",
      "final = base.merge(db, how='left')\n",
      "final.mlo_loc = final.mlo_loc.fillna(\"control\")\n",
      "6/76:\n",
      "pivot = final.pivot_table(index='mlo_loc',columns='clase',aggfunc='size')\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot.suma\n",
      "pivot = pivot.fillna(0)\n",
      "6/77: pivot['suma'] = pivot.featured + pivot.lc\n",
      "6/78:\n",
      "\n",
      "ax = pivot.set_index(pivot.index.map(tablas['translate']).fillna(\"Control\")).fillna(0).sort_values('suma')[[\"featured\",'lc','no_lc']].plot.barh(#figsize=(3,4),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','darkseagreen','lightgrey']\n",
      "                                                                   )\n",
      "plt.yticks(fontsize=13)\n",
      "plt.legend(['Fatured LC zones', 'LC Zones', 'NO LC zones'], title='Proteins with:', bbox_to_anchor=(1.05, 1),ncol=3, loc='upper left')\n",
      " 7/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      " 7/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      " 7/3:\n",
      "dominios = pd.read_csv(\"/home/fernando/git/mlo/todo/tablas/toR/fromR_dominios_pfam.csv\")\n",
      "clanes = pd.read_csv(\"/home/fernando/git/mlo/todo/tablas/toR/fromR_clanes_pfam.csv\")\n",
      " 7/4:\n",
      "total_prot_box1 = 591\n",
      "total_prot_sp = 20375\n",
      " 7/5: dominios['sig'] = dominios.ph_sp_box1_adj < 0.05\n",
      " 7/6:\n",
      "clanes = pd.read_csv('/home/fernando/git/mlo/todo/tablas/Pfam-A.clans.tsv','\\t',header=None)\n",
      "clanes = pd.read_csv('/home/fernando/git/mlo/todo/tablas/Pfam-A','\\t',header=None)\n",
      "clanes.columns = ['pfam_id','clan_id','clan_name','domain','pfam_domain_name']\n",
      "domain_ids = clanes.set_index('pfam_id')['domain'].to_dict()\n",
      " 7/7:\n",
      "clanes = pd.read_csv('/home/fernando/git/mlo/todo/tablas/Pfam-A.clans.tsv','\\t',header=None)\n",
      "#clanes = pd.read_csv('/home/fernando/git/mlo/todo/tablas/Pfam-A','\\t',header=None)\n",
      "clanes.columns = ['pfam_id','clan_id','clan_name','domain','pfam_domain_name']\n",
      "domain_ids = clanes.set_index('pfam_id')['domain'].to_dict()\n",
      " 7/8: dominios['domain_name'] = dominios.uniprot.map(domain_ids)\n",
      " 7/9: dominios\n",
      "7/10: dominios = tablas['human_dominios_box1']\n",
      "7/11: dominios\n",
      "7/12: dominios['domain_name'] = dominios.uniprot.map(domain_ids)\n",
      "7/13: domain_ids\n",
      "7/14:\n",
      "dominios = pd.read_csv(\"/home/fernando/git/mlo/todo/tablas/toR/fromR_dominios_pfam.csv\")\n",
      "clanes = pd.read_csv(\"/home/fernando/git/mlo/todo/tablas/toR/fromR_clanes_pfam.csv\")\n",
      "7/15: #dominios = tablas['human_dominios_box1']\n",
      "7/16:\n",
      "total_prot_box1 = 591\n",
      "total_prot_sp = 20375\n",
      "7/17: dominios['sig'] = dominios.ph_sp_box1_adj < 0.05\n",
      "7/18:\n",
      "clanes = pd.read_csv('/home/fernando/git/mlo/todo/tablas/Pfam-A.clans.tsv','\\t',header=None)\n",
      "#clanes = pd.read_csv('/home/fernando/git/mlo/todo/tablas/Pfam-A','\\t',header=None)\n",
      "clanes.columns = ['pfam_id','clan_id','clan_name','domain','pfam_domain_name']\n",
      "domain_ids = clanes.set_index('pfam_id')['domain'].to_dict()\n",
      "7/19: dominios\n",
      "7/20: dominios['domain_name'] = dominios.pfam_acc.map(domain_ids)\n",
      "7/21:\n",
      "dominios['prop_sp'] = dominios.sp / total_prot_sp\n",
      "dominios['prop_box1'] = dominios.box1 / total_prot_box1\n",
      "7/22: dominios['sig'] = dominios.ph_sp_box1_adj < 0.05\n",
      "7/23: to_plot = dominios.query(\"sig\").query('box1 > 4').sort_values('box1').set_index(\"domain_name\")[['prop_box1','prop_sp']]\n",
      "7/24: d = {k:i for i,k in enumerate(to_plot.index)}\n",
      "7/25: dominios.query(\"sig\").query('box1 > 4').sort_values('box1', ascending=False)\n",
      "7/26: dominios['log'] = dominios.ph_sp_box1_adj.map(lambda x: -np.log(x))\n",
      "7/27: import numpy as np\n",
      "7/28: dominios['log'] = dominios.ph_sp_box1_adj.map(lambda x: -np.log(x))\n",
      "7/29: !ls\n",
      "7/30:\n",
      "dominios.query(\"sig\").query('box1 > 4').sort_values('box1').set_index(\"domain_name\").log.plot.barh(width =.75,figsize=(30,3))\n",
      "#plt.savefig('../figuras/box1_enrich_domain_b.svg')\n",
      "7/31: dominios\n",
      "7/32: dominios[dominios.sig]\n",
      "7/33: dominios[dominios.sig].query('mlo > 5')\n",
      "7/34: dominios[dominios.sig].query('mlo > 10')\n",
      "7/35: dominios\n",
      "7/36: doms = tablas['human_box1_dominios']\n",
      "7/37: doms = tablas['human_dominios_box1']\n",
      "7/38: doms\n",
      "7/39: doms_sig = doms[doms.domain.isin(dominios[dominios.sig].uniprot)]\n",
      "7/40: doms_sig = doms[doms.domain.isin(dominios[dominios.sig].domain)]\n",
      "7/41: dominios\n",
      "7/42: doms_sig = doms[doms.domain.isin(dominios[dominios.sig].domain_name)]\n",
      "7/43: doms_sig\n",
      "7/44: doms_sig.domain.value_counts()\n",
      "7/45: doms_sig.drop_duplicates([\"uniprot\",'domain']).domain.value_counts()\n",
      "7/46: doms_sig.drop_duplicates([\"uniprot\",'mlo_loc']).domain.value_counts()\n",
      "7/47: doms_sig.drop_duplicates([\"domain\",'mlo_loc']).domain.value_counts()\n",
      "7/48: doms_sig.drop_duplicates([\"domain\",'mlo_loc']).domain.value_counts().value_counts()\n",
      "7/49: doms_sig\n",
      "7/50: doms_sig.pivot_table(index='domain',columns='mlo_loc',aggfunc='size')\n",
      "7/51: pivot = doms_sig.pivot_table(index='domain',columns='mlo_loc',aggfunc='size')\n",
      "7/52: doms_sig.query('domain == \"zf-CCCH\"')\n",
      "7/53: pivot = doms_sig.drop_duplicates(['domain','uniprot','mlo_loc']).pivot_table(index='domain',columns='mlo_loc',aggfunc='size')\n",
      "7/54: pivot\n",
      "7/55: pivot['suma'] = pivot.sum(1)\n",
      "7/56: pivot.suma.value_coounts()\n",
      "7/57: pivot.suma.value_counts()\n",
      "7/58: pivot.astype(bool).suma.value_counts()\n",
      "7/59: pivot.fillna(0).astype(bool).suma.value_counts()\n",
      "7/60: pivot.fillna(0)#.astype(bool).suma.value_counts()\n",
      "7/61: pivot.fillna(0).astype(int).astype(bool).suma.value_counts()\n",
      "7/62: pivot.fillna(0).astype(int)#.astype(bool).suma.value_counts()\n",
      "7/63: pivot.fillna(0).astype(int).astype(bool)#.suma.value_counts()\n",
      "7/64: pivot.fillna(0).astype(bool).astype(int)#.suma.value_counts()\n",
      "7/65: pivot.fillna(0).astype(bool).astype(int).suma.value_counts()\n",
      "7/66: pivot = doms_sig.drop_duplicates(['domain','uniprot','mlo_loc']).pivot_table(index='domain',columns='mlo_loc',aggfunc='size')\n",
      "7/67: pivot['suma'] = pivot.fillna(0).astype(bool).astype(int).sum(1)\n",
      "7/68: pivot\n",
      "7/69: doms\n",
      "7/70: doms_sig\n",
      "7/71: doms_sig.drop_duplicates([\"uniprot\",'domain'])\n",
      "7/72: doms_sig.drop_duplicates([\"uniprot\",'domain']).domain.value_counts()\n",
      "7/73: pivot = doms_sig.drop_duplicates(['domain','uniprot','mlo_loc']).pivot_table(index='domain',columns='mlo_loc',aggfunc='size')\n",
      "7/74: pivot['suma'] = pivot.fillna(0).astype(bool).astype(int).sum(1)\n",
      "7/75: pivot['suma_total'] = doms_sig.drop_duplicates([\"uniprot\",'domain']).domain.value_counts()\n",
      "7/76: pivot.sort_values(['suma_total','suma'],ascending=False)\n",
      "7/77: pivot.sort_values(['suma_total','suma'],ascending=False)[:20]\n",
      "7/78: pivot.sort_values(['suma_total','suma'],ascending=False).to_clipboard()\n",
      "7/79: pivot.sort_values(['suma_total','suma'],ascending=False).to_clipboard()\n",
      "7/80: pivot.sort_values(['suma_total','suma'],ascending=False).to_clipboard()\n",
      "7/81: pivot.sort_values(['suma_total','suma'],ascending=False)\n",
      "7/82: pivot.sort_values(['suma_total','suma'],ascending=False)[:30]\n",
      "7/83: (dominios.ph_sp_box1_adj < 0.05).sum()\n",
      "7/84: (dominios.ph_sp_box1_adj < 0.01).sum()\n",
      "7/85: dominios['sig'] = dominios.ph_sp_box1_adj < 0.01\n",
      "7/86: to_plot = dominios.query(\"sig\").query('box1 > 4').sort_values('box1').set_index(\"domain_name\")[['prop_box1','prop_sp']]\n",
      "7/87: d = {k:i for i,k in enumerate(to_plot.index)}\n",
      "7/88: import numpy as np\n",
      "7/89: dominios['log'] = dominios.ph_sp_box1_adj.map(lambda x: -np.log(x))\n",
      "7/90: dominios[dominios.sig].query('mlo > 10')\n",
      "7/91:\n",
      "dominios.query(\"sig\").query('box1 > 4').sort_values('box1').set_index(\"domain_name\").log.plot.barh(width =.75,figsize=(30,3))\n",
      "#plt.savefig('../figuras/box1_enrich_domain_b.svg')\n",
      "7/92: doms = tablas['human_dominios_box1']\n",
      "7/93: doms_sig = doms[doms.domain.isin(dominios[dominios.sig].domain_name)]\n",
      "7/94: doms_sig.drop_duplicates([\"domain\",'mlo_loc']).domain.value_counts().value_counts()\n",
      "7/95: pivot = doms_sig.drop_duplicates(['domain','uniprot','mlo_loc']).pivot_table(index='domain',columns='mlo_loc',aggfunc='size')\n",
      "7/96: pivot['suma'] = pivot.fillna(0).astype(bool).astype(int).sum(1)\n",
      "7/97: pivot['suma_total'] = doms_sig.drop_duplicates([\"uniprot\",'domain']).domain.value_counts()\n",
      "7/98: pivot.sort_values(['suma_total','suma'],ascending=False)[:30]\n",
      "7/99: pivot.sort_values(['suma','suma_total'],ascending=False)[:30]\n",
      "7/100: dominios['sig'] = dominios.ph_sp_box1_adj < 0.05\n",
      "7/101: pivot = doms_sig.drop_duplicates(['domain','uniprot','mlo_loc']).pivot_table(index='domain',columns='mlo_loc',aggfunc='size')\n",
      "7/102: pivot['suma'] = pivot.fillna(0).astype(bool).astype(int).sum(1)\n",
      "7/103: pivot['suma_total'] = doms_sig.drop_duplicates([\"uniprot\",'domain']).domain.value_counts()\n",
      "7/104: pivot.sort_values(['suma','suma_total'],ascending=False)[:30]\n",
      "7/105: pivot.sort_values(['suma_total','suma'],ascending=False)[:30]\n",
      "7/106: pivot.query('suma > 5').sort_values(['suma_total','suma'],ascending=False)[:30]\n",
      "7/107: pivot.query('suma > 3').sort_values(['suma_total','suma'],ascending=False)[:30]\n",
      "7/108: pivot.query('suma > 2').sort_values(['suma_total','suma'],ascending=False)[:30]\n",
      "7/109: pivot.query('suma > 1').sort_values(['suma_total','suma'],ascending=False)[:30]\n",
      "7/110: pivot.query('suma > 1').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/111: pivot.query('suma_total > 5').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/112: pivot.query('suma_total >= 5').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/113: pivot.query('suma_total >= 4').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/114: pivot.query('suma_total >= 5').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/115: dominios['sig'] = dominios.ph_sp_box1_adj < 0.05\n",
      "7/116:\n",
      "dominios.query(\"sig\").query('box1 > 4').sort_values('box1').set_index(\"domain_name\").log.plot.barh(width =.75,figsize=(30,3))\n",
      "#plt.savefig('../figuras/box1_enrich_domain_b.svg')\n",
      "7/117: pivot = doms_sig.drop_duplicates(['domain','uniprot','mlo_loc']).pivot_table(index='domain',columns='mlo_loc',aggfunc='size')\n",
      "7/118: pivot['suma'] = pivot.fillna(0).astype(bool).astype(int).sum(1)\n",
      "7/119: pivot['suma_total'] = doms_sig.drop_duplicates([\"uniprot\",'domain']).domain.value_counts()\n",
      "7/120: pivot.query('suma_total >= 5').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/121: doms_sig = doms[doms.domain.isin(dominios[dominios.sig].domain_name)]\n",
      "7/122: pivot.query('suma_total >= 5').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/123: pivot = doms_sig.drop_duplicates(['domain','uniprot','mlo_loc']).pivot_table(index='domain',columns='mlo_loc',aggfunc='size')\n",
      "7/124: pivot['suma'] = pivot.fillna(0).astype(bool).astype(int).sum(1)\n",
      "7/125: pivot['suma_total'] = doms_sig.drop_duplicates([\"uniprot\",'domain']).domain.value_counts()\n",
      "7/126: pivot.query('suma_total >= 5').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/127: pivot.query('suma_total >= 4').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/128: pivot.query('suma_total >= 5').sort_values(['suma_total','suma'],ascending=False)#[:30]\n",
      "7/129: domains_to_plot = pivot.query('suma_total >= 5').sort_values(['suma_total','suma'],ascending=False).index.tolist()#[:30]\n",
      "7/130: domains_to_plot\n",
      "7/131:\n",
      "# funcion para anotar el heatmap (poner los numeros)\n",
      "def annotate_heatmap2(im, data=None, valfmt=\"{x:.2f}\",\n",
      "                     textcolors=[\"#2e2e2e\", \"white\"],offset=0,\n",
      "                     threshold=None, **textkw):\n",
      "\n",
      "    if not isinstance(data, (list, np.ndarray)):\n",
      "        data = im.get_array()\n",
      "        data2 = matriz_doms.T.values\n",
      "\n",
      "    # Normalize the threshold to the images color range.\n",
      "    if threshold is not None:\n",
      "        threshold = im.norm(threshold)\n",
      "    else:\n",
      "        threshold = im.norm(data.max())/2.\n",
      "\n",
      "    # Set default alignment to center, but allow it to be\n",
      "    # overwritten by textkw.\n",
      "    kw = dict(horizontalalignment=\"center\",\n",
      "              verticalalignment=\"center\")\n",
      "    kw.update(textkw)\n",
      "\n",
      "    # Get the formatter in case a string is supplied\n",
      "    #if isinstance(valfmt, str):\n",
      "    #    valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
      "    def to_int(number):\n",
      "        try:\n",
      "            return(int(number))\n",
      "        except:\n",
      "            return ''        \n",
      "    # Loop over the data and create a `Text` for each \"pixel\".\n",
      "    # Change the text's color depending on the data.\n",
      "    texts = []\n",
      "    for i in range(data.shape[0]):\n",
      "        for j in range(data.shape[1]):\n",
      "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "            text = im.axes.text(j+offset, i+offset, to_int(data[i, j]), **kw, size=14, weight='bold')\n",
      "            texts.append(text)\n",
      "    for i in range(data.shape[0]):\n",
      "        for j in range(data.shape[1]):\n",
      "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "            text = im.axes.text(j-offset, i-offset, to_int(data2[i, j]),size=14, **kw)\n",
      "            texts.append(text)\n",
      "    return texts\n",
      "7/132:\n",
      "dc = tablas['human_dis_box1']\n",
      "dc = dc[['uniprot','dc']].drop_duplicates()\n",
      "7/133: doms_to_plot = dom_enr\n",
      "7/134: domdis = dominios[['uniprot','domain']].drop_duplicates().merge(dc)\n",
      "7/135: domdis['dis_bool'] = domdis.dc.map(lambda x: 'H-D' if x >= 0.7 else ( 'S' if x < 0.30 else 'D'))\n",
      "7/136: dom_enr = dominios.query(\"sig\").query('box1 > 4').sort_values('box1',ascending=False).domain_name.tolist()\n",
      "7/137: dom_enr.insert(1,'PLD')\n",
      "7/138: # Cuadro Box  1\n",
      "7/139: dominios = tablas[\"human_dominios_box1\"]\n",
      "7/140: dominios = dominios[dominios.domain.isin(dom_enr)]\n",
      "7/141:\n",
      "# Obtener cantidad de proteinas con cada dominio (o cantidad de dominios), para MLO\n",
      "\n",
      "d_unicos = dominios.drop_duplicates(['uniprot','mlo_loc','domain']).groupby(['mlo_loc','domain']).agg('size').reset_index()\n",
      "d_unicos.columns = ['mlo_loc','domain','counts']\n",
      "\n",
      "d_varios = dominios.groupby(['mlo_loc','domain']).agg(\"size\").reset_index()\n",
      "d_varios.columns = ['mlo_loc','domain','counts']\n",
      "7/142:\n",
      "conteos = d_unicos.merge(d_varios[['mlo_loc','domain','counts']],on=['mlo_loc','domain'],suffixes=('_prot','_dom')).sort_values('counts_prot',ascending=False)\n",
      "conteos\n",
      "7/143: doms = tablas['human_dominios_box1']\n",
      "7/144: doms_sig = doms[doms.domain.isin(dominios[dominios.sig].domain_name)]\n",
      "7/145: doms_sig.drop_duplicates([\"domain\",'mlo_loc']).domain.value_counts().value_counts()\n",
      "7/146:\n",
      "dc = tablas['human_dis_box1']\n",
      "dc = dc[['uniprot','dc']].drop_duplicates()\n",
      "7/147: doms_to_plot = dom_enr\n",
      "7/148: domdis = dominios[['uniprot','domain']].drop_duplicates().merge(dc)\n",
      "7/149: domdis['dis_bool'] = domdis.dc.map(lambda x: 'H-D' if x >= 0.7 else ( 'S' if x < 0.30 else 'D'))\n",
      "7/150: domdis_pivot = domdis.pivot_table(index='domain',columns='dis_bool',aggfunc='size').fillna(0)[['S','D','H-D']].sort_values('S',ascending=False)#[:10].plot.bar(stacked=True, color=['steelblue','salmon','red'])\n",
      "7/151: domdis_pivot\n",
      "7/152:\n",
      "doms_to_plot = domains_to_plot\n",
      "doms_to_plot.insert(1,'PLD')\n",
      "7/153:\n",
      "dc = tablas['human_dis_box1']\n",
      "dc = dc[['uniprot','dc']].drop_duplicates()\n",
      "7/154:\n",
      "doms_to_plot = domains_to_plot\n",
      "doms_to_plot.insert(1,'PLD')\n",
      "7/155: domdis = dominios[['uniprot','domain']].drop_duplicates().merge(dc)\n",
      "7/156: domdis['dis_bool'] = domdis.dc.map(lambda x: 'H-D' if x >= 0.7 else ( 'S' if x < 0.30 else 'D'))\n",
      "7/157: domdis_pivot = domdis.pivot_table(index='domain',columns='dis_bool',aggfunc='size').fillna(0)[['S','D','H-D']].sort_values('S',ascending=False)#[:10].plot.bar(stacked=True, color=['steelblue','salmon','red'])\n",
      "7/158: domdis_pivot\n",
      "7/159: total_bar = domdis_pivot.reindex(dom_enr).drop(\"zf-C3HC4_3\").copy()\n",
      "7/160: conteos.pivot_table(index='domain',columns='mlo_loc')#.loc[d_unicos.query('rank <= 5').domain.unique()]\n",
      "7/161:\n",
      "mat = (dominios[dominios.domain.isin(d_unicos.domain)].drop_duplicates([\"uniprot\",'mlo_loc','domain']).\n",
      "       pivot_table(index='domain',columns='mlo_loc',aggfunc='count')[\"uniprot\"])\n",
      "7/162: doms_to_plot = dominios.drop_duplicates([\"uniprot\",'domain']).domain.value_counts().loc[mat.index].sort_values(ascending=False).index.tolist()\n",
      "7/163: doms_to_plot\n",
      "7/164:\n",
      "doms_to_plot = domains_to_plot\n",
      "doms_to_plot.insert(1,'PLD')\n",
      "7/165: print(doms_to_plot)\n",
      "7/166: domains_to_plot = pivot.query('suma_total >= 5').sort_values(['suma_total','suma'],ascending=False).index.tolist()#[:30]\n",
      "7/167: domains_to_plot\n",
      "7/168:\n",
      "doms_to_plot = domains_to_plot.copy()\n",
      "doms_to_plot.insert(1,'PLD')\n",
      "7/169: domdis = dominios[['uniprot','domain']].drop_duplicates().merge(dc)\n",
      "7/170: domdis['dis_bool'] = domdis.dc.map(lambda x: 'H-D' if x >= 0.7 else ( 'S' if x < 0.30 else 'D'))\n",
      "7/171: domdis_pivot = domdis.pivot_table(index='domain',columns='dis_bool',aggfunc='size').fillna(0)[['S','D','H-D']].sort_values('S',ascending=False)#[:10].plot.bar(stacked=True, color=['steelblue','salmon','red'])\n",
      "7/172: domdis_pivot\n",
      "7/173: total_bar = domdis_pivot.reindex(dom_enr).drop(\"zf-C3HC4_3\").copy()\n",
      "7/174: conteos.pivot_table(index='domain',columns='mlo_loc')#.loc[d_unicos.query('rank <= 5').domain.unique()]\n",
      "7/175:\n",
      "mat = (dominios[dominios.domain.isin(d_unicos.domain)].drop_duplicates([\"uniprot\",'mlo_loc','domain']).\n",
      "       pivot_table(index='domain',columns='mlo_loc',aggfunc='count')[\"uniprot\"])\n",
      "7/176: mlos = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts().drop('other').index.tolist()\n",
      "7/177: print(doms_to_plot)\n",
      "7/178: matriz_prot = mat.loc[doms_to_plot][mlos]\n",
      "7/179: dominios.drop_duplicates([\"uniprot\",'domain']).domain.value_counts().loc[mat.index].sort_values(ascending=False)\n",
      "7/180:\n",
      "matriz_doms = (d_varios[d_varios.domain.isin(doms_to_plot)].\n",
      "               pivot_table(index='domain',columns='mlo_loc')['counts']\n",
      "               .loc[doms_to_plot,mlos])\n",
      "7/181: total_bar\n",
      "7/182:\n",
      "# funcion para anotar el heatmap (poner los numeros)\n",
      "def annotate_heatmap2(im, data=None, valfmt=\"{x:.2f}\",\n",
      "                     textcolors=[\"#2e2e2e\", \"white\"],offset=0,\n",
      "                     threshold=None, **textkw):\n",
      "\n",
      "    if not isinstance(data, (list, np.ndarray)):\n",
      "        data = im.get_array()\n",
      "        data2 = matriz_doms.T.values\n",
      "\n",
      "    # Normalize the threshold to the images color range.\n",
      "    if threshold is not None:\n",
      "        threshold = im.norm(threshold)\n",
      "    else:\n",
      "        threshold = im.norm(data.max())/2.\n",
      "\n",
      "    # Set default alignment to center, but allow it to be\n",
      "    # overwritten by textkw.\n",
      "    kw = dict(horizontalalignment=\"center\",\n",
      "              verticalalignment=\"center\")\n",
      "    kw.update(textkw)\n",
      "\n",
      "    # Get the formatter in case a string is supplied\n",
      "    #if isinstance(valfmt, str):\n",
      "    #    valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
      "    def to_int(number):\n",
      "        try:\n",
      "            return(int(number))\n",
      "        except:\n",
      "            return ''        \n",
      "    # Loop over the data and create a `Text` for each \"pixel\".\n",
      "    # Change the text's color depending on the data.\n",
      "    texts = []\n",
      "    for i in range(data.shape[0]):\n",
      "        for j in range(data.shape[1]):\n",
      "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "            text = im.axes.text(j+offset, i+offset, to_int(data[i, j]), **kw, size=14, weight='bold')\n",
      "            texts.append(text)\n",
      "    for i in range(data.shape[0]):\n",
      "        for j in range(data.shape[1]):\n",
      "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "            text = im.axes.text(j-offset, i-offset, to_int(data2[i, j]),size=14, **kw)\n",
      "            texts.append(text)\n",
      "    return texts\n",
      "7/183:\n",
      "import matplotlib.gridspec as gridspec\n",
      "#     Plot\n",
      "fig = plt.figure(figsize=(13,8))\n",
      "gs = gridspec.GridSpec(2,1,height_ratios=[10,20])\n",
      "gs.update(wspace=0.00, hspace=0.150)\n",
      "# Axes\n",
      "ax1 = fig.add_subplot(gs[0] )\n",
      "ax3 = fig.add_subplot(gs[1],sharex= ax1 )\n",
      "# Plots\n",
      "total_bar[[\"S\", 'D', 'H-D']].plot.bar(stacked=True, ax=ax1, color=['steelblue','salmon','red'])\n",
      "\n",
      "for i,v in enumerate(total_bar.sum(axis=1)):\n",
      "    ax1.text(i , v +2, str(int(v)), size=12,horizontalalignment='center')\n",
      "\n",
      "# heatmap\n",
      "extent = (0.5, matriz_prot.T.shape[1], matriz_prot.T.shape[0], 0)\n",
      "im = ax3.imshow(matriz_prot.T, cmap=\"GnBu\", aspect='auto')\n",
      "ax3.set_xticks(np.arange(len(doms_to_plot)))\n",
      "ax3.set_yticks(np.arange(len(mlos)))\n",
      "ax3.set_xticklabels(doms_to_plot)\n",
      "ax3.set_yticklabels(mlos)\n",
      "ax3.tick_params(axis='x', rotation=90)\n",
      "b, t = ax3.get_ylim(); # discover the values for bottom and top\n",
      "b += 0.5 # Add 0.5 to the bottom\n",
      "t -= 0.5 # Subtract 0.5 from the top\n",
      "ax3.set_ylim(b, t) # update the ylim(bottom, top) values\n",
      "texts = annotate_heatmap2(im, offset=0.2)\n",
      "ax3.set_xticklabels(ax3.get_xticklabels(),fontsize=13, rotation=0, horizontalalignment='center');\n",
      "\n",
      "labs = [tablas['translate'][i.get_text()] for i in ax3.get_yticklabels()]\n",
      "ax3.set_yticklabels(labs,fontsize=14)\n",
      "# Legends\n",
      "#ax2.legend(title='Disorder',labels = ['Low', 'Middle', 'High'],\n",
      "#          bbox_to_anchor=(1, 2.5), loc='upper left', borderaxespad=0.\n",
      "#         )\n",
      "ax1.legend( labels = ['Low', 'Middle', 'High'])\n",
      "\n",
      "\n",
      "\n",
      "#Eliminar marcos\n",
      "ax1.axis('off')\n",
      "#ax2.axis('off')\n",
      "ax3.spines['right'].set_visible(False)\n",
      "ax3.spines['top'].set_visible(False)\n",
      "ax3.spines['bottom'].set_visible(False)\n",
      "ax3.spines['left'].set_visible(False)\n",
      "ax3.tick_params(axis='x',which='both', labelbottom='off', labeltop='on')\n",
      "\n",
      "# Minor ticks\n",
      "ax3.set_xticks(np.arange(-.5, len(doms_to_plot), 1), minor=True);\n",
      "ax3.grid(which='minor', color='w',  linewidth=8)\n",
      "\n",
      "#ax3.set_yticks(np.arange(-.5, len(mlos), 1), minor=False);\n",
      "#ax3.grid(which='major', color='w',  linewidth=2)\n",
      "for xmaj in ax3.yaxis.get_majorticklocs():\n",
      "    ax3.axhline(y=xmaj-0.5,color='w',linewidth=2, ls='-')\n",
      "\n",
      "#plt.savefig('../figuras/box1_dominios_heatmap.svg')\n",
      "#ax3.set_frame_on(False)\n",
      "7/184:\n",
      "import matplotlib.gridspec as gridspec\n",
      "#     Plot\n",
      "fig = plt.figure(figsize=(14,8))\n",
      "gs = gridspec.GridSpec(2,1,height_ratios=[10,20])\n",
      "gs.update(wspace=0.00, hspace=0.150)\n",
      "# Axes\n",
      "ax1 = fig.add_subplot(gs[0] )\n",
      "ax3 = fig.add_subplot(gs[1],sharex= ax1 )\n",
      "# Plots\n",
      "total_bar[[\"S\", 'D', 'H-D']].plot.bar(stacked=True, ax=ax1, color=['steelblue','salmon','red'])\n",
      "\n",
      "for i,v in enumerate(total_bar.sum(axis=1)):\n",
      "    ax1.text(i , v +2, str(int(v)), size=12,horizontalalignment='center')\n",
      "\n",
      "# heatmap\n",
      "extent = (0.5, matriz_prot.T.shape[1], matriz_prot.T.shape[0], 0)\n",
      "im = ax3.imshow(matriz_prot.T, cmap=\"GnBu\", aspect='auto')\n",
      "ax3.set_xticks(np.arange(len(doms_to_plot)))\n",
      "ax3.set_yticks(np.arange(len(mlos)))\n",
      "ax3.set_xticklabels(doms_to_plot)\n",
      "ax3.set_yticklabels(mlos)\n",
      "ax3.tick_params(axis='x', rotation=90)\n",
      "b, t = ax3.get_ylim(); # discover the values for bottom and top\n",
      "b += 0.5 # Add 0.5 to the bottom\n",
      "t -= 0.5 # Subtract 0.5 from the top\n",
      "ax3.set_ylim(b, t) # update the ylim(bottom, top) values\n",
      "texts = annotate_heatmap2(im, offset=0.2)\n",
      "ax3.set_xticklabels(ax3.get_xticklabels(),fontsize=13, rotation=0, horizontalalignment='center');\n",
      "\n",
      "labs = [tablas['translate'][i.get_text()] for i in ax3.get_yticklabels()]\n",
      "ax3.set_yticklabels(labs,fontsize=14)\n",
      "# Legends\n",
      "#ax2.legend(title='Disorder',labels = ['Low', 'Middle', 'High'],\n",
      "#          bbox_to_anchor=(1, 2.5), loc='upper left', borderaxespad=0.\n",
      "#         )\n",
      "ax1.legend( labels = ['Low', 'Middle', 'High'])\n",
      "\n",
      "\n",
      "\n",
      "#Eliminar marcos\n",
      "ax1.axis('off')\n",
      "#ax2.axis('off')\n",
      "ax3.spines['right'].set_visible(False)\n",
      "ax3.spines['top'].set_visible(False)\n",
      "ax3.spines['bottom'].set_visible(False)\n",
      "ax3.spines['left'].set_visible(False)\n",
      "ax3.tick_params(axis='x',which='both', labelbottom='off', labeltop='on')\n",
      "\n",
      "# Minor ticks\n",
      "ax3.set_xticks(np.arange(-.5, len(doms_to_plot), 1), minor=True);\n",
      "ax3.grid(which='minor', color='w',  linewidth=8)\n",
      "\n",
      "#ax3.set_yticks(np.arange(-.5, len(mlos), 1), minor=False);\n",
      "#ax3.grid(which='major', color='w',  linewidth=2)\n",
      "for xmaj in ax3.yaxis.get_majorticklocs():\n",
      "    ax3.axhline(y=xmaj-0.5,color='w',linewidth=2, ls='-')\n",
      "\n",
      "#plt.savefig('../figuras/box1_dominios_heatmap.svg')\n",
      "#ax3.set_frame_on(False)\n",
      "7/185:\n",
      "import matplotlib.gridspec as gridspec\n",
      "#     Plot\n",
      "fig = plt.figure(figsize=(15,8))\n",
      "gs = gridspec.GridSpec(2,1,height_ratios=[10,20])\n",
      "gs.update(wspace=0.00, hspace=0.150)\n",
      "# Axes\n",
      "ax1 = fig.add_subplot(gs[0] )\n",
      "ax3 = fig.add_subplot(gs[1],sharex= ax1 )\n",
      "# Plots\n",
      "total_bar[[\"S\", 'D', 'H-D']].plot.bar(stacked=True, ax=ax1, color=['steelblue','salmon','red'])\n",
      "\n",
      "for i,v in enumerate(total_bar.sum(axis=1)):\n",
      "    ax1.text(i , v +2, str(int(v)), size=12,horizontalalignment='center')\n",
      "\n",
      "# heatmap\n",
      "extent = (0.5, matriz_prot.T.shape[1], matriz_prot.T.shape[0], 0)\n",
      "im = ax3.imshow(matriz_prot.T, cmap=\"GnBu\", aspect='auto')\n",
      "ax3.set_xticks(np.arange(len(doms_to_plot)))\n",
      "ax3.set_yticks(np.arange(len(mlos)))\n",
      "ax3.set_xticklabels(doms_to_plot)\n",
      "ax3.set_yticklabels(mlos)\n",
      "ax3.tick_params(axis='x', rotation=90)\n",
      "b, t = ax3.get_ylim(); # discover the values for bottom and top\n",
      "b += 0.5 # Add 0.5 to the bottom\n",
      "t -= 0.5 # Subtract 0.5 from the top\n",
      "ax3.set_ylim(b, t) # update the ylim(bottom, top) values\n",
      "texts = annotate_heatmap2(im, offset=0.2)\n",
      "ax3.set_xticklabels(ax3.get_xticklabels(),fontsize=13, rotation=0, horizontalalignment='center');\n",
      "\n",
      "labs = [tablas['translate'][i.get_text()] for i in ax3.get_yticklabels()]\n",
      "ax3.set_yticklabels(labs,fontsize=14)\n",
      "# Legends\n",
      "#ax2.legend(title='Disorder',labels = ['Low', 'Middle', 'High'],\n",
      "#          bbox_to_anchor=(1, 2.5), loc='upper left', borderaxespad=0.\n",
      "#         )\n",
      "ax1.legend( labels = ['Low', 'Middle', 'High'])\n",
      "\n",
      "\n",
      "\n",
      "#Eliminar marcos\n",
      "ax1.axis('off')\n",
      "#ax2.axis('off')\n",
      "ax3.spines['right'].set_visible(False)\n",
      "ax3.spines['top'].set_visible(False)\n",
      "ax3.spines['bottom'].set_visible(False)\n",
      "ax3.spines['left'].set_visible(False)\n",
      "ax3.tick_params(axis='x',which='both', labelbottom='off', labeltop='on')\n",
      "\n",
      "# Minor ticks\n",
      "ax3.set_xticks(np.arange(-.5, len(doms_to_plot), 1), minor=True);\n",
      "ax3.grid(which='minor', color='w',  linewidth=8)\n",
      "\n",
      "#ax3.set_yticks(np.arange(-.5, len(mlos), 1), minor=False);\n",
      "#ax3.grid(which='major', color='w',  linewidth=2)\n",
      "for xmaj in ax3.yaxis.get_majorticklocs():\n",
      "    ax3.axhline(y=xmaj-0.5,color='w',linewidth=2, ls='-')\n",
      "\n",
      "#plt.savefig('../figuras/box1_dominios_heatmap.svg')\n",
      "#ax3.set_frame_on(False)\n",
      "11/1: tablas['db']\n",
      "11/2:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "11/3:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "11/4: tablas['db']\n",
      "11/5: tablas['boxes']\n",
      "11/6: tablas['human_boxes']\n",
      "11/7: tablas['human_boxes'].query('box == \"box3\"')\n",
      "15/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "15/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "15/3: tablas['human_box1']\n",
      "15/4: tablas['human_box1'].query('db == \"drllps_scaffolds\"')\n",
      "15/5: tablas['human_box1'].query('db == \"drllps_scaffolds\"')\n",
      "15/6:\n",
      "s = tablas['human_box1'].query('db == \"drllps_scaffolds\"')\n",
      "s['prot'] = s.uniprot.map(tablas['gene_names_dict'])\n",
      "15/7: s\n",
      "15/8: s.groupby('prot').agg(list).mlo\n",
      "15/9: s.groupby('prot').agg(list).mlo.to_frame()\n",
      "15/10: a = s.groupby('prot').agg(list).mlo.to_frame()\n",
      "15/11:\n",
      "a = a.to_frame()\n",
      "a['mlos'] = a.mlo.map(lambda x: \", \".join(sorted(set(x))))\n",
      "a['nmlos'] = a.mlo.str.len()\n",
      "15/12:\n",
      "a = a.toframe()\n",
      "a['mlos'] = a.mlo.map(lambda x: \", \".join(sorted(set(x))))\n",
      "a['nmlos'] = a.mlo.str.len()\n",
      "15/13:\n",
      "a['mlos'] = a.mlo.map(lambda x: \", \".join(sorted(set(x))))\n",
      "a['nmlos'] = a.mlo.str.len()\n",
      "15/14: a\n",
      "15/15: a = a.reset_index().sort_valuse('nmlos',ascending=False)\n",
      "15/16: a = a.reset_index().sort_values('nmlos',ascending=False)\n",
      "15/17: a\n",
      "15/18: a[['prot','nmlos','mlos']]\n",
      "15/19: a = a[['prot','nmlos','mlos']]\n",
      "15/20: a[:20]\n",
      "15/21: db = tablas['human_box1'].copy()\n",
      "15/22: db\n",
      "15/23: db\n",
      "15/24: db.drop_duplicates([\"uniprot\",'mlo_loc','db'])\n",
      "15/25: db = db.drop_duplicates([\"uniprot\",'mlo_loc','db'])\n",
      "15/26: a\n",
      "15/27: db[\"prot\"] = db.uniprot.map(tablas['gene_names_dict'])\n",
      "15/28: db.merge(a)\n",
      "15/29: db\n",
      "15/30: db.groupby('uniprot').agg(\"size\").mlo_loc\n",
      "15/31: db.groupby('uniprot').agg(\"size\")\n",
      "15/32: db.groupby('uniprot').agg(\"counts\")\n",
      "15/33: db.groupby('uniprot').agg(\"len\")\n",
      "15/34: db.groupby('uniprot').agg(len)\n",
      "15/35: db\n",
      "15/36: db.mlo_loc.str.len()\n",
      "15/37: db.mlo_loc.str[0]\n",
      "15/38: db.groupby('uniprot')\n",
      "15/39: db.groupby('uniprot')['mlo_loc']\n",
      "15/40: db.groupby('uniprot').agg(list)\n",
      "15/41: db.groupby('uniprot').agg(list).mlo_loc\n",
      "15/42: db.groupby('uniprot').agg(list).mlo_loc.str.len()\n",
      "15/43: db.groupby('uniprot').agg(list).mlo_loc.str.len().to_dict()\n",
      "15/44: db['nmlos'] = db.uniprot.map(db.groupby('uniprot').agg(list).mlo_loc.str.len().to_dict())\n",
      "15/45: db\n",
      "15/46: db['nmlos'] = db.uniprot.map(db.drop_duplicates([\"uniprot\",'mlo_loc']).groupby('uniprot').agg(list).mlo_loc.str.len().to_dict())\n",
      "15/47: db\n",
      "15/48: db.boxplot('nmlos','db')\n",
      "15/49: db.query('mlo_loc != \"other\"').boxplot('nmlos','db')\n",
      "15/50: db.boxplot('nmlos','db')\n",
      "15/51: db[db.uniprot.isin(db.query('db == \"drllps_scaffolds\"'))]\n",
      "15/52: db[db.uniprot.isin(db.query('db == \"drllps_scaffolds\"').uniprot)]\n",
      "15/53: db.query('db == \"drllps_scaffolds\"')\n",
      "15/54: db.query('db == \"drllps_scaffolds\"').drop_duplicates(\"uniprot\")\n",
      "15/55: db.query('db == \"drllps_scaffolds\"').drop_duplicates(\"uniprot\").nmlos.value_counts()\n",
      "15/56: db.query('db == \"drllps_scaffolds\"').drop_duplicates(\"uniprot\").nmlos.value_counts().sum()\n",
      "15/57: 16/73\n",
      "15/58: 14/73\n",
      "15/59: 34/73\n",
      "15/60: db.query('db == \"phasepdb_lt\"').drop_duplicates(\"uniprot\").nmlos.value_counts()\n",
      "15/61: db.query('db == \"phasepdb_lt\"').drop_duplicates(\"uniprot\").nmlos.value_counts().sum()\n",
      "15/62: 572 - 474\n",
      "15/63: 98 / 474\n",
      "15/64: 98 - 57\n",
      "15/65: 98 / 572\n",
      "15/66: 41 / 572\n",
      "15/67: db.query('db == \"phasepro\"').drop_duplicates(\"uniprot\").nmlos.value_counts().sum()\n",
      "15/68: db.query('db == \"phasepro\"').drop_duplicates(\"uniprot\").nmlos.value_counts()\n",
      "15/69: 25 / 59\n",
      "15/70: (59 - 25) / 59\n",
      "15/71: (59 - 25 - 13) / 59\n",
      "15/72: 59 - 25\n",
      "16/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "16/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "16/3: db = tablas['db'].copy()\n",
      "16/4: db.drop_duplicates([\"uniprot\",'mlo_loc','db'])\n",
      "16/5: db = db.drop_duplicates([\"uniprot\",'mlo_loc','db'])\n",
      "16/6: db\n",
      "16/7: db['prot'] = db.uniprot.map(tablas['gene_names_dict'])\n",
      "16/8: db\n",
      "16/9: db.query('org == \"Homo sapiens\"')\n",
      "16/10: db = db.query('org == \"Homo sapiens\"')\n",
      "16/11: db\n",
      "16/12: db.groupby(\"prot\").agg(list).mlo_loc\n",
      "16/13: db.groupby(\"prot\").agg(list).mlo_loc.str.len().todict()\n",
      "16/14: db['nmlos'] = db.prot.map(db.groupby(\"prot\").agg(list).mlo_loc.str.len().to_dict())\n",
      "16/15: db\n",
      "16/16: db['nmlos'] = db.prot.map(db.drop_duplicates(\"uniprot\").groupby(\"prot\").agg(list).mlo_loc.str.len().to_dict())\n",
      "16/17: db\n",
      "16/18: db['nmlos'] = db.prot.map(db.drop_duplicates([\"uniprot\",'mlo_loc']).groupby(\"prot\").agg(list).mlo_loc.str.len().to_dict())\n",
      "16/19: db\n",
      "16/20: db.groupby(index='db', columns='nmlos', aggfunc='size')\n",
      "16/21: db.pivot_table(index='db', columns='nmlos', aggfunc='size')\n",
      "16/22: db_pivot = db.pivot_table(index='db', columns='nmlos', aggfunc='size')\n",
      "16/23: db_pivot\n",
      "16/24: db_pivot['suma'] = db_pivot.sum(1)\n",
      "16/25: db_pivot\n",
      "16/26: db_pivot = db.drop_duplicates([\"uniprot\",'mlo_loc']).pivot_table(index='db', columns='nmlos', aggfunc='size')\n",
      "16/27: db_pivot['suma'] = db_pivot.sum(1)\n",
      "16/28: db_pivot\n",
      "16/29: db_pivot = db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='db', columns='nmlos', aggfunc='size')\n",
      "16/30: db_pivot['suma'] = db_pivot.sum(1)\n",
      "16/31: db_pivot\n",
      "16/32: db_pivot = db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='db', columns='nmlos', aggfunc='size')\n",
      "16/33: db_pivot.columns = db_pivot.columns.astype(int)\n",
      "16/34: db_pivot\n",
      "16/35: db_pivot['suma'] = db_pivot.sum(1)\n",
      "16/36:\n",
      "db_pivot['mas1'] = db_pivot[[2,3,4,5,6]].sum(1)\n",
      "db_pivot['mas2'] = db_pivot[[3,4,5,6]].sum(1)\n",
      "16/37: db_pivot\n",
      "16/38: db.query('drllps_scaffolds')\n",
      "16/39: db.query('db == \"drllps_scaffolds\"')\n",
      "16/40: db.query('db == \"drllps_scaffolds\"').drop_duplicates(\"uniprot\").nmlos.value_counts()\n",
      "16/41: 73 - 15\n",
      "16/42: 73 - 19\n",
      "16/43: 58 / 73\n",
      "16/44: 42 / 73\n",
      "16/45: db.query('db == \"phasepdb_lt\"').drop_duplicates(\"uniprot\").nmlos.value_counts()\n",
      "16/46: db.query('db == \"phasepdb_lt\"').drop_duplicates(\"uniprot\").nmlos.value_counts().sum()\n",
      "16/47: 315/572\n",
      "16/48: 145/570\n",
      "16/49: db.query('db == \"phasepdb_lt\"').drop_duplicates([\"uniprot\",'mlo_loc']).uniprot.value_counts()\n",
      "16/50: db.query('db == \"phasepdb_lt\"').drop_duplicates([\"uniprot\"]).nmlos.value_counts()\n",
      "16/51: 570 - 255\n",
      "16/52: 315 / 570\n",
      "16/53: 315 / 572\n",
      "16/54: 145 / 572\n",
      "16/55: db.query('db == \"phasepdb_lt\"')\n",
      "16/56: db.query('db == \"phasepdb_lt\"').drop_duplicates(\"uniprot\")\n",
      "16/57: db.query('db == \"phasepdb_lt\"').drop_duplicates(\"uniprot\").nmlos.value_counts()\n",
      "16/58: db_pivot_pc = db_pivot.copy()\n",
      "16/59:\n",
      "for col in db_pivot_pc.columns:\n",
      "    db_pivot_pc[col] = db_pivot_pc[col] / db_pivot_pc.suma\n",
      "16/60: db_pivot_pc\n",
      "16/61: db_pivot_pc.columns\n",
      "16/62: db_pivot_pc\n",
      "16/63: db_pivot_pc = db_pivot.copy()\n",
      "16/64:\n",
      "for col in db_pivot_pc.columns:\n",
      "    db_pivot_pc[col] = db_pivot_pc[col] / db_pivot_pc['suma']\n",
      "16/65: db_pivot_pc\n",
      "16/66: db_pivot_pc.suma\n",
      "16/67: db_pivot = db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='db', columns='nmlos', aggfunc='size')\n",
      "16/68: db_pivot.columns = db_pivot.columns.astype(int)\n",
      "16/69:\n",
      "db_pivot['mas1'] = db_pivot[[2,3,4,5,6]].sum(1)\n",
      "db_pivot['mas2'] = db_pivot[[3,4,5,6]].sum(1)\n",
      "16/70: db_pivot['suma'] = db_pivot[[1,2,3,4,5,6]].sum(1)\n",
      "16/71: db_pivot\n",
      "16/72: db_pivot_pc = db_pivot.copy()\n",
      "16/73:\n",
      "for col in db_pivot_pc.columns:\n",
      "    db_pivot_pc[col] = db_pivot_pc[col] / db_pivot_pc['suma']\n",
      "16/74: db_pivot_pc\n",
      "16/75: db_pivot_pc[1,\"mas1\",'mas2']\n",
      "16/76: db_pivot_pc[[1,\"mas1\",'mas2']]\n",
      "16/77: db_pivot_pc[[1,\"mas1\",'mas2']].loc['phasepro','phasepdb_lt','drllps_scaffolds']\n",
      "16/78: db_pivot_pc[[1,\"mas1\",'mas2']].loc[['phasepro','phasepdb_lt','drllps_scaffolds']]\n",
      "16/79: db_pivot_pc[[1,\"mas1\",'mas2']].loc[['phasepro','phasepdb_lt','drllps_scaffolds']].plot.barh()\n",
      "16/80: db_pivot_pc[[1,\"mas1\",'mas2']].loc[['phasepro','phasepdb_lt','drllps_scaffolds']].T.plot.barh()\n",
      "16/81: db_pivot_pc[[1,\"mas1\",'mas2']].loc[['phasepro','phasepdb_lt','drllps_scaffolds']].T.plot.bar()\n",
      "16/82: db_pivot_pc[[1,\"mas1\",'mas2']].loc[['phasepro','phasepdb_lt','drllps_scaffolds']].plot.bar()\n",
      "16/83: db_pivot_pc[[1,\"mas1\",'mas2']].loc[['phasepro','phasepdb_lt','drllps_scaffolds']].plot.bar(stacked=True)\n",
      "16/84: db_pivot_pc[[1,\"mas1\",'mas2']].loc[['phasepro','phasepdb_lt','drllps_scaffolds']].plot.bar()\n",
      "16/85: db\n",
      "16/86: db['scaffold'] = db.db == \"drllps_scaffolds\"\n",
      "16/87: db\n",
      "16/88: db['scaffold'] = db.uniprot.isin(db.query('db == \"drllps_scaffold\"').uniprot)\n",
      "16/89: db.query('nmlos > 3')#.sc\n",
      "16/90: db['scaffold'] = db.uniprot.isin(db.query('db == \"drllps_scaffolds\"').uniprot)\n",
      "16/91: db.query('nmlos > 3')#.sc\n",
      "16/92: db.drop_duplicates(\"uniprot\").query('nmlos > 3').scaffold.value_counts()\n",
      "16/93: db.drop_duplicates(\"uniprot\").query('nmlos > 4').scaffold.value_counts()\n",
      "16/94: db.drop_duplicates(\"uniprot\").query('nmlos > 5').scaffold.value_counts()\n",
      "16/95: db.drop_duplicates(\"uniprot\").query('nmlos > 2').scaffold.value_counts()\n",
      "16/96: db.drop_duplicates(\"uniprot\").query('nmlos > 3').scaffold.value_counts()\n",
      "16/97: db.drop_duplicates(\"uniprot\").query('nmlos > 4').scaffold.value_counts()\n",
      "16/98: tablas['human_box1_lc']\n",
      "16/99: tablas['human_lc_box1']\n",
      "16/100: tablas['human_lc_box1'].query('aa1 == \"Q\"')\n",
      "16/101:\n",
      "qs = tablas['human_lc_box1'].query('aa1 == \"Q\"')\n",
      "qs[qs.uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"'))]\n",
      "16/102:\n",
      "qs = tablas['human_lc_box1'].query('aa1 == \"Q\"')\n",
      "qs[qs.uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"').uniprot)]\n",
      "16/103:\n",
      "qs = tablas['human_lc_box1'].query('aa1 == \"Q\"')\n",
      "qs[qs.uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"').uniprot)].sort_values('faa1',ascending=False)\n",
      "16/104: qs[qs.uniprot.isin(tablas['db'].query('db == \"phasepro\"').uniprot)].sort_values('faa1',ascending=False)\n",
      "16/105: qs.sort_values('faa1',ascending=False)\n",
      "16/106: qs.sort_values('faa1',ascending=False)[:20]\n",
      "16/107: tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"').uniprot)].sort_values('faa1',ascending=False)\n",
      "16/108: tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"').uniprot)].sort_values('faa1',ascending=False)[:30]\n",
      "16/109: tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"').uniprot)].sort_values('faa1',ascending=False).query('len > 10')[:30]\n",
      "16/110: tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"').uniprot)].sort_values('faa1',ascending=False).query('largo > 10')[:30]\n",
      "17/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "17/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "17/3: boxes = tablas['human_boxes']\n",
      "17/4: boxes.box.value_counts()\n",
      "17/5:\n",
      "boxes = pd.read_csv('tmp_boxes_human.csv')[['uniprot','mlo_loc','box']].drop_duplicates()\n",
      "boxes.box.value_counts()\n",
      "17/6: db = boxes.merge(tablas['tidy'][[\"uniprot\",'org']]).query('org == \"Homo sapiens\"')\n",
      "17/7: boxes = tablas['human_boxes']\n",
      "17/8: boxes.box.value_counts()\n",
      "17/9: db = boxes.merge(tablas['tidy'][[\"uniprot\",'org']]).query('org == \"Homo sapiens\"')\n",
      "17/10: db = db.drop_duplicates()\n",
      "17/11: db.drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "17/12: db.query('box == \"box1\"').mlo_loc.value_counts()\n",
      "17/13:\n",
      "mlos = db.query('box == \"box1\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts().index.tolist()\n",
      "mlos.append(mlos.pop(mlos.index('other')))\n",
      "mlos\n",
      "17/14: box_counts = db.drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "17/15:\n",
      "pivot = db.drop_duplicates().pivot_table(index='mlo_loc',columns='box',aggfunc='size')\n",
      "pivot[\"total\"] = pivot.sum(1)\n",
      "17/16:\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['total']\n",
      "17/17: pivot = pivot.reindex(reversed(mlos))\n",
      "17/18: pivot = pivot.set_index(pivot.index.map(tablas['translate']))\n",
      "17/19: pivot.columns = ['Box 1', 'Box 2', 'Box 3','total']\n",
      "17/20:\n",
      "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
      "colors = [i for i in colors if i != \"#ff7f0e\"]\n",
      "#colors[0] = \"#3273a099\"\n",
      "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n",
      "17/21:\n",
      "ax = pivot.drop(columns=\"total\").plot.barh(stacked=True, width=.75)\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "#plt.savefig('figuras/2_mlo_counts.svg')\n",
      "17/22: pivot\n",
      "17/23:\n",
      "ax = pivot.reindex([tablas[\"translate\"][i] for i in reversed(mlos)]).drop(columns=\"total\").sort_values(\"box1\").plot.barh(stacked=True, width=.75)\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "17/24:\n",
      "ax = pivot.reindex([tablas[\"translate\"][i] for i in reversed(mlos)]).drop(columns=\"total\").sort_values(\"Box 1\").plot.barh(stacked=True, width=.75)\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "17/25:\n",
      "ax = pivot.reindex([tablas[\"translate\"][i] for i in reversed(mlos)]).drop(columns=\"total\").sort_values(\"Box 1\").plot.barh(stacked=True, width=.75,colors=tablas['colors_3b'])\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "17/26: tablas.keys()\n",
      "17/27:\n",
      "ax = pivot.reindex([tablas[\"translate\"][i] for i in reversed(mlos)]).drop(columns=\"total\").sort_values(\"Box 1\").plot.barh(stacked=True, width=.75,color=tablas['colors_3b'])\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "17/28:\n",
      "ax = pivot.reindex([tablas[\"translate\"][i] for i in reversed(mlos)]).drop(columns=\"total\").sort_values(\"Box 1\").drop('Other').plot.barh(stacked=True, width=.75,color=tablas['colors_3b'])\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "16/111: tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"').query('mlo_loc == \"nuclear_speckles\"').uniprot)].sort_values('faa1',ascending=False).query('largo > 10')[:30]\n",
      "16/112: ns = tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"').query('mlo_loc == \"nuclear_speckles\"').uniprot)].sort_values('faa1',ascending=False).query('largo > 10')[:30]\n",
      "16/113: ns['prot'] = ns.uniprot.map(tablas['gene_names_dict'])\n",
      "16/114: ns\n",
      "16/115: ns[[\"uniprot\",'prot','seq']]\n",
      "23/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "23/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "23/3: ns = tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].query('db == \"drllps_scaffolds\"').query('mlo_loc == \"nuclear_speckles\"').uniprot)].sort_values('faa1',ascending=False).query('largo > 10')[:30]\n",
      "23/4: ns['prot'] = ns.uniprot.map(tablas['gene_names_dict'])\n",
      "23/5: ns[[\"uniprot\",'prot','seq']]\n",
      "23/6: tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].query('faa1 == \"P\"').uniprot)].sort_values('faa1',ascending=False).query('largo > 10')[:30]\n",
      "23/7: tablas['human_lc_box1']\n",
      "23/8: tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].query('aa1 == \"P\"').uniprot)].sort_values('faa1',ascending=False).query('largo > 10')[:30]\n",
      "23/9: tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].uniprot)].query('aa1 == \"P\"').sort_values('faa1',ascending=False).query('largo > 10')[:30]\n",
      "23/10: ns['prot'] = ns.uniprot.map(tablas['gene_names_dict'])\n",
      "23/11: tablas['human_lc_box1']['prot'] = tablas['human_lc_box1'].uniprot.map(tablas['gene_names_dict'])\n",
      "23/12: tablas['human_lc_box1'][tablas['human_lc_box1'].uniprot.isin(tablas['db'].uniprot)].query('aa1 == \"P\"').sort_values('faa1',ascending=False).query('largo > 10')[:30]\n",
      "25/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "25/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "25/3: tablas['human_box1']\n",
      "25/4: human = tablas['human_box1']\n",
      "25/5: human = human.drop_duplicates([\"uniprot\",'web'])\n",
      "25/6: human\n",
      "25/7: human.query('web == \"phasepdb\"')\n",
      "25/8: for i in human.query('web == \"phasepdb\"').uniprot.unique():print(i)\n",
      "25/9: for i in human.query('web == \"phasepro\"').uniprot.unique():print(i)\n",
      "25/10: for i in human.query('web == \"drllps\"').uniprot.unique():print(i)\n",
      "25/11: tablas['db']\n",
      "25/12: tablas['boxes']\n",
      "25/13: tablas['info']\n",
      "25/14: tablas['human_boxes']\n",
      "25/15: tablas['human_boxes'].pivot_table(index='mlo_loc',columns='box',aggfunc='size')\n",
      "25/16: tablas['human_boxes'].pivot_table(index='mlo_loc',columns='box',aggfunc='size').sort_values('box1', ascending=False)\n",
      "25/17: tabla = tablas['human_boxes'].pivot_table(index='mlo_loc',columns='box',aggfunc='size').sort_values('box1', ascending=False)\n",
      "25/18: tabla.columns = ['Box 1', 'Box 2', 'Box 3']\n",
      "25/19: tabla.to_clipboard()\n",
      "25/20: tabla.to_clipboard(excel=True)\n",
      "25/21: tabla\n",
      "25/22: tabla.index = tabla.index.map(tablas['translate'])\n",
      "25/23: tabla\n",
      "25/24: tabla.fillna(0).astype(int)\n",
      "25/25: tablas['db']\n",
      "25/26: tablas['db'].query('organism == \"Homo sapiens\"')\n",
      "25/27: tablas['db'].query('organism == \"Homo sapiens\"').query('mlo_loc == \"cytoplasmic_stress_granule\"')\n",
      "25/28: tablas['db'].query('organism == \"Homo sapiens\"').query('mlo_loc == \"cytoplasmic_stress_granule\"').drop_duplicates(\"uniprot\")\n",
      "25/29: tablas['db'].query('organism == \"Homo sapiens\"').mlo_loc.value_counts()\n",
      "25/30: tablas['db'].query('organism == \"Homo sapiens\"').mlo.value_counts()\n",
      "25/31: tablas['db'].query('organism == \"Homo sapiens\"').mlo.value_counts()[:30]\n",
      "25/32: tablas['db'].query('organism == \"Homo sapiens\"').mlo.value_counts()\n",
      "25/33: tablas['db'].query('organism == \"Homo sapiens\"').mlo_loc.value_counts()\n",
      "25/34: tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()\n",
      "25/35: tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'db','mlo_loc']).pivot_table(index='mlo_loc',columns='db',aggfunc='size')\n",
      "25/36: tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'db','mlo_loc']).pivot_table(index='mlo_loc',columns='db',aggfunc='size').fillna(0).astype(int)\n",
      "25/37: dbs = tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'db','mlo_loc']).pivot_table(index='mlo_loc',columns='db',aggfunc='size').fillna(0).astype(int)\n",
      "25/38: dbs.index = dbs.index.map(tablas['translate']).loc[tabla.index]\n",
      "25/39: dbs.index = dbs.index.map(tablas['translate'])\n",
      "25/40: db.loc[tabla.index]\n",
      "25/41: dbs.loc[tabla.index]\n",
      "25/42: dbs = tablas['db'].drop_duplicates([\"uniprot\",'db','mlo_loc']).pivot_table(index='mlo_loc',columns='db',aggfunc='size').fillna(0).astype(int)\n",
      "25/43: dbs.index = dbs.index.map(tablas['translate'])\n",
      "25/44: dbs.loc[tabla.index]\n",
      "25/45: tablas['db'].query('organism == \"Homo sapiens\"')\n",
      "25/46: tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()\n",
      "25/47: tablas['db'].drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()\n",
      "25/48: tablas['db'].drop_duplicates([\"uniprot\",'mlo_loc']).mlo.value_counts()\n",
      "25/49: tablas['db'].query(\"organism == 'Homo sapiens'\").drop_duplicates([\"uniprot\",'mlo_loc']).mlo.value_counts()\n",
      "25/50: tablas['db'].query(\"organism == 'Homo sapiens'\").drop_duplicates([\"uniprot\",'mlo_loc']).mlo.value_counts()[:20]\n",
      "36/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "import altair as alt\n",
      "import numpy as np\n",
      "36/2:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "36/3:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "36/4: tablas['human_lc_box1']\n",
      "36/5: tablas['human_lc_box1'].query('method == \"SEG_intermediate\"')\n",
      "36/6: tablas['human_lc_box1'].query('method == \"SEG_intermediate\"').query('uniprot == \"P52948\"')\n",
      "39/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "39/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "39/3: db_subset = tablas['db'].drop_duplicates([\"uniprot\",'db'])\n",
      "39/4:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 3, figsize=(11,11), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "counts = df.query('web == \"phasepdb\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "colors = {k:v for k,v in zip(labels, c[:len(labels)])}\n",
      "\n",
      "\n",
      "\n",
      "axs[0].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[0].set_title('PhaSepDb ({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"drllps\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS ({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/5:\n",
      "orgs_tot = db_subset.org.value_counts()\n",
      "others = orgs_tot[orgs_tot <= 20].sum()\n",
      "orgs_tot_top = orgs_tot[orgs_tot > 20]\n",
      "orgs_tot_top['Other'] = others\n",
      "\n",
      "db_subset['org'] = db_subset.org\n",
      "db_subset.loc[~db_subset.org.isin(list(orgs_tot_top.index)), 'org'] = 'Other'\n",
      "39/6:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 3, figsize=(11,11), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "counts = df.query('web == \"phasepdb\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "colors = {k:v for k,v in zip(labels, c[:len(labels)])}\n",
      "\n",
      "\n",
      "\n",
      "axs[0].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[0].set_title('PhaSepDb ({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"drllps\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS ({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/7: db_subset\n",
      "39/8: labels\n",
      "39/9:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 3, figsize=(11,11), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "counts = df.query('web == \"phasepdb\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "colors = {k:v for k,v in zip(labels, c[:len(labels)])}\n",
      "\n",
      "\n",
      "\n",
      "axs[0].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[0].set_title('PhaSepDb ({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"drllps\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS ({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/10:\n",
      "orgs_tot = db_subset.org.value_counts()\n",
      "others = orgs_tot[orgs_tot <= 20].sum()\n",
      "orgs_tot_top = orgs_tot[orgs_tot > 20]\n",
      "orgs_tot_top['Other'] = others\n",
      "\n",
      "db_subset['org'] = db_subset.org_short\n",
      "db_subset.loc[~db_subset.org.isin(list(orgs_tot_top.index)), 'org'] = 'Other'\n",
      "39/11:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 3, figsize=(11,11), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "counts = df.query('web == \"phasepdb\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "colors = {k:v for k,v in zip(labels, c[:len(labels)])}\n",
      "\n",
      "\n",
      "\n",
      "axs[0].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[0].set_title('PhaSepDb ({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"drllps\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS ({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/12:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 6, figsize=(11,11), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "colors = {k:v for k,v in zip(labels, c[:len(labels)])}\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Clients({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Regulators({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[0].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[0].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/13:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 6, figsize=(11,11), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "colors = {k:v for k,v in zip(labels, c[:len(labels)])}\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/14: labels\n",
      "39/15: colors\n",
      "39/16: labels\n",
      "39/17: colors = {k:v for k,v in zip(labels, c[:len(labels)])}\n",
      "39/18: colors\n",
      "39/19:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 6, figsize=(11,11), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/20:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/21:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regultators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/22: counts_web\n",
      "39/23: counts_web = db_subset.db.value_counts().to_dict()\n",
      "39/24: counts_web\n",
      "39/25:\n",
      "df = db_subset\n",
      "counts_web = db_subset.web.value_counts().to_dict()\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regultators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/26:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regultators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/27: counts_web = db_subset.db.value_counts().to_dict()\n",
      "39/28:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regultators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/29:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/30:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/31:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/32:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/33:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/34:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/35:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/36:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.1,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/37:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/38:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/39:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "plt.savefig('pie_orgs.svg')\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/40:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.1,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/41:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/42:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/43:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/44:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12',});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/45:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=1.2,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/46:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/47:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/48:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/49:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/50:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/51:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "plt.savefig('pie_orgs.svg')\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/52: db\n",
      "39/53: tablas['db']\n",
      "39/54: tablas['db'].drop_duplicates([\"uniprot\",'db']).uniprot.value_counts()\n",
      "39/55: tablas['db'].drop_duplicates([\"uniprot\",'db']).db.value_counts()\n",
      "39/56:\n",
      "df = db_subset\n",
      "\n",
      "fig, axs = plt.subplots(1, 6, figsize=(20,20), gridspec_kw={'hspace': 0.01, 'wspace': 0.01})\n",
      "\n",
      "\n",
      "c =sns.color_palette(\"muted\", 10)\n",
      "\n",
      "\n",
      "counts = df.query('web == \"phasepro\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[0].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[0].set_title('PhasePro({})'.format(counts_web['phasepro']), fontsize=16)\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "\n",
      "counts = df.query('db == \"drllps_scaffolds\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[1].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[1].set_title('DrLLPS Scaffolds({})'.format(counts_web['drllps_scaffolds']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_clients\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[2].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[2].set_title('DrLLPS Clients({})'.format(counts_web['drllps_clients']), fontsize=16)\n",
      "\n",
      "counts = df.query('db == \"drllps_regulators\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[3].pie( counts,labels = labels, colors = [colors[lab] for lab in labels],\n",
      "     wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'});\n",
      "axs[3].set_title('DrLLPS Regulators({})'.format(counts_web['drllps_regulators']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_ht\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[4].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[4].set_title('PhaSepDb HT({})'.format(counts_web['phasepdb_ht']), fontsize=16)\n",
      "\n",
      "\n",
      "counts = df.query('db == \"phasepdb_lt\"').org.value_counts()\n",
      "labels = list(counts.index)\n",
      "axs[5].pie( counts.tolist(),labels = labels, colors = [colors[lab] for lab in labels],\n",
      "             wedgeprops=dict(width=0.5, edgecolor='w'),autopct='%1.f%%',pctdistance=0.75,textprops={'size':'12',\"color\":'w'})\n",
      "axs[5].set_title('PhaSepDb LT({})'.format(counts_web['phasepdb_lt']), fontsize=16)\n",
      "\n",
      "\n",
      "plt.legend(loc='lower center', bbox_to_anchor=(-0.5, -0.22),\n",
      "          ncol=3, fancybox=True, shadow=True,  prop={'size': 12});\n",
      "\n",
      "plt.savefig('pie_orgs____.svg')\n",
      "\n",
      "#plt.legend(colors, loc='upper center', ncol=1,   prop={'size': 15});\n",
      "39/57: db\n",
      "39/58: tablas['db']\n",
      "39/59: tablas['db'].query('organism == \"Mus musculus\"').query('web == \"drllps\"')\n",
      "39/60: tablas['db'].query('organism == \"Mus musculus\"').query('web == \"drllps\"').drop_duplicates([\"uniprto\", 'db']).db.value_counts()\n",
      "39/61: tablas['db'].query('organism == \"Mus musculus\"').query('web == \"drllps\"').drop_duplicates([\"uniprot\", 'db']).db.value_counts()\n",
      "39/62: tablas['db'].query('organism == \"Mus musculus\"').query('web == \"drllps\"').drop_duplicates([\"uniprot\", 'db'])#.db.value_counts()\n",
      "39/63: tablas['db'].query('organism == \"Drosophila melanogaster\"').query('web == \"drllps\"').drop_duplicates([\"uniprot\", 'db'])#.db.value_counts()\n",
      "39/64: tablas['db'].query('organism == \"Drosophila melanogaster\"').query('db == \"drllps_scaffolds\"').drop_duplicates([\"uniprot\", 'db'])#.db.value_counts()\n",
      "39/65: tablas['db'].query('organism == \"Drosophila melanogaster\"').db.value_counts()\n",
      "39/66: drl = pd.read_csv('/home/fernando/git/mlo/todo/dbs/entrada/drllps/drllps_19_12_19_proc.tab',sep='\\t')\n",
      "39/67: drl\n",
      "39/68: drl.query('organism == \"Homo sapiens\"')\n",
      "39/69: drl.query('organism == \"Drosophila Melanogaster\"').qe\n",
      "39/70: drl.query('organism == \"Drosophila Melanogaster\"')\n",
      "39/71: drl.query('organism == \"Drosophila melanogaster\"')\n",
      "39/72: drl = pd.read_csv('/home/fernando/git/mlo/todo/dbs/entrada/drllps/23_02_2020/drllps.tab',sep='\\t')\n",
      "39/73: drl\n",
      "39/74: drl.Condensate\n",
      "39/75: drl.Condensate.str.split(', ')\n",
      "39/76: drl.Condensate = drl.Condensate.str.split(', ')\n",
      "39/77: drl.explode('Condensate')\n",
      "39/78: drl = drl.explode('Condensate')\n",
      "39/79: drl\n",
      "39/80: drl.query('Species == \"Drosophila melanogaster\"')\n",
      "39/81: drl.query('Species == \"Drosophila melanogaster\"').query('LLPS type ==  \"scaffold\"')\n",
      "39/82: dro = drl.query('Species == \"Drosophila melanogaster\"')\n",
      "39/83: dro[dro['LLPS type'] == \"scaffold\"]\n",
      "39/84: dro[dro['LLPS type'] == \"Scaffold\"]\n",
      "39/85: dro\n",
      "39/86: dro[dro['LLPS Type'] == \"Scaffold\"]\n",
      "39/87: dro\n",
      "39/88: dro.query('Condensate != \"Droplet\"')\n",
      "39/89: dro.query('Condensate != \"Droplet\"')['LLPS Type']\n",
      "39/90: dro.query('Condensate != \"Droplet\"')['LLPS Type'].value_counts()\n",
      "39/91: tablas['db']\n",
      "39/92: tablas['db'].query('db == \"drllps_scaffolds\"')\n",
      "39/93: tablas['db'].query('db == \"drllps_scaffolds\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()\n",
      "39/94: tablas['human_box1'].mlo_loc.value_counts()\n",
      "39/95: tablas['human_box1'].mlo_loc.value_counts().index.tolist()\n",
      "39/96:\n",
      "tablas['db'].query('db == \"drllps_scaffolds\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()[\n",
      " 'nucleolus',\n",
      " 'nuclear_speckles',\n",
      " 'cytoplasmic_stress_granule',\n",
      " 'p_body',\n",
      " 'paraspeckles',\n",
      " 'pml_body',\n",
      " 'cajal_body',\n",
      " 'nuclear_pore_complex',\n",
      " 'postsynaptic_density','other']\n",
      "39/97: tablas['db'].query('db == \"drllps_scaffolds\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()\n",
      "39/98:\n",
      "tablas['db'].query('db == \"drllps_scaffolds\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()[[\n",
      " 'nucleolus',\n",
      " 'nuclear_speckles',\n",
      " 'cytoplasmic_stress_granule',\n",
      " 'p_body',\n",
      " 'paraspeckles',\n",
      " 'pml_body',\n",
      " 'cajal_body',\n",
      " 'nuclear_pore_complex',\n",
      " 'postsynaptic_density','other']]\n",
      "39/99:\n",
      "tablas['db'].query('db == \"drllps_regulators\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()[[\n",
      " 'nucleolus',\n",
      " 'nuclear_speckles',\n",
      " 'cytoplasmic_stress_granule',\n",
      " 'p_body',\n",
      " 'paraspeckles',\n",
      " 'pml_body',\n",
      " 'cajal_body',\n",
      " 'nuclear_pore_complex',\n",
      " 'postsynaptic_density','other']]\n",
      "39/100:\n",
      "tablas['db'].query('db == \"drllps_regulators\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()#[[\n",
      " 'nucleolus',\n",
      " 'nuclear_speckles',\n",
      " 'cytoplasmic_stress_granule',\n",
      " 'p_body',\n",
      " 'paraspeckles',\n",
      " 'pml_body',\n",
      " 'cajal_body',\n",
      " 'nuclear_pore_complex',\n",
      " 'postsynaptic_density','other']]\n",
      "39/101:\n",
      "orden = [\n",
      " 'nucleolus',\n",
      " 'nuclear_speckles',\n",
      " 'cytoplasmic_stress_granule',\n",
      " 'p_body',\n",
      " 'paraspeckles',\n",
      " 'pml_body',\n",
      " 'cajal_body',\n",
      " 'nuclear_pore_complex',\n",
      " 'postsynaptic_density','other']\n",
      "39/102: tablas['db'].query('db == \"drllps_regulators\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts()\n",
      "39/103: tablas['boxes']\n",
      "39/104: tablas.keys()\n",
      "39/105: tabas['human_boxes']\n",
      "39/106: tablas['human_boxes']\n",
      "39/107: tablas['db']\n",
      "39/108: tablas['db'].drop_duplicates([\"uniprot\",'db'])\n",
      "39/109: tablas['db'].drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "39/110: tablas['db'].drop_duplicates([\"uniprot\",'db']).pivot_table(index='org',columns='db',aggfunc='size')\n",
      "39/111: tablas['db'].drop_duplicates([\"uniprot\",'db']).org.value_counts()\n",
      "39/112: tablas['db'].drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "39/113: tablas['db'].drop_duplicates([\"uniprot\",'db']).org.value_counts()[:6]\n",
      "39/114: tablas['db'].drop_duplicates([\"uniprot\",'db']).org.value_counts()[:6].index\n",
      "39/115: tablas['db'].drop_duplicates([\"uniprot\",'db']).org.value_counts()[:6].index.list()\n",
      "39/116: tablas['db'].drop_duplicates([\"uniprot\",'db']).org.value_counts()[:6].index.tolist()\n",
      "39/117: tablas['db'].drop_duplicates([\"uniprot\",'db'])\n",
      "39/118: tablas['db'].drop_duplicates([\"uniprot\",'db']).org.value_counts()\n",
      "39/119: tablas['db'].drop_duplicates([\"uniprot\",'db']).org.value_counts()[:10].index.tolist()\n",
      "39/120: db\n",
      "39/121: db = tablas['db'].copy()\n",
      "39/122:\n",
      "db.org_short = db.org\n",
      "db.loc[~db.org.isin(tablas['db'].drop_duplicates([\"uniprot\",'db']).org.value_counts()[:10].index.tolist()),'org_short'] = 'Other'\n",
      "39/123: tablas['db'].drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "39/124: db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "39/125:\n",
      "orgs = db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "orgs['suma'] = orgs.sum(1)\n",
      "orgs = orgs.sort_values('suma',ascending=False)\n",
      "39/126: orgs\n",
      "39/127: orgs.astype(int)\n",
      "39/128: orgs.fillna(0).astype(int)\n",
      "39/129: tablas['db'].drop_duplicates([\"uniprot\",'db']).org.value_counts()[:10].index.tolist()\n",
      "39/130: tablas['db']\n",
      "39/131: db = tablas['db'].copy()\n",
      "39/132: orgs = db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "39/133: orgs.fillna(0).astype(int)\n",
      "39/134: db.loc[db.org == \"Arabidopsis thaliana\",'org_short'] = \"Arabidopsis thaliana\"\n",
      "39/135: orgs = db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "39/136: orgs.fillna(0).astype(int)\n",
      "39/137:\n",
      "db.loc[db.org == \"Arabidopsis thaliana\",'org_short'] = \"Arabidopsis thaliana\"\n",
      "db.loc[db.org == \"other\",'org_short'] = \"Other\"\n",
      "39/138: orgs = db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "39/139: orgs.index = orgs.index.map(lamda)\n",
      "39/140: orgs.fillna(0).astype(int)\n",
      "39/141:\n",
      "db.loc[db.org == \"Arabidopsis thaliana\",'org_short'] = \"Arabidopsis thaliana\"\n",
      "db.loc[db.org == \"Other\",'org_short'] = \"Other\"\n",
      "39/142: orgs = db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "39/143: orgs.fillna(0).astype(int)\n",
      "39/144: db.org_short.str.Capitalize()\n",
      "39/145: db.org_short.str.capitalize()\n",
      "39/146: db.org_short = db.org_short.str.capitalize()\n",
      "39/147: db.loc[db.org == \"Arabidopsis thaliana\",'org_short'] = \"Arabidopsis thaliana\"\n",
      "39/148: orgs = db.drop_duplicates([\"uniprot\",'db']).pivot_table(index='org_short',columns='db',aggfunc='size')\n",
      "39/149: orgs.fillna(0).astype(int)\n",
      "39/150: orgs.fillna(0).astype(int)[['phasepro','drllps_scaffolds','drllps_clients','drllps_regulators','phasepdb_lt','phasepdb_ht']]\n",
      "39/151: orgs = orgs.fillna(0).astype(int)[['phasepro','drllps_scaffolds','drllps_clients','drllps_regulators','phasepdb_lt','phasepdb_ht']]\n",
      "39/152: orgs\n",
      "39/153: orgs.sort_values('phasepro')\n",
      "39/154: orgs['suma'] = orgs.sum(1)\n",
      "39/155: orgs.sort_values('suma',ascending=False)\n",
      "39/156: orgs.sort_values('drllps_scaffolds',ascending=False)\n",
      "39/157: orgs.sort_values('drllps_scaffolds',ascending=False).index.tolist()\n",
      "39/158:\n",
      "org_orden = ['Homo sapiens',\n",
      " 'Saccharomyces cerevisiae',\n",
      " 'Caenorhabditis elegans',\n",
      " 'Mus musculus',\n",
      "\n",
      " 'Arabidopsis thaliana',\n",
      " 'Drosophila melanogaster',\n",
      " 'Xenopus laevis', 'Other',]\n",
      "39/159: orgs.reindex(org_orden)\n",
      "39/160: orgs = orgs.reindex(org_orden)\n",
      "39/161: orgs.columns\n",
      "39/162: del orgs['suma']\n",
      "39/163:\n",
      "orgs.columns = ['PhasePro', 'Drllps Scaffolds', 'DrLLPS Clients', 'DrLLPS Regulators',\n",
      "       'PhaSepDB_LT', 'PhaSepDB_HT']\n",
      "39/164: orgs\n",
      "39/165: db.drop_duplicates([\"uniprot\"]).org_short.value_counts()\n",
      "39/166: db.drop_duplicates([\"uniprot\"]).org_short.value_counts()[org_orden]\n",
      "39/167: db.drop_duplicates([\"uniprot\"]).org_short.value_counts()[org_orden].values\n",
      "39/168: db.drop_duplicates([\"uniprot\"]).org_short.value_counts()[org_orden]\n",
      "39/169: tablas['human_boxes']\n",
      "39/170: tablas['human_boxes'].box3\n",
      "39/171: tablas['human_boxes'].query('box == \"box3\"')\n",
      "39/172: print(tablas['info'])\n",
      "39/173: tablas['db'].query('org == \"Homo sapiens\"').query('db == \"phasepdb_ht\"')#.tablas['human_boxes'].query('box == \"box3\"')\n",
      "39/174: tablas['db'].query('org == \"Homo sapiens\"').query('db == \"phasepdb_ht\"').drop_duplicates(\"uniprot\")#.tablas['human_boxes'].query('box == \"box3\"')\n",
      "39/175: tablas['db'].query('org == \"Homo sapiens\"').query('db == \"phasepdb_ht\"').drop_duplicates(\"uniprot\").merge(tablas['human_boxes'].query('box == \"box3\"'))\n",
      "39/176: tablas['db'].query('org == \"Homo sapiens\"').query('db == \"phasepdb_ht\"').query('mlo == \"null_phasepdb_ht\"').drop_duplicates(\"uniprot\").merge(tablas['human_boxes'].query('box == \"box3\"'))\n",
      "39/177: tablas['db'].query('org == \"Homo sapiens\"').query('db == \"phasepdb_ht\"').query('mlo == \"null_phasepdb_ht\"')#.drop_duplicates(\"uniprot\").merge(tablas['human_boxes'].query('box == \"box3\"'))\n",
      "39/178: tablas['db'].query('org == \"Homo sapiens\"').query('db == \"phasepdb_ht\"').query('mlo == \"null_phasepdb_ht\"').drop_duplicates(\"uniprot\")#.drop_duplicates(\"uniprot\").merge(tablas['human_boxes'].query('box == \"box3\"'))\n",
      "39/179: tablas['human_box1']\n",
      "39/180: b1 = tablas['human_box1']\n",
      "39/181: b1['prot'] = b1.uniprot.map(tablas['gene_dict_names'])\n",
      "39/182: b1['prot'] = b1.uniprot.map(tablas['gene_names_dict'])\n",
      "39/183: for i in b1.query('web == phasepdb').prot.unique():print(i)\n",
      "39/184: for i in b1.query('web == \"phasepdb\"').prot.unique():print(i)\n",
      "39/185: db.uniprot = db.uniprot.str.strip()\n",
      "39/186: b1['prot'] = b1.uniprot.map(tablas['gene_names_dict'])\n",
      "39/187: for i in b1.query('web == \"phasepdb\"').prot.unique():print(i)\n",
      "39/188: len(b1.query('web == \"phasepdb\"').prot.unique())\n",
      "39/189: for i in b1.query('web == \"phasepro\"').prot.unique():print(i)\n",
      "39/190: for i in b1.query('web == \"drllps_scaffolds\"').prot.unique():print(i)\n",
      "39/191: for i in b1.query('web == \"drllps\"').prot.unique():print(i)\n",
      "39/192:\n",
      "drllps = [\"DDX1\",\"RPL23A\",\"KPNA2\",\"RPL5\",\"PTBP1\",\"XPO1\",\"SGO1\",\"RARA\",\"ITSN1\",\"SUMO3\"]\n",
      "phasepro = [\"PRNP\",\"ZFP36L1\",\"GATA3\",\"ESR1\",\"U2AF2\",\"YTHDF1\",\"YTHDF3\",\"CPEB3\"]\n",
      "phasepdb_drllps = [\"C9orf72\",\"G3BP1\",\"COIL\",\"PSPC1\",\"SYNGAP1\",\"SRSF2\",\"RBFOX1\",\"CIRBP\",\"DAZAP1\",\"HNRNPA1L2\",\"HNRNPH1\",\"HNRNPH3\",\"RBM3\",\"KPNB1\",\"ELAVL1\",\"PIAS2\"\n",
      ",\"TP53\",\"FYN\",\"SURF6\",\"GNL2\",\"MAP1LC3B\",\"CSTF2\",\"CSTF2T\",\"TIAL1\",\"HNRNPA3\",\"HNRNPH2\",\"HNRNPDL\",\"HNRNPA0\",\"NUP153\",\"AXIN1\",\"CPEB2\",\"ZNF207\"]\n",
      "phasepdb_phasepro = [\"NCK1\",\"ELN\",\"HSPB2\",\"LGALS3\",\"LAT\",\"SOS1\",\"NONO\",\"SFPQ\",\"CGAS\",\"MED1\",\"BRD4\",\"KHDRBS1\",\"DYRK1A\",\"DYRK3\",\"MATR3\",\"YTHDF2\",\"TP53BP1\",\"CCNT1\",\"SPOP\",\"MORC3\"]\n",
      "las3 = [\"FUS\",\"FMR1\",\"TAF15\",\"TARDBP\",\"EWSR1\",\"HNRNPA1\",\"HNRNPA2B1\",\"TIA1\",\"NPHS1\",\"WASL\",\"DDX4\",\"DDX3X\",\"RBM14\",\"CBX5\",\"NPM1\",\"UBQLN2\",\"MAPT\",\"TNRC6B\",\"GRB2\",\"HNRNPD\",\"POLR2A\",\"CBX2\",\"PML\",\"DLG4\",\"AGO2\",\"SYN1\",\"SYN2\",\"NUP98\",\"SQSTM1\",\"DAXX\"]\n",
      "39/193: for i in sorted(phasepro):print(i)\n",
      "39/194: for i in sorted(drllps):print(i)\n",
      "39/195: for i in sorted(phasepdb_phasepro):print(i)\n",
      "39/196: for i in sorted(las3):print(i)\n",
      "39/197: for i in sorted(phasepdb_drllps):print(i)\n",
      "43/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "43/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "43/3:\n",
      "import pandas as pd\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "43/4: db = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo_loc'])\n",
      "43/5: db['gene_name'] = db.uniprot.map(tablas['gene_names_dict'])\n",
      "43/6: db.mlo_loc = db.mlo_loc.map(tablas['translate'])\n",
      "43/7:\n",
      "basesdatos = tablas['human_box1'].drop_duplicates([\"uniprot\",'web'])\n",
      "basesdatos['gene_name'] = basesdatos.uniprot.map(tablas['gene_names_dict'])\n",
      "ndbs = basesdatos.gene_name.value_counts().to_dict()\n",
      "43/8:\n",
      "pivot = db.pivot_table(index='gene_name',columns='mlo_loc',aggfunc='size')\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "43/9: #pivot.index = pivot.index.map(lambda x: x + \" - \" + str(counts_grande[x]))\n",
      "43/10:\n",
      "# calcular numero exacto de MLOS\n",
      "\n",
      "db_grande = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo'])\n",
      "\n",
      "db_grande.mlo_loc = db_grande.mlo_loc.map(tablas['translate'])\n",
      "\n",
      "db_grande['gene_name'] = db_grande.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "counts_grande = db_grande.gene_name.value_counts().to_dict()\n",
      "\n",
      "db_grande.gene_name.value_counts().value_counts()\n",
      "43/11: pivot['ndbs'] = pivot.index.map(ndbs)\n",
      "43/12: columnas = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body', 'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD','Other', 'suma','ndbs']\n",
      "43/13: chico = pivot.query('suma > 1')#[mlos]\n",
      "43/14: chico = pivot.query('suma >= 2')\n",
      "43/15: chico = chico.sort_values('suma',ascending=False)[columnas]\n",
      "43/16:\n",
      "chico4 = chico.query('suma > 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico3 = chico.query('suma == 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico2 = chico.query('suma == 2').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "43/17:\n",
      "chico3omas = chico.query('suma >= 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico2omas = chico.query('suma >= 2').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "43/18:\n",
      "ax = sns.heatmap(chico3.drop(columns=['ndbs','suma']),cmap='Blues')\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "43/19:\n",
      "plt.subplots(figsize=(4,13))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde3.svg')\n",
      "43/20:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/21: tablas['db'].query('org == \"Homo sapiens\"')\n",
      "43/22: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"])\n",
      "43/23: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"]).merge(tablas['human_boxes'])\n",
      "43/24: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"]).merge(tablas['human_boxes'])[\"uniprot\",'mlo_loc','box']\n",
      "43/25: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"]).merge(tablas['human_boxes'])[[\"uniprot\",'mlo_loc','box']]\n",
      "43/26: data = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"]).merge(tablas['human_boxes'])[[\"uniprot\",'mlo_loc','box']]\n",
      "43/27: data['prot'] = data.uniprot.map(tablas['gene_names_dict'])\n",
      "43/28: data\n",
      "43/29: data.pivot_table(index=\"prot\",columns='mlo_loc',values='box')\n",
      "43/30: data.pivot(index=\"prot\",columns='mlo_loc',values='box')\n",
      "43/31: adta\n",
      "43/32: data\n",
      "43/33: data.melt(id_vars='prot')\n",
      "43/34: data.melt(id_vars='prot',value_vars='mlo_loc')\n",
      "43/35: data.pivot_table(index=\"prot\",columns='mlo_loc',values='box')\n",
      "43/36: data.pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=list)\n",
      "43/37: data.pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=list).dropna()\n",
      "43/38: data.pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=list)\n",
      "43/39: data.pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=list)[[\"cajal_body\",\"cytoplasmic_stress_granule\"]]\n",
      "43/40: data.pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=list)[[\"cajal_body\",\"cytoplasmic_stress_granule\"]].dropna()\n",
      "43/41: data.pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/42: pivoted = data.pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/43: pivoted\n",
      "43/44: pivoted.apply(lambda x: len(x), axis=1)\n",
      "43/45: pivoted.apply(lambda x: len(x.isna()), axis=1)\n",
      "43/46: pivoted[\"suma\"] = pivoted.apply(lambda x: len(x) - sum(x.notna()), axis=1)\n",
      "43/47: pivoted\n",
      "43/48: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/49: pivoted\n",
      "43/50: pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/51: pivoted.apply(lambda x: sum(x.notna()), axis=1).value_countsS()\n",
      "43/52: pivoted.apply(lambda x: sum(x.notna()), axis=1).value_counts()\n",
      "43/53: pivoted.iloc[0]\n",
      "43/54: del pivoted['suma']\n",
      "43/55: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/56: pivoted\n",
      "43/57: pivoted.sort_values('suma',ascending=False)\n",
      "43/58: pivoted = data.sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/59: del pivoted['suma']\n",
      "43/60: pivoted.sort_values('suma',ascending=False)\n",
      "43/61: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/62: pivoted.sort_values('suma',ascending=False)\n",
      "43/63: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == \"box1\"]))\n",
      "43/64: pivoted.sort_values('suma_box1',ascending=False)\n",
      "43/65: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == \"box1\"]),axis1)\n",
      "43/66: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == \"box1\"]),axis = 1)\n",
      "43/67: pivoted.sort_values('suma_box1',ascending=False)\n",
      "43/68: pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2')\n",
      "43/69: data\n",
      "43/70: data.box.replace('box','')\n",
      "43/71: data.box.str[-1]\n",
      "43/72: data.box.str[-1].astype(int)\n",
      "43/73: data.box = data.box.str[-1].astype(int)\n",
      "43/74: pivoted = data.sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/75: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/76: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/77: pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2')\n",
      "43/78: pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2')[\"NONO\"]\n",
      "43/79: pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2').loc[\"NONO\"]\n",
      "43/80: pivoted.sort_values('suma_box1',ascending=False).loc[\"NONO\"]\n",
      "43/81: pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2')\n",
      "43/82: pivoted = data.sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/83: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/84: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/85: pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2')\n",
      "43/86: pivoted = data.sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/87: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/88: pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2')\n",
      "43/89: to_plot = pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2')\n",
      "43/90: ax = sns.heatmap(to_plot.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "43/91: ax = sns.heatmap(to_plot.drop(columns=['suma_box1']),cmap='Blues',lw=1.5)\n",
      "43/92: ax = sns.heatmap(to_plot.drop(columns=['suma_box1']),lw=1.5)\n",
      "43/93: ax = sns.heatmap(to_plot.drop(columns=['suma_box1']))\n",
      "43/94: ax = sns.heatmap(to_plot.drop(columns=['suma_box1']), cmap=['blue','green','red'])\n",
      "43/95: pivoted = data.query('mlo_loc != \"other\"').sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/96: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/97: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/98: to_plot = pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2')\n",
      "43/99: ax = sns.heatmap(to_plot.drop(columns=['suma_box1']), cmap=['blue','green','red'])\n",
      "43/100: to_plot\n",
      "43/101: pivoted = data.query('mlo_loc != \"other\"').sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/102: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/103: #pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/104: to_plot = pivoted.sort_values('suma_box1',ascending=False).query('suma_box1 >= 2')\n",
      "43/105: ax = sns.heatmap(to_plot.drop(columns=['suma_box1']), cmap=['blue','green','red'])\n",
      "43/106:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).drop(columns=['suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/107:\n",
      "orden = ['suma', 'Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD', 'Other',\n",
      "       ]\n",
      "43/108:\n",
      "orden = ['suma', 'Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\n",
      "       ]\n",
      "43/109:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.sort_values(orden,ascending=False).drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/110:\n",
      "orden = ['suma', 'Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\n",
      "       ]\n",
      "43/111:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).drop(columns=['suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/112:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).drop(columns=['suma_box1']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/113:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).drop(columns=['suma_box1']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/114: to_plot\n",
      "43/115:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.drop(columns=['suma_box1']).sort_values(orden,ascending=False),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/116:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/117:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False),cmap='Blues',lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/118:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap='Blues',lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/119: del to_plot['suma']\n",
      "43/120: del to_plot['suma_box1']\n",
      "43/121:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap='Blues',lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/122:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.fillna(0),cmap='Blues',lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/123:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.fillna(0),cmap=[\"blue\",'green','red'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/124:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"blue\",'green','red'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/125: pivoted = data.query('mlo_loc != \"other\"').sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/126: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/127: to_plot = pivoted.sort_values(['suma_box1','index'],ascending=[False,True]).query('suma_box1 >= 2').sort_values([])\n",
      "43/128:  pivoted.reset_index()\n",
      "43/129:\n",
      "orden = ['suma', 'Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\n",
      "       ]\n",
      "43/130: to_plot = pivoted.reset_index().sort_values(['suma_box1','prot']),ascending=[False,True]).set_index(\"prot\").query('suma_box1 >= 2')[orden]\n",
      "43/131: to_plot = pivoted.reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").query('suma_box1 >= 2')[orden]\n",
      "43/132:\n",
      "to_plot = pivoted.reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").query('suma_box1 >= 2')\n",
      "to_plot.index = to_plot.index.map(tablas['translate'])\n",
      "43/133: to_plot\n",
      "43/134:\n",
      "to_plot = pivoted.reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").query('suma_box1 >= 2')\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "43/135: to_plot\n",
      "43/136:\n",
      "to_plot = pivoted.reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").query('suma_box1 >= 2')\n",
      "del to_plot['suma_box1']\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "43/137: to_plot\n",
      "43/138:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"blue\",'green','red'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/139:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"blue\",'green','red'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/140:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/141:\n",
      "orden = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\n",
      "       ]\n",
      "43/142:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot[orden],cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/143: data = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"]).merge(tablas['human_boxes'])[[\"uniprot\",'mlo_loc','box']]\n",
      "43/144: data['prot'] = data.uniprot.map(tablas['gene_names_dict'])\n",
      "43/145: data.box = data.box.str[-1].astype(int)\n",
      "43/146: pivoted = data.query('mlo_loc != \"other\"').sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/147: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/148:\n",
      "to_plot = pivoted.reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").query('suma_box1 >= 2')\n",
      "del to_plot['suma_box1']\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "43/149:\n",
      "to_plot = pivoted.reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").query('suma_box1 >= 2')\n",
      "del to_plot['suma']\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "43/150:\n",
      "to_plot = pivoted.reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").query('suma >= 2')\n",
      "del to_plot['suma']\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "43/151:\n",
      "orden = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\n",
      "       ]\n",
      "43/152:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot[orden],cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/153: to_plot\n",
      "43/154:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot[orden]filna(),cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/155:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot[orden].fillna(0),cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/156: to_plot\n",
      "43/157: data = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"]).merge(tablas['human_boxes'])[[\"uniprot\",'mlo_loc','box']]\n",
      "43/158: data['prot'] = data.uniprot.map(tablas['gene_names_dict'])\n",
      "43/159: data.box = data.box.str[-1].astype(int)\n",
      "43/160: pivoted = data.query('mlo_loc != \"other\"').sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/161: pivoted\n",
      "43/162: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/163: to_plot = pivoted.reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").query('suma >= 2')\n",
      "43/164: to_plot\n",
      "43/165: to_plot.query('suma >= 3')\n",
      "43/166: to_plot.query('suma >= 4')\n",
      "43/167:\n",
      "del to_plot['suma']\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "43/168:\n",
      "orden = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\n",
      "       ]\n",
      "43/169: to_plot\n",
      "43/170: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/171: to_plot = pivoted.reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").query('suma >= 42')\n",
      "43/172: to_plot.query('suma >= 4')\n",
      "43/173: to_plot = pivoted.reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").query('suma >= 4')\n",
      "43/174: to_plot.query('suma >= 4')\n",
      "43/175: to_plot = pivoted.reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").query('suma >= 5')\n",
      "43/176: to_plot.query('suma >= 4')\n",
      "43/177:\n",
      "orden = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\n",
      "       ]\n",
      "43/178: to_plot\n",
      "43/179:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot[orden].fillna(0),cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/180:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.fillna(0),cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/181:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/182:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.drop(columns='suma'),cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/183: data = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"]).merge(tablas['human_boxes'])[[\"uniprot\",'mlo_loc','box']]\n",
      "43/184: data['prot'] = data.uniprot.map(tablas['gene_names_dict'])\n",
      "43/185: data.box = data.box.str[-1].astype(int)\n",
      "43/186: pivoted = data.query('mlo_loc != \"other\"').sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/187: pivoted\n",
      "43/188: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/189: pivoted['suma_box1'] = pivoted.apply(lambda x: len([i for i in x if i == 1]) - 1,axis = 1)\n",
      "43/190: pivoted\n",
      "43/191: pivoted.sort_values('suma_box1')\n",
      "43/192: pivoted['suma_box1'] = pivoted.drop(columns='sua').apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/193: pivoted['suma_box1'] = pivoted.drop(columns='suma').apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/194: pivoted.sort_values('suma_box1')\n",
      "43/195: pivoted.query('suma_box1 > 1')\n",
      "43/196: pivoted = data.query('mlo_loc != \"other\"').sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/197: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/198: pivoted['suma_box1'] = pivoted.drop(columns='suma').apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/199: pivoted.query('suma_box1 > 1')\n",
      "43/200: pivoted.query('suma_box1 > 1')\n",
      "43/201:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").drop(colums=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translaste'])\n",
      "43/202:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translaste'])\n",
      "to_plot[orden]\n",
      "43/203:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot[orden]\n",
      "43/204:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.drop(columns='suma'),cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/205:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/206:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/207:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/208:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/209:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/210:\n",
      "to_plot = pivoted.reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/211:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/212:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/213:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/214:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot,cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/215:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot.fillna(0),cmap=['w',\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/216: to_plot\n",
      "43/217:\n",
      "to_plot = pivoted.reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/218:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot.fillna(0),cmap=['w',\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/219:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot.query('suma> 1').fillna(0),cmap=['w',\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/220:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot.query('suma > 1').fillna(0),cmap=['w',\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/221:\n",
      "to_plot = pivoted.reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/222:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot.query('suma > 1').fillna(0),cmap=['w',\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/223:\n",
      "to_plot = pivoted.query('suma > 1').reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/224:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot.fillna(0),cmap=['w',\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/225:\n",
      "#plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot.fillna(0),cmap=['w',\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/226:\n",
      "#plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot.fillna(0),cmap=['w',\"#6681b2ff\",'#7eb57bff','#bc7979ff'])\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/227:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/228:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot,cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/229:\n",
      "#plt.subplots(figsize=(4,25))\n",
      "ax = sns.clustermap(to_plot.fillna(0),cmap=['w',\"#6681b2ff\",'#7eb57bff','#bc7979ff'])\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/230:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False),cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'],lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/231: to_plot\n",
      "43/232: to_plot.applymap(lambda x: -x)\n",
      "43/233: to_plot = to_plot.applymap(lambda x: -x)\n",
      "43/234:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False),cmap=reversed([\"#6681b2ff\",'#7eb57bff','#bc7979ff']),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/235:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False),cmap=[\"#6681b2ff\",'#7eb57bff','#bc7979ff'].reverse(),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/236:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False),cmap=list(reversed([\"#6681b2ff\",'#7eb57bff','#bc7979ff'])),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/237: data = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"]).merge(tablas['human_boxes'])[[\"uniprot\",'mlo_loc','box']]\n",
      "43/238: data['prot'] = data.uniprot.map(tablas['gene_names_dict'])\n",
      "43/239: data.box = data.box.str[-1].astype(int)\n",
      "43/240: pivoted = data.sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "43/241: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "43/242: pivoted['suma_box1'] = pivoted.drop(columns='suma').apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "43/243: pivoted.query('suma_box1 > 1')\n",
      "43/244:\n",
      "orden = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\n",
      "       ]\n",
      "43/245:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/246: to_plot = to_plot.applymap(lambda x: -x)\n",
      "43/247:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False),cmap=list(reversed([\"#6681b2ff\",'#7eb57bff','#bc7979ff'])),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/248:\n",
      "orden = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\"Other\"\n",
      "       ]\n",
      "43/249:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/250: to_plot = to_plot.applymap(lambda x: -x)\n",
      "43/251:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False),cmap=list(reversed([\"#6681b2ff\",'#7eb57bff','#bc7979ff'])),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/252: to_plot\n",
      "43/253: to_plot.applymap(lambda x: 1 if x.notna())\n",
      "43/254: to_plot.applymap(lambda x: 1 if x.notna())\n",
      "43/255: to_plot.applymap(lambda x: x)\n",
      "43/256: to_plot.applymap(lambda x: True if x.notna())\n",
      "43/257: to_plot.applymap(lambda x: \"True\" if x.notna())\n",
      "43/258: to_plot.applymap(lambda x: \"True\" if x == 0)\n",
      "43/259:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "43/260: to_plot.applymap(bool)\n",
      "43/261: to_plot.applymap(lambda x: x.notna())\n",
      "43/262: to_plot.applymap(lambda x: x.notnull())\n",
      "43/263: to_plot\n",
      "43/264: to_plot.applymap(lambda x: x if x > 0)\n",
      "43/265: to_plot.applymap(lambda x: 1 if isinstance(x, int) else x)\n",
      "43/266:\n",
      "df_sort = to_plot.applymap(lambda x: 1 if isinstance(x, int) else x)\n",
      "df_sort.sort_values(orden,ascending=False)\n",
      "43/267: to_plot\n",
      "43/268: to_plot.applymap(int)\n",
      "43/269: to_plot\n",
      "43/270: to_plot.iloc[0,-2]\n",
      "43/271: type(to_plot.iloc[0,-2])\n",
      "43/272: to_plot.iloc[0,-2].isna()\n",
      "43/273: to_plot.iloc[0,-2].isnull()\n",
      "43/274: chimas2\n",
      "43/275: chico2omas\n",
      "43/276: chico2omas.index\n",
      "43/277:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).reindex(chico2omas.index),cmap=list(reversed([\"#6681b2ff\",'#7eb57bff','#bc7979ff'])),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/278:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False)loc[chico2omas.index],cmap=list(reversed([\"#6681b2ff\",'#7eb57bff','#bc7979ff'])),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/279:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).loc[chico2omas.index],cmap=list(reversed([\"#6681b2ff\",'#7eb57bff','#bc7979ff'])),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/280:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).loc[chico2omas.index.tolist()],cmap=list(reversed([\"#6681b2ff\",'#7eb57bff','#bc7979ff'])),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/281: chico2omas.index.tolist()\n",
      "43/282:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).loc[chico2omas.sort_values(orden,ascending=False).index],cmap=list([\"#6681b2ff\",'#7eb57bff','#bc7979ff']),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/283:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).loc[chico2omas.sort_values(orden,ascending=False).index.tolist()],cmap=list([\"#6681b2ff\",'#7eb57bff','#bc7979ff']),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/284:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).reindex(chico2omas.sort_values(orden,ascending=False).index.tolist()),cmap=list([\"#6681b2ff\",'#7eb57bff','#bc7979ff']),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "43/285:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.sort_values(orden,ascending=False).drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/286:\n",
      "import pandas as pd\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "43/287: db = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo_loc'])\n",
      "43/288: db['gene_name'] = db.uniprot.map(tablas['gene_names_dict'])\n",
      "43/289: db.mlo_loc = db.mlo_loc.map(tablas['translate'])\n",
      "43/290:\n",
      "basesdatos = tablas['human_box1'].drop_duplicates([\"uniprot\",'web'])\n",
      "basesdatos['gene_name'] = basesdatos.uniprot.map(tablas['gene_names_dict'])\n",
      "ndbs = basesdatos.gene_name.value_counts().to_dict()\n",
      "43/291:\n",
      "pivot = db.pivot_table(index='gene_name',columns='mlo_loc',aggfunc='size')\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "43/292: #pivot.index = pivot.index.map(lambda x: x + \" - \" + str(counts_grande[x]))\n",
      "43/293:\n",
      "# calcular numero exacto de MLOS\n",
      "\n",
      "db_grande = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo'])\n",
      "\n",
      "db_grande.mlo_loc = db_grande.mlo_loc.map(tablas['translate'])\n",
      "\n",
      "db_grande['gene_name'] = db_grande.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "counts_grande = db_grande.gene_name.value_counts().to_dict()\n",
      "\n",
      "db_grande.gene_name.value_counts().value_counts()\n",
      "43/294: pivot['ndbs'] = pivot.index.map(ndbs)\n",
      "43/295: columnas = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body', 'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD','Other', 'suma','ndbs']\n",
      "43/296: chico = pivot.query('suma > 1')#[mlos]\n",
      "43/297: chico = pivot.query('suma >= 2')\n",
      "43/298: chico = chico.sort_values('suma',ascending=False)[columnas]\n",
      "43/299:\n",
      "chico4 = chico.query('suma > 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico3 = chico.query('suma == 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico2 = chico.query('suma == 2').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "43/300:\n",
      "chico3omas = chico.query('suma >= 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico2omas = chico.query('suma >= 2').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "43/301:\n",
      "ax = sns.heatmap(chico3.drop(columns=['ndbs','suma']),cmap='Blues')\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "43/302:\n",
      "plt.subplots(figsize=(5,4))\n",
      "ax = sns.heatmap(chico4.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('hetamap_4.svg')\n",
      "43/303:\n",
      "plt.subplots(figsize=(4,13))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde3.svg')\n",
      "43/304:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/305:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.sort_values(orden,ascending=False).drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "43/306:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.sort_values(orden,ascending=False).drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "50/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "50/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "50/3:\n",
      "import pandas as pd\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "50/4: db = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo_loc'])\n",
      "50/5: db['gene_name'] = db.uniprot.map(tablas['gene_names_dict'])\n",
      "50/6: db.mlo_loc = db.mlo_loc.map(tablas['translate'])\n",
      "50/7:\n",
      "basesdatos = tablas['human_box1'].drop_duplicates([\"uniprot\",'web'])\n",
      "basesdatos['gene_name'] = basesdatos.uniprot.map(tablas['gene_names_dict'])\n",
      "ndbs = basesdatos.gene_name.value_counts().to_dict()\n",
      "50/8:\n",
      "pivot = db.pivot_table(index='gene_name',columns='mlo_loc',aggfunc='size')\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "50/9: #pivot.index = pivot.index.map(lambda x: x + \" - \" + str(counts_grande[x]))\n",
      "50/10:\n",
      "# calcular numero exacto de MLOS\n",
      "\n",
      "db_grande = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo'])\n",
      "\n",
      "db_grande.mlo_loc = db_grande.mlo_loc.map(tablas['translate'])\n",
      "\n",
      "db_grande['gene_name'] = db_grande.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "counts_grande = db_grande.gene_name.value_counts().to_dict()\n",
      "\n",
      "db_grande.gene_name.value_counts().value_counts()\n",
      "50/11: pivot['ndbs'] = pivot.index.map(ndbs)\n",
      "50/12: columnas = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body', 'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD','Other', 'suma','ndbs']\n",
      "50/13: chico = pivot.query('suma > 1')#[mlos]\n",
      "50/14: chico = pivot.query('suma >= 2')\n",
      "50/15: chico = chico.sort_values('suma',ascending=False)[columnas]\n",
      "50/16:\n",
      "chico4 = chico.query('suma > 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico3 = chico.query('suma == 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico2 = chico.query('suma == 2').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "50/17:\n",
      "chico3omas = chico.query('suma >= 3').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "chico2omas = chico.query('suma >= 2').reset_index().sort_values(['suma','gene_name'],ascending=[False,True]).set_index(\"gene_name\")\n",
      "50/18:\n",
      "ax = sns.heatmap(chico3.drop(columns=['ndbs','suma']),cmap='Blues')\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "50/19:\n",
      "plt.subplots(figsize=(5,4))\n",
      "ax = sns.heatmap(chico4.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('hetamap_4.svg')\n",
      "50/20:\n",
      "plt.subplots(figsize=(4,13))\n",
      "ax = sns.heatmap(chico3omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde3.svg')\n",
      "50/21:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "50/22:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.sort_values(orden,ascending=False).drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "50/23:\n",
      "orden = [\"suma\",'Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\"Other\"\n",
      "       ]\n",
      "50/24:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(chico2omas.sort_values(orden,ascending=False).drop(columns=['ndbs','suma']),cmap='Blues',lw=1.5)\n",
      "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "#plt.savefig('../paper/prot_mlo_masde2.svg')\n",
      "50/25: data = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates(['uniprot',\"mlo_loc\"]).merge(tablas['human_boxes'])[[\"uniprot\",'mlo_loc','box']]\n",
      "50/26: data['prot'] = data.uniprot.map(tablas['gene_names_dict'])\n",
      "50/27: data.box = data.box.str[-1].astype(int)\n",
      "50/28: pivoted = data.sort_values('box').pivot_table(index=\"prot\",columns='mlo_loc',values='box',aggfunc=\"first\")\n",
      "50/29: pivoted[\"suma\"] = pivoted.apply(lambda x: sum(x.notna()), axis=1)\n",
      "50/30: pivoted['suma_box1'] = pivoted.drop(columns='suma').apply(lambda x: len([i for i in x if i == 1]),axis = 1)\n",
      "50/31: pivoted.query('suma_box1 > 1')\n",
      "50/32:\n",
      "orden = ['Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\"Other\"\n",
      "       ]\n",
      "50/33:\n",
      "to_plot = pivoted.query('suma_box1 > 1').reset_index().sort_values(['suma_box1','prot'],ascending=[False,True]).set_index(\"prot\").drop(columns=['suma','suma_box1'])\n",
      "to_plot.columns = to_plot.columns.map(tablas['translate'])\n",
      "to_plot = to_plot[orden]\n",
      "50/34:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).reindex(chico2omas.sort_values(orden,ascending=False).index.tolist()),cmap=list([\"#6681b2ff\",'#7eb57bff','#bc7979ff']),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "50/35:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.sort_values(orden,ascending=False).loc[chico2omas.sort_values(orden,ascending=False).index.tolist()],cmap=list([\"#6681b2ff\",'#7eb57bff','#bc7979ff']),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "50/36:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.loc[chico2omas.sort_values(orden,ascending=False).index.tolist()],cmap=list([\"#6681b2ff\",'#7eb57bff','#bc7979ff']),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "50/37:\n",
      "orden = [\"suma\",'Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\"Other\"\n",
      "       ]\n",
      "50/38:\n",
      "orden = [\"suma\",'Nucleolus', 'Nuclear Speckles', 'Stress Granule', 'P-body',\n",
      "       'Paraspeckles', 'PML body', 'Cajal Body', 'NPC', 'PSD',\"Other\"\n",
      "       ]\n",
      "50/39:\n",
      "plt.subplots(figsize=(4,25))\n",
      "ax = sns.heatmap(to_plot.loc[chico2omas.sort_values(orden,ascending=False).index.tolist()],cmap=list([\"#6681b2ff\",'#7eb57bff','#bc7979ff']),lw=1.5)\n",
      "#ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right')\n",
      "\n",
      "plt.ylabel('Protein');\n",
      "plt.xlabel('MLO');\n",
      "plt.savefig('../paper/prot_mlo_noother_boxes.svg')\n",
      "50/40: to_plot.loc[chico2omas.sort_values(orden,ascending=False).index.tolist()]\n",
      "50/41:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.gridspec as gridspec\n",
      "import numpy as np\n",
      "\n",
      "dominios = dominios.query('box == \"box1\"').query('org == \"Homo sapiens\"')\n",
      "dominios['mlo'] = dominios['mlo_loc']\n",
      "dominios = dominios.merge(tidy[['uniprot', 'dc']], how=\"left\")\n",
      "\n",
      "def plot_heatmap_domains(dominios):\n",
      "\n",
      "    dominios_top = []\n",
      "    dominios_top_dic = {}\n",
      "    for mlo, df in dominios.groupby('mlo'):\n",
      "        count =  df.drop_duplicates(['uniprot', 'domain']).domain.value_counts()\n",
      "        for dom in count[count > 1].index[:5]:\n",
      "    #        print(mlo, '    ', dom)\n",
      "            dominios_top.append(dom)\n",
      "        dominios_top_dic[mlo] = count[count > 1].index[:5].tolist()\n",
      "\n",
      "    dominios_top = list(set(dominios_top))\n",
      "\n",
      "    # orden de los graficos\n",
      "    orden_mlos = list(dominios.drop_duplicates([\"uniprot\", 'mlo']).mlo.value_counts().index)\n",
      "    orden_mlos.append(orden_mlos.pop(orden_mlos.index(\"other\")))\n",
      "    orden_dominios = dominios[dominios.domain.isin(dominios_top)].drop_duplicates([\"uniprot\",'domain']).domain.value_counts().index\n",
      "\n",
      "\n",
      "    # Heatmap\n",
      "    hm = dominios[dominios.domain.isin(dominios_top)].drop_duplicates([\"uniprot\",'mlo', 'domain']).pivot_table(index='domain', columns='mlo', aggfunc='count')['end'].T\n",
      "    hm = hm[orden_dominios].reindex(orden_mlos)\n",
      "\n",
      "    dominios['dis_bool'] = dominios.dc.map(lambda x: 'H-D' if x >= 0.7 else ( 'S' if x < 0.30 else 'D'))\n",
      "    total_bar = (dominios[dominios.domain.isin(dominios_top)].drop_duplicates([\"uniprot\",'domain'])[['domain', 'dis_bool', 'uniprot']]\n",
      "                .pivot_table(index='domain', columns=['dis_bool'], aggfunc='count')[\"uniprot\"]\n",
      "                .reindex(orden_dominios))\n",
      "\n",
      "    hm_dup = dominios[dominios.domain.isin(dominios_top)].pivot_table(index='domain', columns='mlo', aggfunc='count')['end'].T\n",
      "    hm_dup = hm_dup[orden_dominios].reindex(orden_mlos)\n",
      "\n",
      "    def annotate_heatmap2(im, data=None, valfmt=\"{x:.2f}\",\n",
      "                        textcolors=[\"#2e2e2e\", \"white\"],offset=0,\n",
      "                        threshold=None, **textkw):\n",
      "\n",
      "        if not isinstance(data, (list, np.ndarray)):\n",
      "            data = im.get_array()\n",
      "            data2 = hm_dup.values\n",
      "\n",
      "        # Normalize the threshold to the images color range.\n",
      "        if threshold is not None:\n",
      "            threshold = im.norm(threshold)\n",
      "        else:\n",
      "            threshold = im.norm(data.max())/2.\n",
      "\n",
      "        # Set default alignment to center, but allow it to be\n",
      "        # overwritten by textkw.\n",
      "        kw = dict(horizontalalignment=\"center\",\n",
      "                verticalalignment=\"center\")\n",
      "        kw.update(textkw)\n",
      "\n",
      "        # Get the formatter in case a string is supplied\n",
      "        #if isinstance(valfmt, str):\n",
      "        #    valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
      "        def to_int(number):\n",
      "            try:\n",
      "                return(int(number))\n",
      "            except:\n",
      "                return ''        \n",
      "        # Loop over the data and create a `Text` for each \"pixel\".\n",
      "        # Change the text's color depending on the data.\n",
      "        texts = []\n",
      "        for i in range(data.shape[0]):\n",
      "            for j in range(data.shape[1]):\n",
      "                kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "                text = im.axes.text(j+offset, i+offset, to_int(data[i, j]), **kw, size=12, weight='bold')\n",
      "                texts.append(text)\n",
      "        for i in range(data.shape[0]):\n",
      "            for j in range(data.shape[1]):\n",
      "                kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "                text = im.axes.text(j-offset, i-offset, to_int(data2[i, j]), **kw)\n",
      "                texts.append(text)\n",
      "        return texts\n",
      "\n",
      "        #     Plot\n",
      "    fig = plt.figure(figsize=(20,10))\n",
      "    gs = gridspec.GridSpec(2,1,height_ratios=[10,20])\n",
      "    gs.update(wspace=0.025, hspace=0.05)\n",
      "    # Axes\n",
      "    ax1 = fig.add_subplot(gs[0] )\n",
      "    ax3 = fig.add_subplot(gs[1],sharex= ax1 )\n",
      "    # Plots\n",
      "    total_bar[[\"S\", 'D', 'H-D']].plot.bar(stacked=True, ax=ax1, color=['steelblue','salmon','red'])\n",
      "\n",
      "    for i,v in enumerate(total_bar.sum(axis=1)):\n",
      "        ax1.text(i , v +7, str(int(v)), size=12,horizontalalignment='center')\n",
      "\n",
      "    # heatmap\n",
      "    im = ax3.imshow(hm, cmap=\"GnBu\", aspect='auto')\n",
      "    ax3.set_xticks(np.arange(len(orden_dominios)))\n",
      "    ax3.set_yticks(np.arange(len(orden_mlos)))\n",
      "    ax3.set_xticklabels(orden_dominios)\n",
      "    ax3.set_yticklabels(orden_mlos)\n",
      "    ax3.tick_params(axis='x', rotation=90)\n",
      "    b, t = ax3.get_ylim(); # discover the values for bottom and top\n",
      "    b += 0.5 # Add 0.5 to the bottom\n",
      "    t -= 0.5 # Subtract 0.5 from the top\n",
      "    ax3.set_ylim(b, t) # update the ylim(bottom, top) values\n",
      "    texts = annotate_heatmap2(im, offset=0.2)\n",
      "    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, horizontalalignment='right');\n",
      "    ax3.grid(False)\n",
      "    # Legends\n",
      "    #ax2.legend(title='Disorder',labels = ['Low', 'Middle', 'High'],\n",
      "    #          bbox_to_anchor=(1, 2.5), loc='upper left', borderaxespad=0.\n",
      "    #         )\n",
      "    ax1.legend( labels = ['Low', 'Middle', 'High'])\n",
      "\n",
      "    # Eliminar marcos\n",
      "    ax1.axis('off')\n",
      "    #ax2.axis('off')\n",
      "    ax3.spines['right'].set_visible(False)\n",
      "    ax3.spines['top'].set_visible(False)\n",
      "    ax3.spines['bottom'].set_visible(False)\n",
      "    ax3.spines['left'].set_visible(False)\n",
      "\n",
      "    #return fig\n",
      "50/42:\n",
      "import pandas as pd\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.gridspec as gridspec\n",
      "import numpy as np\n",
      "\n",
      "def plot_heatmap_domains(dominios):\n",
      "\n",
      "    dominios_top = []\n",
      "    dominios_top_dic = {}\n",
      "    for mlo, df in dominios.groupby('mlo'):\n",
      "        count =  df.drop_duplicates(['uniprot', 'domain']).domain.value_counts()\n",
      "        for dom in count[count > 1].index[:5]:\n",
      "    #        print(mlo, '    ', dom)\n",
      "            dominios_top.append(dom)\n",
      "        dominios_top_dic[mlo] = count[count > 1].index[:5].tolist()\n",
      "\n",
      "    dominios_top = list(set(dominios_top))\n",
      "\n",
      "    # orden de los graficos\n",
      "    orden_mlos = list(dominios.drop_duplicates([\"uniprot\", 'mlo']).mlo.value_counts().index)\n",
      "    orden_mlos.append(orden_mlos.pop(orden_mlos.index(\"other\")))\n",
      "    orden_dominios = dominios[dominios.domain.isin(dominios_top)].drop_duplicates([\"uniprot\",'domain']).domain.value_counts().index\n",
      "\n",
      "\n",
      "    # Heatmap\n",
      "    hm = dominios[dominios.domain.isin(dominios_top)].drop_duplicates([\"uniprot\",'mlo', 'domain']).pivot_table(index='domain', columns='mlo', aggfunc='count')['end'].T\n",
      "    hm = hm[orden_dominios].reindex(orden_mlos)\n",
      "\n",
      "    dominios['dis_bool'] = dominios.dc.map(lambda x: 'H-D' if x >= 0.7 else ( 'S' if x < 0.30 else 'D'))\n",
      "    total_bar = (dominios[dominios.domain.isin(dominios_top)].drop_duplicates([\"uniprot\",'domain'])[['domain', 'dis_bool', 'uniprot']]\n",
      "                .pivot_table(index='domain', columns=['dis_bool'], aggfunc='count')[\"uniprot\"]\n",
      "                .reindex(orden_dominios))\n",
      "\n",
      "    hm_dup = dominios[dominios.domain.isin(dominios_top)].pivot_table(index='domain', columns='mlo', aggfunc='count')['end'].T\n",
      "    hm_dup = hm_dup[orden_dominios].reindex(orden_mlos)\n",
      "\n",
      "    def annotate_heatmap2(im, data=None, valfmt=\"{x:.2f}\",\n",
      "                        textcolors=[\"#2e2e2e\", \"white\"],offset=0,\n",
      "                        threshold=None, **textkw):\n",
      "\n",
      "        if not isinstance(data, (list, np.ndarray)):\n",
      "            data = im.get_array()\n",
      "            data2 = hm_dup.values\n",
      "\n",
      "        # Normalize the threshold to the images color range.\n",
      "        if threshold is not None:\n",
      "            threshold = im.norm(threshold)\n",
      "        else:\n",
      "            threshold = im.norm(data.max())/2.\n",
      "\n",
      "        # Set default alignment to center, but allow it to be\n",
      "        # overwritten by textkw.\n",
      "        kw = dict(horizontalalignment=\"center\",\n",
      "                verticalalignment=\"center\")\n",
      "        kw.update(textkw)\n",
      "\n",
      "        # Get the formatter in case a string is supplied\n",
      "        #if isinstance(valfmt, str):\n",
      "        #    valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
      "        def to_int(number):\n",
      "            try:\n",
      "                return(int(number))\n",
      "            except:\n",
      "                return ''        \n",
      "        # Loop over the data and create a `Text` for each \"pixel\".\n",
      "        # Change the text's color depending on the data.\n",
      "        texts = []\n",
      "        for i in range(data.shape[0]):\n",
      "            for j in range(data.shape[1]):\n",
      "                kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "                text = im.axes.text(j+offset, i+offset, to_int(data[i, j]), **kw, size=12, weight='bold')\n",
      "                texts.append(text)\n",
      "        for i in range(data.shape[0]):\n",
      "            for j in range(data.shape[1]):\n",
      "                kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "                text = im.axes.text(j-offset, i-offset, to_int(data2[i, j]), **kw)\n",
      "                texts.append(text)\n",
      "        return texts\n",
      "\n",
      "        #     Plot\n",
      "    fig = plt.figure(figsize=(20,10))\n",
      "    gs = gridspec.GridSpec(2,1,height_ratios=[10,20])\n",
      "    gs.update(wspace=0.025, hspace=0.05)\n",
      "    # Axes\n",
      "    ax1 = fig.add_subplot(gs[0] )\n",
      "    ax3 = fig.add_subplot(gs[1],sharex= ax1 )\n",
      "    # Plots\n",
      "    total_bar[[\"S\", 'D', 'H-D']].plot.bar(stacked=True, ax=ax1, color=['steelblue','salmon','red'])\n",
      "\n",
      "    for i,v in enumerate(total_bar.sum(axis=1)):\n",
      "        ax1.text(i , v +7, str(int(v)), size=12,horizontalalignment='center')\n",
      "\n",
      "    # heatmap\n",
      "    im = ax3.imshow(hm, cmap=\"GnBu\", aspect='auto')\n",
      "    ax3.set_xticks(np.arange(len(orden_dominios)))\n",
      "    ax3.set_yticks(np.arange(len(orden_mlos)))\n",
      "    ax3.set_xticklabels(orden_dominios)\n",
      "    ax3.set_yticklabels(orden_mlos)\n",
      "    ax3.tick_params(axis='x', rotation=90)\n",
      "    b, t = ax3.get_ylim(); # discover the values for bottom and top\n",
      "    b += 0.5 # Add 0.5 to the bottom\n",
      "    t -= 0.5 # Subtract 0.5 from the top\n",
      "    ax3.set_ylim(b, t) # update the ylim(bottom, top) values\n",
      "    texts = annotate_heatmap2(im, offset=0.2)\n",
      "    ax3.set_xticklabels(ax3.get_xticklabels(), rotation=45, horizontalalignment='right');\n",
      "    ax3.grid(False)\n",
      "    # Legends\n",
      "    #ax2.legend(title='Disorder',labels = ['Low', 'Middle', 'High'],\n",
      "    #          bbox_to_anchor=(1, 2.5), loc='upper left', borderaxespad=0.\n",
      "    #         )\n",
      "    ax1.legend( labels = ['Low', 'Middle', 'High'])\n",
      "\n",
      "    # Eliminar marcos\n",
      "    ax1.axis('off')\n",
      "    #ax2.axis('off')\n",
      "    ax3.spines['right'].set_visible(False)\n",
      "    ax3.spines['top'].set_visible(False)\n",
      "    ax3.spines['bottom'].set_visible(False)\n",
      "    ax3.spines['left'].set_visible(False)\n",
      "\n",
      "    #return fig\n",
      "50/43:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib.colors import LogNorm\n",
      "\n",
      "def make_shared_counts(df_exp, organism = \"Homo sapiens\", savefile=\"graficos/conteo_mlo_locs.png\", n=15, orden=[], ret=False):\n",
      "    \"\"\"\n",
      "    El dataframe tiene que venir sin duplicados unipro y mlo_loc\n",
      "    \"\"\"\n",
      "\n",
      "\n",
      "    # - Carga de la base de ddatos \n",
      "\n",
      "    #df_exp = db_long.query('box == \"box1\"').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    #organism = \"Homo sapiens\"\n",
      "    #orden = []\n",
      "\n",
      "    if organism != \"Homo sapiens\":\n",
      "        #df_org = df_exp[df_exp.org == organism]\n",
      "        df_org = df_exp.query('org != \"Homo sapiens\"')\n",
      "    else:\n",
      "        df_org = df_exp.query('org == \"Homo sapiens\"')\n",
      "\n",
      "    df_org = df_org.drop_duplicates(['uniprot', 'mlo_loc'])\n",
      "\n",
      "    counts = df_org.mlo_loc.value_counts()\n",
      "\n",
      "    if not orden:\n",
      "        orden = counts.index.tolist()\n",
      "    else:\n",
      "        orden = [i for i in orden if i in counts.index.tolist()]\n",
      "    #print(orden)\n",
      "    #descargar mlo_locs con menos de 15 proteinas anotadas \n",
      "    #n_mlo_loc_min = 15\n",
      "    #mlo_locs = counts[counts >= n].index.tolist()\n",
      "\n",
      "\n",
      "    # generar una matriz de cantidad de proteinas compartidas utilizando el producto punto entre matrices\n",
      "    mat = df_org.drop_duplicates(['uniprot', 'mlo_loc']).pivot_table(index='uniprot', columns='mlo_loc', aggfunc='count', margins=True)[\"mlo\"].fillna(0)\n",
      "    mat_bool = mat.applymap(bool).reset_index()\n",
      "    mat_bool = mat_bool.drop(columns=['All'])\n",
      "    matriz = mat_bool.drop(columns='uniprot').applymap(int).values\n",
      "    conteo_mlo_locs = matriz.T.dot(matriz)\n",
      "\n",
      "    conteo_df = pd.DataFrame(conteo_mlo_locs, columns=mat_bool.columns[1:])\n",
      "\n",
      "    conteo_df.set_index(orden)\n",
      "\n",
      "    conteo_df = conteo_df.applymap(lambda x: x -1) # que no se sumen asi mismos\n",
      "\n",
      "    cmap = 'GnBu'\n",
      "    # mascara del triangulo superior para graficar\n",
      "    mask = np.zeros_like(conteo_df)\n",
      "    mask[np.triu_indices_from(mask)] = True\n",
      "\n",
      "    #if organism == \"Homo sapiens\":\n",
      "    #    mask = np.ones_like(conteo_df)\n",
      "    #    mask[np.triu_indices_from(mask)] = False\n",
      "    #    np.fill_diagonal(mask, True)\n",
      "    #    cmap = \"YlOrRd\"\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    # genearar el df para graficar\n",
      "    df_plot = conteo_df.set_index(conteo_df.columns).reindex(orden)[orden]\n",
      "    df_plot.index =  df_plot.index + \" (\"  + counts[orden].astype(str) + \")\"\n",
      "    _ = list(range(len(conteo_mlo_locs)))\n",
      "\n",
      "    # rellenar la diagonal con proteinas, unicas\n",
      "    uniques = df_org.drop_duplicates(\"uniprot\", keep=False).mlo_loc.value_counts()#[orden]\n",
      "    for m in orden:\n",
      "        if m not in uniques.index.tolist():\n",
      "            uniques[m] = 0\n",
      "    uniques = uniques[orden]\n",
      "    df_plot.values[_,_] = list(uniques)\n",
      "\n",
      "    sns.set(rc={'figure.figsize':(4,4)})\n",
      "\n",
      "\n",
      "    # --- Plot\n",
      "    ax = sns.heatmap(df_plot.applymap(lambda x: x + 0.1), cmap=cmap,annot = True, fmt='.0f', mask=((mask - 1) * -1).T, cbar=False, norm=LogNorm(0,100))\n",
      "    bottom, top = ax.get_ylim()\n",
      "    ax.set_ylim(bottom + 0.5, top - 0.5);\n",
      "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, horizontalalignment='right');\n",
      "    plt.yticks(fontsize=14)\n",
      "    #fig = ax.get_figure()\n",
      "    #plt.tight_layout()\n",
      "    #if organism == \"Homo sapiens\":\n",
      "    #    plt.tick_params(labeltop=True)\n",
      "    #    ax.set_xticklabels(ax.get_xticklabels(), rotation=90);\n",
      "    if ret:\n",
      "        return ax\n",
      "\n",
      "    return df_plot\n",
      "\n",
      "#fig.savefig(savefile , bbox_inches = \"tight\")\n",
      "#return fig\n",
      "\n",
      "\n",
      "def annotate_heatmap2(im, data=None, valfmt=\"{x:.2f}\",\n",
      "                     textcolors=[\"#2e2e2e\", \"white\"],offset=0,\n",
      "                     threshold=None, **textkw):\n",
      "\n",
      "    if not isinstance(data, (list, np.ndarray)):\n",
      "        data = im.get_array()\n",
      "        data2 = dfs_plots[1].values\n",
      "\n",
      "    # Normalize the threshold to the images color range.\n",
      "    if threshold is not None:\n",
      "        threshold = im.norm(threshold)\n",
      "    else:\n",
      "        threshold = im.norm(data.max())/2.\n",
      "\n",
      "    # Set default alignment to center, but allow it to be\n",
      "    # overwritten by textkw.\n",
      "    kw = dict(horizontalalignment=\"center\",\n",
      "              verticalalignment=\"center\")\n",
      "    kw.update(textkw)\n",
      "\n",
      "    # Get the formatter in case a string is supplied\n",
      "    #if isinstance(valfmt, str):\n",
      "    #    valfmt = matplotlib.ticker.StrMethodFormatter(valfmt)\n",
      "    def to_int(number):\n",
      "        try:\n",
      "            return(int(number))\n",
      "        except:\n",
      "            return ''        \n",
      "    # Loop over the data and create a `Text` for each \"pixel\".\n",
      "    # Change the text's color depending on the data.\n",
      "    texts = []\n",
      "    for i in range(data.shape[0]):\n",
      "        for j in range(data.shape[1]):\n",
      "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "            text = im.axes.text(j+offset, i+offset, to_int(data[i, j]), **kw, size=15, weight='bold')\n",
      "            texts.append(text)\n",
      "    for i in range(data.shape[0]):\n",
      "        for j in range(data.shape[1]):\n",
      "            kw.update(color=textcolors[int(im.norm(data[i, j]) > threshold)])\n",
      "            text = im.axes.text(j-offset, i-offset, to_int(data2[i, j]), **kw)\n",
      "            texts.append(text)\n",
      "    return texts\n",
      "50/44: data\n",
      "50/45:\n",
      "from pys import heatmap_mlos\n",
      "import importlib\n",
      "\n",
      "dfs_plots = []\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = data[data.box == box].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "\n",
      "i\n",
      "\n",
      "for box in ['box1']:\n",
      "    df = data[data.box == box].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    dfs_plots.append(heatmap_mlos.make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()a\n",
      "50/46:\n",
      "from pys import heatmap_mlos\n",
      "import importlib\n",
      "\n",
      "dfs_plots = []\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = data[data.box == box].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "\n",
      "i\n",
      "\n",
      "for box in ['box1']:\n",
      "    df = data[data.box == box].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    dfs_plots.append(heatmap_mlos.make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/47:\n",
      "\n",
      "import importlib\n",
      "\n",
      "dfs_plots = []\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = data[data.box == box].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "\n",
      "i\n",
      "\n",
      "for box in ['box1']:\n",
      "    df = data[data.box == box].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    dfs_plots.append(heatmap_mlos.make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/48:\n",
      "dfs_plots = []\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = data[data.box == box].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "\n",
      "i\n",
      "\n",
      "for box in ['box1']:\n",
      "    df = data[data.box == box].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    dfs_plots.append(heatmap_mlos.make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/49:\n",
      "for box in ['box1']:\n",
      "    df = data.query('box == \"box1\"').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = 'Homo sapiens'\n",
      "    dfs_plots.append(heatmap_mlos.make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/50:\n",
      "for box in ['box1']:\n",
      "    df = data.query('box == \"box1\"').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = 'Homo sapiens'\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/51:\n",
      "for box in ['box1']:\n",
      "    df = data.query('box == \"box1\"').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = 'Homo sapiens'\n",
      "    make_shared_counts(df, organism='Homo sapiens',orden=orden)\n",
      "    plt.show()\n",
      "50/52: df\n",
      "50/53:\n",
      "for box in ['box1']:\n",
      "    df = data.query('box == 1').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = 'Homo sapiens'\n",
      "    make_shared_counts(df, organism='Homo sapiens',orden=orden)\n",
      "    plt.show()\n",
      "50/54:\n",
      "df['mlo'] = df.mlo_loc\n",
      "df = data.query('box == 1').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "df['org'] = 'Homo sapiens'\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden)\n",
      "plt.show()\n",
      "50/55:\n",
      "df = data.query('box == 1').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "df['mlo'] = df.mlo_loc\n",
      "\n",
      "df['org'] = 'Homo sapiens'\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden)\n",
      "plt.show()\n",
      "50/56:\n",
      "df = data.query('box == 1')\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden)\n",
      "plt.show()\n",
      "50/57:\n",
      "df = tablas['human_box1']\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden)\n",
      "plt.show()\n",
      "50/58:\n",
      "\n",
      "db = (pd.read_csv('/home/fernando/git/mlo/todo/dbs/working_db/latest_exp.csv')\n",
      "            .query('web != \"llpsdb\"')\n",
      "            .query('mlo != \"Droplet\"'))\n",
      "\n",
      "orgs = ['Homo sapiens', 'Saccharomyces cerevisiae', 'Caenorhabditis elegans', \n",
      "        'Drosophila melanogaster', 'Mus musculus', 'Xenopus laevis']\n",
      "db.loc[~db.mlo_loc.isin(mlos), 'mlo_loc'] = 'other'\n",
      "db['org_short'] = db.org.map(lambda x: x if x in orgs else 'other')\n",
      "\n",
      "db.loc[db.db.isin(['phasepdb_rev', 'phasepdb_uniprot']), 'db'] = 'phasepdb_lt'\n",
      "\n",
      "db_long = db\n",
      "\n",
      "# Agregar los boxes de cada proteina\n",
      "boxes_df = pd.read_csv('/home/fernando/git/mlo/todo/tablas/boxes.csv')\n",
      "db_long = db_long.merge(boxes_df[['uniprot','mlo_loc','box']])\n",
      "50/59:\n",
      "mlos = pd.read_csv('/home/fernando/git/mlo/todo/dbs/subsets/human_tresdb_exp.csv').mlo_loc.unique().tolist()\n",
      "db = (pd.read_csv('/home/fernando/git/mlo/todo/dbs/working_db/latest_exp.csv')\n",
      "            .query('web != \"llpsdb\"')\n",
      "            .query('mlo != \"Droplet\"'))\n",
      "\n",
      "orgs = ['Homo sapiens', 'Saccharomyces cerevisiae', 'Caenorhabditis elegans', \n",
      "        'Drosophila melanogaster', 'Mus musculus', 'Xenopus laevis']\n",
      "db.loc[~db.mlo_loc.isin(mlos), 'mlo_loc'] = 'other'\n",
      "db['org_short'] = db.org.map(lambda x: x if x in orgs else 'other')\n",
      "\n",
      "db.loc[db.db.isin(['phasepdb_rev', 'phasepdb_uniprot']), 'db'] = 'phasepdb_lt'\n",
      "\n",
      "db_long = db\n",
      "\n",
      "# Agregar los boxes de cada proteina\n",
      "boxes_df = pd.read_csv('/home/fernando/git/mlo/todo/tablas/boxes.csv')\n",
      "db_long = db_long.merge(boxes_df[['uniprot','mlo_loc','box']])\n",
      "50/60: db_long\n",
      "50/61:\n",
      "\n",
      "make_shared_counts(db_long, organism='Homo sapiens',orden=orden)\n",
      "plt.show()\n",
      "50/62:\n",
      "\n",
      "\n",
      "orden= [ 'nucleolus', 'nuclear_speckles', 'cytoplasmic_stress_granule', 'p_body',\n",
      " 'paraspeckles', 'pml_body', 'cajal_body', 'nuclear_pore_complex', 'postsynaptic_density','other']\n",
      "50/63:\n",
      "\n",
      "make_shared_counts(db_long, organism='Homo sapiens',orden=orden)\n",
      "plt.show()\n",
      "50/64:\n",
      "df = tablas['human_box1']\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden)\n",
      "plt.show()\n",
      "50/65:\n",
      "df = tablas['human_box1']\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden)\n",
      "plt.show()\n",
      "plt.savefig('heatmap_shared_bymlo_box1.svg')\n",
      "50/66:\n",
      "dfs_plots = []\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = tablas['human_boxes'].query('box == @box').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] == \"Homo sapiens\"\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/67:\n",
      "dfs_plots = []\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = tablas['human_boxes'].query('box == @box').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = \"Homo sapiens\"\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/68:\n",
      "dfs_plots = []\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = tablas['human_boxes'].query('box == @box').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = \"Homo sapiens\"\n",
      "    df['mlo'] = df.mlo_loc\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/69:\n",
      "dfs_plots = []\n",
      "orden_fancy = [tablas['translate'][i] for i in orden]\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = tablas['human_boxes'].query('box == @box').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = \"Homo sapiens\"\n",
      "    df['mlo'] = df.mlo_loc.map(tablas[\"translate\"])\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/70:\n",
      "dfs_plots = []\n",
      "orden_fancy = [tablas['translate'][i] for i in orden]\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = tablas['human_boxes'].query('box == @box').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = \"Homo sapiens\"\n",
      "    df['mlo'] = df.mlo_loc.map(tablas[\"translate\"])\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden_fancy))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/71: orden_fancy\n",
      "50/72: df\n",
      "50/73:\n",
      "dfs_plots = []\n",
      "orden_fancy = [tablas['translate'][i] for i in orden]\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = tablas['human_boxes'].query('box == @box').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = \"Homo sapiens\"\n",
      "    df['mlo_loc'] = df.mlo_loc.map(tablas[\"translate\"])\n",
      "    df['mlo'] = df.mlo_loc.map(tablas[\"translate\"])\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden_fancy))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/74:\n",
      "dfs_plots = []\n",
      "orden_fancy = [tablas['translate'][i] for i in orden]\n",
      "for box in ['box1','box2','box3']:\n",
      "    df = tablas['human_boxes'].query('box == @box').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "    df['org'] = \"Homo sapiens\"\n",
      "    df['mlo_loc'] = df.mlo_loc.map(tablas[\"translate\"])\n",
      "    df['mlo'] = df.mlo_loc\n",
      "    dfs_plots.append(make_shared_counts(df, organism='Homo sapiens',orden=orden_fancy))\n",
      "    plt.title(box)\n",
      "    plt.show()\n",
      "50/75:\n",
      "df = tablas['human_box1']\n",
      "orden_fancy = [tablas['translate'][i] for i in orden]\n",
      "df['mlo_loc'] = df.mlo_loc.map(tablas['translate'])\n",
      "df.mlo = df.mlo_loc\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden)\n",
      "plt.show()\n",
      "plt.savefig('heatmap_shared_bymlo_box1.svg')\n",
      "50/76:\n",
      "df = tablas['human_box1']\n",
      "orden_fancy = [tablas['translate'][i] for i in orden]\n",
      "df['mlo_loc'] = df.mlo_loc.map(tablas['translate'])\n",
      "df.mlo = df.mlo_loc\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden_fancy)\n",
      "plt.show()\n",
      "plt.savefig('heatmap_shared_bymlo_box1.svg')\n",
      "50/77: df\n",
      "50/78:\n",
      "df = tablas['human_box1']\n",
      "orden_fancy = [tablas['translate'][i] for i in orden]\n",
      "df['mlo_loc'] = df.mlo_loc.map(tablas['translate'])\n",
      "df.mlo = df.mlo_loc\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden_fancy)\n",
      "plt.show()\n",
      "plt.savefig('heatmap_shared_bymlo_box1.svg')\n",
      "50/79:\n",
      "df = tablas['human_box1']\n",
      "orden_fancy = [tablas['translate'][i] for i in orden]\n",
      "df['mlo_loc'] = df.mlo_loc.map(tablas['translate'])\n",
      "df.mlo = df.mlo_loc\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden_fancy)\n",
      "\n",
      "plt.savefig('heatmap_shared_bymlo_box1.svg')\n",
      "50/80:\n",
      "df = tablas['human_box1']\n",
      "orden_fancy = [tablas['translate'][i] for i in orden]\n",
      "df['mlo_loc'] = df.mlo_loc.map(tablas['translate'])\n",
      "df.mlo = df.mlo_loc\n",
      "make_shared_counts(df, organism='Homo sapiens',orden=orden_fancy)\n",
      "\n",
      "plt.savefig('heatmap_shared_bymlo_box1.svg')\n",
      "42/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "42/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "42/3: pd.set_option('mode.chained_assignment', None)\n",
      "42/4:\n",
      "# load data\n",
      "db = tablas['human_dis_box1']\n",
      "dis = db[['uniprot', 'mlo_loc', 'dc']]\n",
      "\n",
      "# load control table\n",
      "control = pd.read_csv('/home/fernando/git/mlo/todo/tablas/control_disorder.csv')\n",
      "control = control[['uniprot', 'mlo', 'dc']].rename(columns={'mlo':'mlo_loc'})\n",
      "\n",
      "# merge MLOs DC and Swiss-prot DC\n",
      "todo = pd.concat([dis,control])\n",
      "\n",
      "lab_translate = tablas['translate']\n",
      "lab_translate['control'] = 'Control'\n",
      "lab_translate['Control'] = 'control'\n",
      "42/5:\n",
      "# argregar numero de mlos\n",
      "contadores = (todo.uniprot.value_counts() - 1).to_dict()\n",
      "todo['nmlos'] = todo.uniprot.map(contadores)\n",
      "todo['nmlos_3'] = todo.nmlos.map(lambda x: x if x < 3 else 3)\n",
      "\n",
      "# ordenar datos por cantida de proteinas\n",
      "todo['orden_mlo'] = todo.mlo_loc.map(todo.mlo_loc.value_counts().to_dict())\n",
      "todo.loc[todo.mlo_loc == \"control\", 'orden_mlo'] = 0  # poner los control al final\n",
      "todo = todo.sort_values('orden_mlo', ascending=False)\n",
      "42/6: dis\n",
      "42/7: todo\n",
      "42/8:\n",
      "db = tablas['human_dis']\n",
      "dis = db[['uniprot', 'mlo_loc', 'dc']]\n",
      "42/9: dis\n",
      "42/10: tod = pd.concag([dis,control])\n",
      "42/11: tod = pd.concat([dis,control])\n",
      "42/12: todo = pd.concat([dis,control])\n",
      "42/13: todO\n",
      "42/14: todO\n",
      "42/15: todo\n",
      "42/16: todo.uniprot.value_counts()\n",
      "42/17: todo['mlo'] = todo.uniprot.map(todo.uniprot.value_counts().todict())\n",
      "42/18: todo['mlo'] = todo.uniprot.map(todo.uniprot.value_counts().to_dict())\n",
      "42/19: todo\n",
      "42/20: todo.boxplot('dc','mlo')\n",
      "42/21: todo.merge(tablas['human_boxes'],how='left').fillna(\"control\")\n",
      "42/22: boxes = todo.merge(tablas['human_boxes'],how='left').fillna(\"control\")\n",
      "42/23: todo.boxplot('dc','boxes')\n",
      "42/24: todo.boxplot('dc','box')\n",
      "42/25: boxes.boxplot('dc','box')\n",
      "42/26: boxes = todo.filln(0).merge(tablas['human_boxes'],how='left').fillna(\"control\")\n",
      "42/27: boxes.boxplot('dc','box')\n",
      "42/28: boxes = todo.fillna(0).merge(tablas['human_boxes'],how='left').fillna(\"control\")\n",
      "42/29: boxes.boxplot('dc','box')\n",
      "42/30: boxes.boxplot('dc',['box','mlo'])\n",
      "42/31: boxes.boxplot('dc',['box','mlo'],figsize=(20,3))\n",
      "60/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "import altair as alt\n",
      "import numpy as np\n",
      "61/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "import localcider\n",
      "61/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "61/3:\n",
      "lc = tablas['human_lc_box1'].query('method == \"SEG_intermediate\"').copy()\n",
      "\n",
      "db = tablas['human_box1'][['uniprot','mlo_loc']].drop_duplicates()\n",
      "61/4: lc_db = lc.merge(tablas['human_box1'][['uniprot','mlo_loc']], how='left').drop_duplicates()\n",
      "61/5: plaac = tablas['human_dominios_box1'].query('source == \"plaac\"')\n",
      "61/6:\n",
      "# PLAAC\n",
      "plaac['gene_name'] = plaac.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "print('cantidad de proteinas con PLD:',len(plaac.uniprot.unique()))\n",
      "61/7:\n",
      "plaac_control = pd.read_csv(\"/home/fernando/datos/plaac_swiss_prot_human.tsv\",sep='\\t')\n",
      "\n",
      "\n",
      "plaac_control[\"uniprot\"] = plaac_control.SEQid.str.split(\"|\").str[1]\n",
      "\n",
      "plaac_control = plaac_control[~plaac_control.COREscore.isna()]\n",
      "61/8:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      "61/9:\n",
      "base = sp.copy()\n",
      "base['clase'] = 'no_lc'\n",
      "61/10:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "61/11:\n",
      "mlos = ['nucleolus',\n",
      " 'nuclear_speckles',\n",
      " 'cytoplasmic_stress_granule',\n",
      " 'p_body',\n",
      " 'paraspeckles',\n",
      " 'pml_body',\n",
      " 'cajal_body',\n",
      " 'nuclear_pore_complex',\n",
      " 'postsynaptic_density', 'other',]\n",
      "61/12: lc_db\n",
      "62/1: import json\n",
      "62/2:\n",
      "with open(\"/home/fernando/Downloads/idpontology_disprot_8_v0.1.0.json\",'r') as f:\n",
      "    data = json.load(f)\n",
      "62/3: data\n",
      "62/4: pd.DataFrame.from_dict(data)\n",
      "62/5: import pandas as pd\n",
      "62/6: pd.DataFrame.from_dict(data)\n",
      "62/7: df = pd.DataFrame.from_dict(data)\n",
      "62/8: df\n",
      "62/9: df[['name','is_a','def']]\n",
      "62/10: df = df[['name','is_a','def']]\n",
      "62/11: df.to_csv('tmp.csv')\n",
      "62/12: df = pd.DataFrame.from_dict(data)\n",
      "62/13: df\n",
      "62/14: df = df[['namespace','name','is_a','def']]\n",
      "62/15: df\n",
      "62/16: df.to_csv('tmp.csv')\n",
      "61/13: lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)\n",
      "61/14: lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)['seq'].to_frame()\n",
      "61/15:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)['seq'].to_frame()\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "61/16: seqs\n",
      "61/17: sp\n",
      "61/18: \"\".join(sp.seq.values)\n",
      "61/19:\n",
      "aas = \"\".join(sp[:100].seq.values)\n",
      "aas\n",
      "61/20:\n",
      "aas = \"\".join(sp[:100].seq.values)\n",
      "list(aas)\n",
      "61/21:\n",
      "aas = \"\".join(sp[:100].seq.values)\n",
      "aas = list(set(aas))\n",
      "61/22: aas\n",
      "61/23: seq\n",
      "61/24: seqs.seq\n",
      "61/25:\n",
      "for uniprot,seq in seqs.seq:\n",
      "    print(uniprot)\n",
      "61/26:\n",
      "for uniprot,seq in seqs.seq.items():\n",
      "    print(uniprot)\n",
      "61/27:\n",
      "for uniprot,seq in seqs.seq.items():\n",
      "    print(seq)\n",
      "61/28: from localcider.sequenceParameters import SequenceParameters\n",
      "61/29:\n",
      "freqs = {}\n",
      "for uniprot,seq in seqs.seq[:10].items():\n",
      "    SequenceParameters(seq).get_amino_acid_fractions()\n",
      "61/30:\n",
      "freqs = {}\n",
      "for uniprot,seq in seqs.seq[:10].items():\n",
      "    print(SequenceParameters(seq).get_amino_acid_fractions())\n",
      "61/31:\n",
      "freqs = {}\n",
      "for uniprot,seq in seqs.seq.items():\n",
      "    freqs[uniprot] = SequenceParameters(seq).get_amino_acid_fractions()\n",
      "61/32: freq_df = pd.DataFrame.from_dict(freqs)\n",
      "61/33: freq_df\n",
      "61/34: freq_df = pd.DataFrame.from_dict(freqs,'index')\n",
      "61/35: freq_df\n",
      "61/36: seqs\n",
      "61/37:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq','mlo_loc']]\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "61/38: seqs\n",
      "61/39: seqs.explode('mlo_loc')\n",
      "61/40: seqs.explode('mlo_loc').drop_duplicates()\n",
      "61/41: seqs = seqs.explode('mlo_loc').drop_duplicates()\n",
      "61/42: seqs.agg('mlo_loc').agg(lambda x: ''.join())\n",
      "61/43: seqs.agg('mlo_loc').agg(lambda x: ''.join(x))\n",
      "61/44: seqs.agg('mlo_loc')#.agg(lambda x: ''.join(x))\n",
      "61/45: seqs.agg('mlo_loc')#.agg(lambda x: ''.join(x))\n",
      "61/46: seqs#.agg('mlo_loc')#.agg(lambda x: ''.join(x))\n",
      "61/47: seqs.groupby('mlo_loc')#.agg(lambda x: ''.join(x))\n",
      "61/48: seqs.groupby('mlo_loc').agg(lambda x: ''.join(x))\n",
      "61/49: seqs['mlo'] = seqs.groupby('mlo_loc').agg(lambda x: ''.join(x))\n",
      "61/50: seqs.loc['control'] = \"\".join(sp.drop_duplicates().seq)\n",
      "61/51: seqs\n",
      "61/52:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq','mlo_loc']]\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "61/53: seqs = seqs.explode('mlo_loc').drop_duplicates()\n",
      "61/54: seqs['mlo'] = seqs.groupby('mlo_loc').agg(lambda x: ''.join(x))\n",
      "61/55: seqs.loc['control','seq'] = \"\".join(sp.drop_duplicates().seq)\n",
      "61/56: seqs\n",
      "61/57:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq','mlo_loc']]\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "61/58: seqs = seqs.explode('mlo_loc').drop_duplicates()\n",
      "61/59: seqs = seqs.groupby('mlo_loc').agg(lambda x: ''.join(x))\n",
      "61/60: seqs.loc['control','seq'] = \"\".join(sp.drop_duplicates().seq)\n",
      "61/61: seqs\n",
      "61/62: seqs.seq.str.len()\n",
      "61/63:\n",
      "aas = \"\".join(sp[:10].seq.values)\n",
      "aas = list(set(aas))\n",
      "61/64: len(aas)\n",
      "61/65: from localcider.sequenceParameters import SequenceParameters\n",
      "61/66: seqs\n",
      "61/67: seqs\n",
      "61/68: from localcider.sequenceParameters import SequenceParameters\n",
      "61/69:\n",
      "freqs = {}\n",
      "for mlo,seq in seqs.seq.items():\n",
      "    freqs[mlo] = SequenceParameters(seq).get_amino_acid_fractions()\n",
      "61/70: aas\n",
      "61/71: seqs\n",
      "61/72: seqs.seq.contains(\"U\")\n",
      "61/73: seqs[seqs.seq.str.contains(\"U\")]\n",
      "61/74:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq','mlo_loc']]\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "61/75: seqs = seqs.explode('mlo_loc').drop_duplicates()\n",
      "61/76: seqs = seqs.groupby('mlo_loc').agg(lambda x: ''.join(x))\n",
      "61/77:\n",
      "sp = sp[~sp.seq.contains(\"U\")]\n",
      "seqs.loc['control','seq'] = \"\".join(sp.drop_duplicates().seq)\n",
      "61/78:\n",
      "sp = sp[~sp.seq.str.contains(\"U\")]\n",
      "seqs.loc['control','seq'] = \"\".join(sp.drop_duplicates().seq)\n",
      "61/79:\n",
      "aas = \"\".join(sp[:10].seq.values)\n",
      "aas = list(set(aas))\n",
      "61/80: aas\n",
      "61/81: from localcider.sequenceParameters import SequenceParameters\n",
      "61/82:\n",
      "freqs = {}\n",
      "for mlo,seq in seqs.seq.items():\n",
      "    freqs[mlo] = SequenceParameters(seq).get_amino_acid_fractions()\n",
      "61/83: seqs\n",
      "61/84:\n",
      "freqs = {}\n",
      "for mlo,seq in seqs.seq[:3].items():\n",
      "    freqs[mlo] = SequenceParameters(seq).get_amino_acid_fractions()\n",
      "61/85: freq_df = pd.DataFrame.from_dict(freqs)\n",
      "61/86: freq_df\n",
      "61/87:\n",
      "freqs = {}\n",
      "for mlo,seq in seqs.seq[:-1].items():\n",
      "    freqs[mlo] = SequenceParameters(seq).get_amino_acid_fractions()\n",
      "61/88: freq_df = pd.DataFrame.from_dict(freqs)\n",
      "61/89: freq_df\n",
      "61/90:\n",
      "freqs = {}\n",
      "for mlo,seq in seqs.seq.items():\n",
      "    freqs[mlo] = SequenceParameters(seq).get_amino_acid_fractions()\n",
      "61/91: from collections import Counter\n",
      "61/92:\n",
      "freqs = {}\n",
      "for mlo,seq in seqs.seq.items():\n",
      "    counts = Counter(seq)\n",
      "    freq = {k:v/len(seq) for k,v in counts}\n",
      "    freqs[mlo] = freq\n",
      "61/93:\n",
      "freqs = {}\n",
      "for mlo,seq in seqs.seq.items():\n",
      "    counts = Counter(seq)\n",
      "    freq = {k:v/len(seq) for k,v in counts.items()}\n",
      "    freqs[mlo] = freq\n",
      "61/94: freqs\n",
      "61/95: freq_df = pd.DataFrame.from_dict(freqs)\n",
      "61/96: freq_df\n",
      "61/97:\n",
      "for col in freq_df.columns:\n",
      "    freq_df[col] = (freq_df[col] - freq_df.control) / freq_df.control\n",
      "61/98: freq_df\n",
      "61/99: freq_df.t\n",
      "61/100: freq_df.T\n",
      "61/101: sns.heatmap(freq_df.T)\n",
      "61/102: sns.heatmap(freq_df.T,cmap='Blu')\n",
      "61/103: sns.heatmap(freq_df.T,cmap='coolwarm')\n",
      "61/104: sns.heatmap(freq_df.T,cmap='PiYG')\n",
      "61/105: sns.heatmap(freq_df.T,cmap='bwr')\n",
      "61/106: sns.heatmap(freq_df.T,cmap='vlag')\n",
      "61/107: sns.diverging_palette(220, 20, as_cmap=True)\n",
      "61/108: sns.choose_diverging_palette()\n",
      "61/109: sns.choose_diverging_palette(220, 20, as_cmap=True)\n",
      "61/110: sns.choose_diverging_palette(220, 20)\n",
      "61/111: sns.choose_diverging_palette()\n",
      "61/112: sns.choose_diverging_palette(False)\n",
      "61/113: sns.heatmap(freq_df.T,cmap='vlag',center=0)\n",
      "61/114: sns.heatmap(freq_df.T,cmap='coolwarm',center=0)\n",
      "61/115: sns.heatmap(freq_df.T,cmap='vlag',center=0)\n",
      "61/116: freq_df\n",
      "61/117: freq_df.sum(1)\n",
      "61/118: freq_df[\"suma\"] = freq_df.sum(1)\n",
      "61/119: freq\n",
      "61/120: freqs\n",
      "61/121: freqs_df\n",
      "61/122: freqs_df\n",
      "61/123: freq_df\n",
      "61/124: sns.heatmap(freq_df.sort_values('suma').T,cmap='vlag',center=0)\n",
      "61/125: sns.heatmap(freq_df.sort_values('suma').T.drop('suma'),cmap='vlag',center=0)\n",
      "61/126: sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='vlag',center=0)\n",
      "61/127: sns.heatmap(freq_df.sort_values('bwr',ascending=False).T.drop('suma'),cmap='vlag',center=0)\n",
      "61/128: sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='bwr',center=0)\n",
      "61/129:\n",
      "def plot_examples(cms):\n",
      "    \"\"\"\n",
      "    helper function to plot two colormaps\n",
      "    \"\"\"\n",
      "    np.random.seed(19680801)\n",
      "    data = np.random.randn(30, 30)\n",
      "\n",
      "    fig, axs = plt.subplots(1, 2, figsize=(6, 3), constrained_layout=True)\n",
      "    for [ax, cmap] in zip(axs, cms):\n",
      "        psm = ax.pcolormesh(data, cmap=cmap, rasterized=True, vmin=-4, vmax=4)\n",
      "        fig.colorbar(psm,\n",
      "61/130:\n",
      "def plot_examples(cms):\n",
      "    \"\"\"\n",
      "    helper function to plot two colormaps\n",
      "    \"\"\"\n",
      "    np.random.seed(19680801)\n",
      "    data = np.random.randn(30, 30)\n",
      "\n",
      "    fig, axs = plt.subplots(1, 2, figsize=(6, 3), constrained_layout=True)\n",
      "    for [ax, cmap] in zip(axs, cms):\n",
      "        psm = ax.pcolormesh(data, cmap=cmap, rasterized=True, vmin=-4, vmax=4)\n",
      "        fig.colorbar(psm, ax=ax)\n",
      "    plt.show()\n",
      "61/131:\n",
      "top = cm.get_cmap('Oranges_r', 128)\n",
      "bottom = cm.get_cmap('Blues', 128)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 1, 128)),\n",
      "                       bottom(np.linspace(0, 1, 128))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([viridis, newcmp])\n",
      "61/132:\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from matplotlib import cm\n",
      "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
      "61/133:\n",
      "top = cm.get_cmap('Oranges_r', 128)\n",
      "bottom = cm.get_cmap('Blues', 128)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 1, 128)),\n",
      "                       bottom(np.linspace(0, 1, 128))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([viridis, newcmp])\n",
      "61/134:\n",
      "top = cm.get_cmap('Oranges_r', 128)\n",
      "bottom = cm.get_cmap('Blues', 128)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 1, 128)),\n",
      "                       bottom(np.linspace(0, 1, 128))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([ newcmp])\n",
      "61/135:\n",
      "top = cm.get_cmap('Oranges_r', 128)\n",
      "bottom = cm.get_cmap('Blues', 128)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 1, 128)),\n",
      "                       bottom(np.linspace(0, 1, 128))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples(newcmp)\n",
      "61/136:\n",
      "top = cm.get_cmap('Oranges_r', 128)\n",
      "bottom = cm.get_cmap('Blues', 128)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 1, 128)),\n",
      "                       bottom(np.linspace(0, 1, 128))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([newcmp])\n",
      "61/137:\n",
      "top = cm.get_cmap('Reds_r', 128)\n",
      "bottom = cm.get_cmap('Blues', 128)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 1, 128)),\n",
      "                       bottom(np.linspace(0, 1, 128))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([newcmp])\n",
      "61/138:\n",
      "top = cm.get_cmap('Reds_r', 5)\n",
      "bottom = cm.get_cmap('Blues', 5)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 1, 5)),\n",
      "                       bottom(np.linspace(0, 1, 5))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([newcmp])\n",
      "61/139:\n",
      "top = cm.get_cmap('Reds_r', 5)\n",
      "bottom = cm.get_cmap('Blues', 5)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 2, 5)),\n",
      "                       bottom(np.linspace(0, 2, 5))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([newcmp])\n",
      "61/140:\n",
      "top = cm.get_cmap('Reds_r', 5)\n",
      "bottom = cm.get_cmap('Blues', 5)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 2, 2.5)),\n",
      "                       bottom(np.linspace(0, 2, 3))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([newcmp])\n",
      "61/141:\n",
      "top = cm.get_cmap('Reds_r', 5)\n",
      "bottom = cm.get_cmap('Blues', 5)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 2, 5)),\n",
      "                       bottom(np.linspace(0, 2, 3))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([newcmp])\n",
      "61/142:\n",
      "top = cm.get_cmap('Reds_r', 5)\n",
      "bottom = cm.get_cmap('Blues', 5)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 4, 5)),\n",
      "                       bottom(np.linspace(0, 2, 3))))\n",
      "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
      "plot_examples([newcmp])\n",
      "61/143:\n",
      "top = cm.get_cmap('Reds_r', 5)\n",
      "bottom = cm.get_cmap('Blues', 5)\n",
      "\n",
      "newcolors = np.vstack((top(np.linspace(0, 4, 5)),\n",
      "                       bottom(np.linspace(0, 2, 3))))\n",
      "newcmp = ListedColormap(newcolors, name='RedBlue')\n",
      "plot_examples([newcmp])\n",
      "61/144: newcolors\n",
      "61/145: top\n",
      "61/146: np.linspace(0, 4, 5)\n",
      "61/147: sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu',center=0)\n",
      "61/148: sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',center=0)\n",
      "61/149:\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r', vmax=.3, center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/150:\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/151:\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5)\n",
      "61/152:\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/153: sns.load_dataset(\"penguins\")\n",
      "61/154: freqs_df\n",
      "61/155: freq_df\n",
      "61/156: freq_df.melt()\n",
      "61/157: freq_df.melt(id_vars='mlo')\n",
      "61/158: freq_df.melt(id_vars='mlo_loc')\n",
      "61/159: freq_df#.melt(id_vars='mlo_loc')\n",
      "61/160: freq_df.melt()\n",
      "61/161: freq_df.melt()[:30]\n",
      "61/162: freq_df.reset_index().melt()\n",
      "61/163:\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/164:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/165: freq_df\n",
      "61/166: freq_df.columns = freq_df.columns.map(tablas['translaste'])\n",
      "61/167: freq_df.columns = freq_df.columns.map(tablas['translate'])\n",
      "61/168:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/169: tablas['translate']['suma'] = 'suma'\n",
      "61/170: freq_df.columns = freq_df.columns.map(tablas['translate'])\n",
      "61/171:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/172: tablas['translate']['suma'] = 'suma'\n",
      "61/173: freq_df.columns = freq_df.columns.map(tablas['translate'])\n",
      "61/174:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/175: freq_df = pd.DataFrame.from_dict(freqs)\n",
      "61/176:\n",
      "for col in freq_df.columns:\n",
      "    freq_df[col] = (freq_df[col] - freq_df.control) / freq_df.control\n",
      "61/177: freq_df[\"suma\"] = freq_df.sum(1)\n",
      "61/178: tablas['translate']['suma'] = 'suma'\n",
      "61/179: freq_df.columns = freq_df.columns.map(tablas['translate'])\n",
      "61/180:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/181:\n",
      "tablas['translate']['suma'] = 'suma'\n",
      "tablas['control']['control'] = 'Swiss-Prot'\n",
      "61/182:\n",
      "tablas['translate']['suma'] = 'suma'\n",
      "tablas['translate']['control'] = 'Swiss-Prot'\n",
      "61/183: freq_df = pd.DataFrame.from_dict(freqs)\n",
      "61/184:\n",
      "for col in freq_df.columns:\n",
      "    freq_df[col] = (freq_df[col] - freq_df.control) / freq_df.control\n",
      "61/185: freq_df[\"suma\"] = freq_df.sum(1)\n",
      "61/186:\n",
      "tablas['translate']['suma'] = 'suma'\n",
      "tablas['translate']['control'] = 'Swiss-Prot'\n",
      "61/187: freq_df.columns = freq_df.columns.map(tablas['translate'])\n",
      "61/188:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
      "61/189:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "61/190:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=16)\n",
      "61/191:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=14);\n",
      "61/192:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/193:\n",
      "_ = plt.subplots(figsize=(9,12))\n",
      "ax = sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/194:\n",
      "_ = plt.subplots(figsize=(10,12))\n",
      "ax = sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/195:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/196:\n",
      "base = sp.copy()\n",
      "base['clase'] = 'no_lc'\n",
      "61/197:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "61/198: base.loc[base.uniprot.isin(lc_control.uniprot),'clase'] = 'lc'\n",
      "61/199: db = tablas['human_box1'][['uniprot','mlo_loc']].drop_duplicates()\n",
      "61/200:\n",
      "final = base.merge(db, how='left')\n",
      "final.mlo_loc = final.mlo_loc.fillna(\"control\")\n",
      "61/201: final\n",
      "61/202: pivot = final.pivot_table(index='mlo_loc',columns='clase',aggfunc='size')\n",
      "61/203:\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot.suma\n",
      "pivot = pivot.fillna(0)\n",
      "61/204: pivot['suma'] = pivot.featured + pivot.lc\n",
      "61/205: pivot\n",
      "61/206: pivot.index = pivot.index.map(tablas['translate'])\n",
      "61/207:\n",
      "\n",
      "ax = pivot.set_index(pivot..fillna(0).sort_values('suma')[[\"featured\",'lc','no_lc']].plot.barh(#figsize=(3,4),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/208:\n",
      "\n",
      "ax = pivot.set_index(pivot.fillna(0).sort_values('suma')[[\"featured\",'lc','no_lc']].plot.barh(#figsize=(3,4),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/209:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('suma')[[\"featured\",'lc','no_lc']].plot.barh(#figsize=(3,4),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/210: pivot\n",
      "61/211:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('suma')[['lc','no_lc']].plot.barh(#figsize=(3,4),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/212:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(#figsize=(3,4),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/213:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(figsize=(8,12),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/214:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(figsize=(3,4),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/215:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(figsize=(4,2),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/216:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/217:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['mediumseagreen','lightgrey']\n",
      "                                                                   )\n",
      "61/218:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['mediumseagreen','lightgrey']\n",
      "                                                                   )\n",
      "plt.legend(False)\n",
      "61/219:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['mediumseagreen','lightgrey']\n",
      "                                                                   )\n",
      "plt.legend('')\n",
      "61/220:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['mediumseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "plt.legend('')\n",
      "61/221:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['mediumseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "61/222: pivot.fillna(0).sort_values('lc')\n",
      "61/223:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "61/224: base.loc[base.uniprot.isin(lc_control.uniprot),'clase'] = 'lc'\n",
      "61/225: base.loc[base.uniprot.isin(plaac_control.uniprot), 'clase'] = 'featured'\n",
      "61/226: pivot['suma'] = pivot.featured + pivot.lc\n",
      "61/227:\n",
      "base = sp.copy()\n",
      "base['clase'] = 'no_lc'\n",
      "61/228:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "61/229: base.loc[base.uniprot.isin(lc_control.uniprot),'clase'] = 'lc'\n",
      "61/230: base.loc[base.uniprot.isin(plaac_control.uniprot), 'clase'] = 'featured'\n",
      "61/231: db = tablas['human_box1'][['uniprot','mlo_loc']].drop_duplicates()\n",
      "61/232:\n",
      "final = base.merge(db, how='left')\n",
      "final.mlo_loc = final.mlo_loc.fillna(\"control\")\n",
      "61/233: pivot = final.pivot_table(index='mlo_loc',columns='clase',aggfunc='size')\n",
      "61/234:\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot.suma\n",
      "pivot = pivot.fillna(0)\n",
      "61/235: pivot['suma'] = pivot.featured + pivot.lc\n",
      "61/236: pivot.index = pivot.index.map(tablas['translate'])\n",
      "61/237: pivot\n",
      "61/238:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[[,'featured','lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['mediumseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "61/239:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['featured','lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['mediumseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "61/240:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['featured','lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['mediumseagreen','darkseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "61/241:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['featured','lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['eagreen','darkseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "61/242:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('lc')[['featured','lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','darkseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "61/243:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('suma')[['featured','lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','darkseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "61/244: pivot.fillna(0).sort_values('suma',ascending=False)\n",
      "61/245: pivot.fillna(0).sort_values('suma',ascending=False).index.tolist()\n",
      "61/246:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist()].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/247:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist()].T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/248:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist()].T,cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/249:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df.sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/250: freq_df\n",
      "61/251: freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]]\n",
      "61/252:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_dffreq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/253:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "61/254:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "plt.savedig('lc_heatmap.svg')\n",
      "61/255:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "plt.savefig('lc_heatmap.svg')\n",
      "61/256:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('suma')[['featured','lc','no_lc']].plot.barh(figsize=(4,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','darkseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "plt.savefig('lc_barplot')\n",
      "61/257:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('suma')[['featured','lc','no_lc']].plot.barh(figsize=(2,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','darkseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "plt.savefig('lc_barplot')\n",
      "61/258:\n",
      "\n",
      "ax = pivot.fillna(0).sort_values('suma')[['featured','lc','no_lc']].plot.barh(figsize=(2,3),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','darkseagreen','lightgrey'],legend=None,\n",
      "                                                                   )\n",
      "plt.savefig('lc_barplot.svg')\n",
      "61/259:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "plt.savefig('lc_heatmap.svg')\n",
      "61/260:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "#plt.savefig('lc_heatmap.svg')\n",
      "61/261:\n",
      "_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5)#, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "#plt.savefig('lc_heatmap.svg')\n",
      "61/262:\n",
      "#_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5)#, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "#plt.savefig('lc_heatmap.svg')\n",
      "61/263:\n",
      "#_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5)#, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "plt.savefig('lc_heatmap_colormap.svg')\n",
      "63/1: import pandas as pd\n",
      "63/2: x = pd.read_excel('tmp.xlsx')\n",
      "63/3: x\n",
      "63/4: type(x)\n",
      "63/5: lista_df = pd.ExcelFile('tmp.xlsx')\n",
      "63/6: lista_df.shet_names\n",
      "63/7: lista_df.sheet_names\n",
      "63/8: dfs = {}\n",
      "63/9:\n",
      "for i in  lista_df.sheet_names:\n",
      "    dfs[sheet_names] = pd.read_excel('tmp.xlsx', sheetname=i)\n",
      "63/10:\n",
      "for i in  lista_df.sheet_names:\n",
      "    dfs[sheet_names] = pd.read_excel('tmp.xlsx', sheet_name=i)\n",
      "63/11:\n",
      "for i in  lista_df.sheet_names:\n",
      "    dfs[i] = pd.read_excel('tmp.xlsx', sheet_name=i)\n",
      "63/12: dfs\n",
      "63/13: dfs.keys()\n",
      "63/14: dfs['boxes']\n",
      "63/15:\n",
      "preguntas = {'preguntaP08': 'Principales Problemas del país',\n",
      "'preguntaP21': 'Evaluación de la gestion del gobierno nacional',\n",
      "'preguntaP22': 'Evaluación del desempeño del Presidente de la Nación',\n",
      "'preguntaP23': 'Evaluación de la gestion del gobierno provincial',\n",
      "'preguntaP24': 'Evaluación del desempeño del Gobernador de la Provincia de Buenos Aires',\n",
      "'preguntaP14': 'Ingresos familiares',\n",
      "'preguntaP13': 'Endeudamiento familiar',\n",
      "'preguntaP15': 'Situación económica personal actual',\n",
      "'preguntaP16': 'Situación económica personal en comparación con seis meses atrás',\n",
      "'preguntaP17': 'Expectativa económica personal para dentro de seis meses',\n",
      "'preguntaP18': 'Situación económica actual del país',\n",
      "'preguntaP19': 'Situación ecónomica del país en comparación con seis meses atrás',\n",
      "'preguntaP20': 'Expectativa económica del país para dentro de seis meses',\n",
      "'preguntaP26': 'Evaluación de la política económica del gobierno nacional',\n",
      "'preguntaP27': 'Evaluación de la política social del gobierno nacional',\n",
      "'preguntaP28': 'Evaluación de la política de seguridad del gobierno nacional',\n",
      "'preguntaP45': 'Consumo de Medios de Comunicación',\n",
      "'preguntaP46': 'Imagen del Congreso Nacional',\n",
      "'preguntaP47': 'Imagen del Poder Judicial',\n",
      "'preguntaP48': '¿Cuán de acuerdo esta con las medidas tomadas por el gobierno nacional para enfrentar la pandemia?',\n",
      "'preguntaP49': '¿Qué imagen tiene del Ministro de Salud Gines Gonzalez Garcia?',\n",
      "'preguntaP50': '¿Cuánto le preocupa su salud, la de su familia y amigos?',\n",
      "'preguntaP51': '¿En la situación actual cual considera que es la actitud mas razonable?',\n",
      "'preguntaP52': '¿Cuán peligroso cree que es el coronavirus?',\n",
      "'preguntaP53': '¿Cuánto le preocupa la disponibilidad de dinero para poder quedarse en su casa?',\n",
      "'preguntaP54': '¿Cuánto le preocupa lo que vaya a pasar con su trabajo una vez finalizada la cuarentena obligatoria?',\n",
      "'preguntaP55': '¿Cuánto le preocupa que aumente la inseguridad?',\n",
      "'preguntaP56': '¿Se siente suficientemente informado sobre el tema?',\n",
      "'preguntaP57': '¿En qué medida está cumpliendo usted con el aislamiento?',\n",
      "'preguntaP58': '¿En qué medida sus vecinos están cumpliendo con el aislamiento?',\n",
      "'preguntaP59': '¿Como evalúa la gestión del Gobierno Provincial de Axel Kicillof en salud?',\n",
      "'preguntaP60': '¿Como evalúa la gestión del Gobierno Provincial de Axel Kicillof en seguridad?',\n",
      "'preguntaP61': '¿Como evalúa la gestión del Gobierno Provincial de Axel Kicillof en obras públicas?',\n",
      "'preguntaP62': '¿Como evalúa la gestión del Gobierno Provincial de Axel Kicillof en desarrollo social?',\n",
      "'preguntaP63': '¿Cuál es su opinión sobre la forma en la que el gobierno está administrando la cuarentena?',\n",
      "'preguntaP64': '¿Cuál es su opinión sobre los grupos que están en contra de la cuarentena?',\n",
      "'preguntaP65': '¿Que le despierta la cuarentena por causa del coronavirus?',\n",
      "'preguntaP66': '¿En qué medida está de acuerdo con que se haya aumentado el impuesto para la compra del dólar ahorro en un 35%?',\n",
      "'preguntaP67': '¿En qué medida está de acuerdo con que se haya aumentado el impuesto para las compras en dólares al exterior en un 35%',\n",
      "'preguntaP68': '¿Quién cree que es responsable de la escasez de dólares en Argentina?',\n",
      "'preguntaP69A': 'Nos gustaría volver a contactarlo para otras encuestas, tanto a través de llamadas como a través de WhatsApp o plataformas online. Usted recibirá premios por su participación ¿Está interesado en que lo contactemos nuevamente?',\n",
      "'preguntaP69B': 'Nos gustaría volver a contactarlo para otras encuestas que se realizan de manera online, tanto a través de computadoras como de tablets o celulares. Usted recibirá premios por su participación ¿Está interesado en que lo contactemos nuevamente?'}\n",
      "63/16:\n",
      "for k,v in preguntas.items():\n",
      "    print(k)\n",
      "63/17:\n",
      "l = []\n",
      "for k,v in preguntas.items():\n",
      "    num = k.rsplit('P')[-1]\n",
      "    texto = v\n",
      "    l.append(num + '\\t' + texto)\n",
      "63/18: for i in l:print(i)\n",
      "63/19:\n",
      "labels = {'labelP08': 'Principales Problema del país',\n",
      "    'labelP21': 'Evaluación de la gestion del gobierno nacional',\n",
      "    'labelP22': 'Evaluación del desempeño del Presidente de la Nación',\n",
      "    'labelP23': 'Evaluación de la gestion del gobierno provincial',\n",
      "    'labelP24': 'Evaluación del desempeño del Gobernador de la Provincia de Buenos Aires',\n",
      "    'labelP14': 'Ingresos familiares',\n",
      "    'labelP13': 'Endeudamiento familiar',\n",
      "    'labelP15': 'Percepción de la situación económica personal',\n",
      "    'labelP16': 'Percepción de la situación económica personal en comparación con seis meses atrás',\n",
      "    'labelP17': 'Expectativa económica personal para dentro de seis meses',\n",
      "    'labelP18': 'Percepción de la situación económica del país',\n",
      "    'labelP19': 'Percepción de la situación económica del país en comparación con seis meses atrás',\n",
      "    'labelP20': 'Expectativa económica del país para dentro de seis meses',\n",
      "    'labelP26': 'Evaluación de la política económica del gobierno nacional',\n",
      "    'labelP27': 'Evaluación de la política social del gobierno nacional',\n",
      "    'labelP28': 'Evaluación de la política de seguridad del gobierno nacional',\n",
      "    'labelP30': 'Imagen de Alberto Fernández',\n",
      "    'labelP31': 'Imagen de Cristina Fernández de Kirchner',\n",
      "    'labelP32': 'Imagen de Mauricio Macri',\n",
      "    'labelP33': 'Imagen de Horacio Rodríguez Larreta',\n",
      "    'labelP34': 'Imagen de Sergio Massa',\n",
      "    'labelP35': 'Imagen de María Eugenia Vidal',\n",
      "    'labelP36': 'Imagen de Axel Kicillof',\n",
      "    'labelP37': 'Imagen de Wado de Pedro',\n",
      "    'labelP38': 'Imagen de Santiago Cafiero',\n",
      "    'labelP39': 'Imagen de Máximo Kirchner',\n",
      "    'labelP40': 'Imagen de Martín Guzmán',\n",
      "    'labelP41': 'Imagen de Malena Malgarini',\n",
      "    'labelP42': 'Imagen de Elisa Carrio',\n",
      "    'labelP43': 'Imagen de Martín Lousteau',\n",
      "    'labelP44': '¿Cuán frecuente es que usted responda encuestas sobre temas políticos y económicos?',\n",
      "    'labelP45': 'Consumo de Medios de Comunicación',\n",
      "    'labelP46': 'Imagen del Congreso Nacional',\n",
      "    'labelP47': 'Imagen del Poder Judicial',\n",
      "    'labelP48': 'Acuerdo con las medidas tomadas por el gobierno nacional',\n",
      "    'labelP49': 'Imagen del Ministro de Salud Gines Gonzalez Garcia',\n",
      "    'labelP50': 'Preocupación por la salud propia, de familiares y amigos',\n",
      "    'labelP51': '¿En la situación actual cual considera que es la actitud mas razonable?',\n",
      "    'labelP52': '¿Cuán peligroso cree que es el coronavirus?',\n",
      "    'labelP53': 'Preocupación por la disponibilidad de dinero',\n",
      "    'labelP54': 'Preocupación por el futuro laboral',\n",
      "    'labelP55': 'Preocupación por el aumento de la inseguridad',\n",
      "    'labelP56': '¿Se siente suficientemente informado sobre el tema?',\n",
      "    'labelP57': '¿En qué medida está cumpliendo usted con el aislamiento?',\n",
      "    'labelP58': '¿En qué medida sus vecinos están cumpliendo con el aislamiento?',    \n",
      "    'labelP59': 'Evaluación de la gestión del Gobierno Provincial de Axel Kicillof en salud',\n",
      "    'labelP60': 'Evaluación de la gestión del Gobierno Provincial de Axel Kicillof en seguridad',\n",
      "    'labelP61': 'Evaluación de la gestión del Gobierno Provincial de Axel Kicillof en obras públicas',\n",
      "    'labelP62': 'Evaluación de la gestión del Gobierno Provincial de Axel Kicillof en desarrollo social',\n",
      "    'labelP63': 'Opinión sobre la forma en la que el gobierno está administrando la cuarentena',\n",
      "    'labelP64': 'Opinión sobre los grupos anti-cuarentena',\n",
      "    'labelP65': '¿Que le despierta la cuarentena por causa del coronavirus?',\n",
      "    'labelP66': 'Opinión sobre el aumento del impuesto para la compra del dólar ahorro en un 35%',\n",
      "    'labelP67': 'Opinión sobre el aumento del impuesto para las compras en dólares al exterior en un 35%',\n",
      "    'labelP68': 'Responsabilidad de la escasez de dólares en Argentina',\n",
      "    'labelP69A': '¿Está interesado en que lo contactemos nuevamente? Fraseo A',\n",
      "    'labelP69B': '¿Está interesado en que lo contactemos nuevamente? Fraseo B'}\n",
      "63/20:\n",
      "l = []\n",
      "for k,v in preguntas.items():\n",
      "    num = k.rsplit('P')[-1]\n",
      "    \n",
      "    texto = v\n",
      "    label = labels['labelP'+num]    \n",
      "    l.append(num + '\\t' + texto + '\\t' + label)\n",
      "63/21: for i in l:print(i)\n",
      "64/1: dataFolderCOVID = './trackingCOVID/data/'\n",
      "64/2: dataFolderAYSA = './trackingAYSA/data/'\n",
      "64/3: filesAYSA = {}\n",
      "64/4:\n",
      "for i in [\"44\",\"45\",\"46\",\"47\", '46_', '47_']:\n",
      "        filasAYSA['csvP' + i] =  [('{}pregunta_P' + i + '.csv').format(dataFolderAYSA),\n",
      "                                          #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                                                                            ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderAYSA)]\n",
      "                                                                            for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "                                                                                    j = str(i).zfill(2)\n",
      "64/5:\n",
      "    filesCOVID['csvP' + j] = [('{}pregunta_P' + j + '.csv').format(dataFolderCOVID),\n",
      "                            #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                                                    ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderCOVID)]\n",
      "64/6: j = str(i).zfill(2)\n",
      "64/7: j\n",
      "64/8:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "64/9: j\n",
      "64/10:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "            filesCOVID['csvP' + j] = [('{}pregunta_P' + j + '.csv').format(dataFolderCOVID),\n",
      "                                        #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                                                                    ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderCOVID)]\n",
      "64/11:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "            filesCOVID['csvP' + j] = [('{}pregunta_P' + j + '.csv').format(dataFolderCOVID),\n",
      "                                        #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                                                                    ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderCOVID)]\n",
      "64/12:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "            print('hola')\n",
      "64/13:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "            print('hola')\n",
      "64/14:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "    j = str(i).zfill(2)\n",
      "    print('hola')\n",
      "64/15:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "    j = str(i).zfill(2)\n",
      "    print('hola')\n",
      "    #filesCOVID['csvP' + j] = [('{}pregunta_P' + j + '.csv').format(dataFolderCOVID),\n",
      "                            #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                            ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderCOVID)]\n",
      "   for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "       j = str(i).zfill(2)\n",
      "           print('hola')\n",
      "               filesCOVID['csvP' + j] = [('{}pregunta_P' + j + '.csv').format(dataFolderCOVID),\n",
      "64/16:                             #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "64/17:\n",
      "\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "    j = str(i).zfill(2)\n",
      "    print('hola')\n",
      "    filesCOVID['csvP' + j] = [('{}pregunta_P' + j + '.csv').format(dataFolderCOVID),\n",
      "                            #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                            ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderCOVID)]\n",
      "64/18:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "    j = str(i).zfill(2)\n",
      "    print('hola')\n",
      "    filesAYSA['csvP' + j] = [('{}pregunta_P' + j + '.csv').format(dataFolderCOVID),\n",
      "                            #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                            ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderCOVID)]\n",
      "64/19: filesAYSA\n",
      "64/20:\n",
      "for i in [\"44\",\"45\",\"46\",\"47\", '46_', '47_']:\n",
      "        filasAYSA['csvP' + i] =  [('{}pregunta_P' + i + '.csv').format(dataFolderAYSA),\n",
      "                                          #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                                                                            ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderAYSA)]\n",
      "64/21:\n",
      "for i in [\"44\",\"45\",\"46\",\"47\", '46_', '47_']:\n",
      "    filesAYSA['csvP' + i] =  [('{}pregunta_P' + i + '.csv').format(dataFolderAYSA),\n",
      "                                  #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                                  ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderAYSA)]\n",
      "64/22: filesAYSA\n",
      "64/23:\n",
      "\n",
      "filesAYSA = {}\n",
      "\n",
      "for i in [\"44\",\"45\",\"46\",\"47\", '46_', '47_']:\n",
      "    filesAYSA['csvP' + i] =  [('{}pregunta_P' + i + '.csv').format(dataFolderAYSA),\n",
      "                                  #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                                  ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderAYSA)]\n",
      "\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "    j = str(i).zfill(2)\n",
      "\n",
      "    filesAYSA['csvP' + j] = [('{}pregunta_P' + j + '.csv').format(dataFolderCOVID),\n",
      "\n",
      "                            #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                            ('{}cruce_pregunta_P' + j + '.csv').format(dataFolderAYSA)]\n",
      "\n",
      "\n",
      "filesAYSA['csv_imagen'] = ['{}imagen_orden_positiva.csv'.format(dataFolderCOVID),\n",
      "                           '{}imagen_orden_ratio.csv'.format(dataFolderCOVID),\n",
      "                           '{}imagen_orden_negativa.csv'.format(dataFolderCOVID),\n",
      "                           '{}serie_imagen.csv'.format(dataFolderCOVID)]\n",
      "\n",
      "\n",
      "filesAYSA['Alberto Fernandez'] = ['{}cruce_pregunta_P39_5opc.csv'.format(dataFolderAYSA),\n",
      "                                   '{}cruce_pregunta_P39_3opc.csv'.format(dataFolderAYSA)]\n",
      "filesAYSA['Horacio Rodriguez Larreta'] = ['{}cruce_pregunta_P40_5opc.csv'.format(dataFolderAYSA),\n",
      "                                           '{}cruce_pregunta_P40_3opc.csv'.format(dataFolderAYSA)]\n",
      "filesAYSA['Axel Kicillof'] = ['{}cruce_pregunta_P41_5opc.csv'.format(dataFolderAYSA),\n",
      "                               '{}cruce_pregunta_P41_3opc.csv'.format(dataFolderAYSA)]\n",
      "64/24:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "64/25: j\n",
      "64/26:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "            filesAYSA['csvP' + j] = ['{}pregunta_P{}.csv'.format(dataFolderCOVID, j),\n",
      "                 #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                      '{}cruce_pregunta_P{}.csv'.format(dataFolderAYSA, j)]\n",
      "64/27:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "            filesAYSA['csvP' + j] = ['{}pregunta_P{}.csv'.format(dataFolderCOVID, j),\n",
      "                #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                    '{}cruce_pregunta_P{}.csv'.format(dataFolderAYSA, j)]\n",
      "64/28:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "            filesAYSA['csvP' + j] = ['{}pregunta_P{}.csv'.format(dataFolderCOVID, j),\n",
      "                #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                    '{}cruce_pregunta_P{}.csv'.format(dataFolderAYSA, j)]\n",
      "64/29:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        j = str(i).zfill(2)\n",
      "            filesAYSA['csvP' + j] = ['{}pregunta_P{}.csv'.format(dataFolderCOVID, j),\n",
      "                #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                    '{}cruce_pregunta_P{}.csv'.format(dataFolderAYSA, j)]\n",
      "64/30:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        i = str(i).zfill(2)\n",
      "64/31:\n",
      "for i in [18, 20, 8, 34, 35, 12, 13, 36]:\n",
      "        i = str(i).zfill(2)\n",
      "            filesAYSA['csvP' + j] = ['{}pregunta_P{}.csv'.format(dataFolderCOVID, i),\n",
      "                #('{}serie_P' + j + '.csv').format(dataFolderCOVID), # Aun no hay\n",
      "                    '{}cruce_pregunta_P{}.csv'.format(dataFolderAYSA, i)]\n",
      "64/32: filesAYSA['csvP44'][0]\n",
      "64/33: # PALETAS\n",
      "64/34: diverging = \"RdYlGn_r\"\n",
      "64/35: diverging_reverse = \"RdYlGn\"\n",
      "64/36: diverging_mini = ['#4bb05c', '#ea5739', '#c9bebd']\n",
      "64/37: secuencial = \"OrRd\"\n",
      "64/38: qualitative = \"Paired\"\n",
      "64/39: siNoNoSabeColorDict = dict(zip(['Si', 'No', 'No sabe'], diverging_mini))\n",
      "64/40: filesAYSA['csvP44'][0]\n",
      "64/41: graphs.getBarplotWithPalette(filesAYSA['csvP44'][0], 'selectTargetAYSA', 'selectOpciones', siNoNoSabeColorDict, name='AYSA-P44-1')\n",
      "64/42: import graphs\n",
      "75/1: import pandas\n",
      "75/2:\n",
      "# Load dataframe from source\n",
      "raw_df = pandas.read_csv(\"https://sisa.msal.gov.ar/datos/descargas/covid-19/files/Covid19Casos.csv\")\n",
      "75/3: raw_df = raw_df[(raw_df.fecha_diagnostico.isna()) | (raw_df.fecha_diagnostico <= today.strftime(\"%Y-%m-%d\"))] #Sacar fechas posteriores al dia de hoy (mal ingresadas)\n",
      "75/4: raw_df.columns = raw_df.columns.str.lower()\n",
      "75/5: raw_df = raw_df[(raw_df.fecha_diagnostico.isna()) | (raw_df.fecha_diagnostico <= today.strftime(\"%Y-%m-%d\"))] #Sacar fechas posteriores al dia de hoy (mal ingresadas)\n",
      "75/6: raw_df.info()\n",
      "75/7:\n",
      "import pandas\n",
      "from time import today\n",
      "75/8:\n",
      "for col in raw_df.columns:\n",
      "    if col.startswith('fecha'):\n",
      "        raw_df[col] = pandas.to_datetime(raw_df[col])\n",
      "75/9: raw_df.info()\n",
      "75/10: time.today()\n",
      "75/11:\n",
      "import pandas\n",
      "import time\n",
      "75/12: time.today()\n",
      "75/13: time.time()\n",
      "75/14:\n",
      "import pandas\n",
      "from datetime import date\n",
      "75/15: today = date.today()\n",
      "75/16: raw_df = raw_df[(raw_df.fecha_diagnostico.isna()) | (raw_df.fecha_diagnostico <= today.strftime(\"%Y-%m-%d\"))] #Sacar fechas posteriores al dia de hoy (mal ingresadas)\n",
      "75/17:\n",
      "for col in raw_df.columns:\n",
      "    if col.startswith('fecha'):\n",
      "        raw_df[col] = pandas.to_datetime(raw_df[col])\n",
      "75/18: time.time()\n",
      "75/19:\n",
      "confirmados = raw_df[raw_df['clasificacion_resumen'] == 'Confirmado']\n",
      "sospechosos = raw_df[raw_df['clasificacion_resumen'] == 'Sospechoso']\n",
      "fallecidos = confirmados.loc[confirmados['fallecido'] == \"SI\"]\n",
      "75/20:\n",
      "confirmados = raw_df[raw_df['clasificacion_resumen'] == 'Confirmado']\n",
      "sospechosos = raw_df[raw_df['clasificacion_resumen'] == 'Sospechoso']\n",
      "fallecidos = confirmados.loc[confirmados['fallecido'] == \"SI\"]\n",
      "75/21:\n",
      "prop_fallecidos = fallecidos['sexo'].value_counts()\n",
      "pc_fallecidos = prop_fallecidos / prop_fallecidos.sum() * 100\n",
      "75/22:\n",
      "print(f'Casos al {raw_df[\"ultima_actualizacion\"].max()}:')\n",
      "print('Confirmados:', len(confirmados))\n",
      "print('Sospechosos:', len(sospechosos))\n",
      "print('Fallecidos:', len(fallecidos))\n",
      "print(f'\\t{pc_fallecidos[\"F\"].round(1)} % Mujeres, {pc_fallecidos[\"M\"].round(1)} % Hombres')\n",
      "75/23:\n",
      "casos_x_provincia = (raw_df.pivot_table(index='residencia_provincia_nombre',\n",
      "                                                      columns='clasificacion_resumen', aggfunc='size')\n",
      "                                      .drop(columns='Sin Clasificar').fillna(0)\n",
      "                                      .sort_values('Confirmado',ascending=False)\n",
      "                                      .astype(int)\n",
      "                                      #.assign(total_testeado=lambda x: sum(x))\n",
      "                                  [['Confirmado','Sospechoso','Descartado']])\n",
      "\n",
      "fallecidos_x_provincia = (raw_df.loc[raw_df['fallecido'] == \"SI\"]\n",
      "                                      .pivot_table(index='residencia_provincia_nombre',\n",
      "                                                      columns='clasificacion_resumen', aggfunc='size')\n",
      "                                      #.drop(columns='Sin Clasificar')\n",
      "                                      .fillna(0)\n",
      "                                      .sort_values('Confirmado',ascending=False)\n",
      "                                      .astype(int)\n",
      "                                      #.assign(total_testeado=lambda x: sum(x))\n",
      "                                      [['Confirmado','Sospechoso','Descartado']])\n",
      "75/24:\n",
      "x_provincia = pandas.concat([casos_x_provincia, fallecidos_x_provincia],axis=1, keys=['Casos', 'Fallecidos']).fillna(0).astype(int)\n",
      "x_provincia\n",
      "75/25:\n",
      "top5_provincias_conf = x_provincia['Casos']['Confirmado'].sort_values(ascending=False).head(5)\n",
      "top5_provincias_fa = x_provincia['Fallecidos']['Confirmado'].sort_values(ascending=False).head(5)\n",
      "\n",
      "print('Confirmados:', top5_provincias_conf, sep='\\n')\n",
      "print()\n",
      "print('Fallecidos:', top5_provincias_fa, sep='\\n')\n",
      "75/26: df_topspro = raw_df.loc[raw_df['residencia_provincia_nombre'].isin(top5_provincias_fa.index.tolist())]\n",
      "75/27: pivot = df_topspro.loc[df_topspro['fecha_diagnostico'] >= \"2020-03\"].pivot_table(index= 'fecha_diagnostico', columns=['residencia_provincia_nombre', 'clasificacion_resumen'],aggfunc='size')\n",
      "75/28:\n",
      "dfs = []\n",
      "for provincia in top5_provincias_fa.index:\n",
      "    \n",
      "    dfs.append(\n",
      "        ((pivot[provincia][\"Confirmado\"] / (pivot[provincia][\"Confirmado\"] + pivot[provincia][\"Descartado\"])).\n",
      "        fillna(0).cumsum().to_frame())\n",
      "        \n",
      "    )\n",
      "df_positividad = pandas.concat(dfs,axis=1)\n",
      "df_positividad.columns = top5_provincias_fa.index.tolist()\n",
      "75/29: df_positividad\n",
      "75/30:\n",
      "ax = df_positividad.plot(kind='line', title='Positividad',figsize=(10,4))\n",
      "ax.set_ylabel('Test Positivos / Total Tests')\n",
      "ax.set_xlabel('Fecha')\n",
      "75/31:\n",
      "dfs = []\n",
      "for provincia in top5_provincias_fa.index:\n",
      "    \n",
      "    dfs.append(\n",
      "        ((pivot[provincia][\"Confirmado\"] / (pivot[provincia][\"Confirmado\"] + pivot[provincia][\"Descartado\"])).\n",
      "        fillna(0).to_frame())\n",
      "        \n",
      "    )\n",
      "df_positividad = pandas.concat(dfs,axis=1)\n",
      "df_positividad.columns = top5_provincias_fa.index.tolist()\n",
      "75/32:\n",
      "ax = df_positividad.rolling(20).mean().plot(kind='line', title='Positividad',figsize=(10,4))\n",
      "ax.set_ylabel('Test Positivos / Total Tests')\n",
      "ax.set_xlabel('Fecha')\n",
      "ax.axhline(0.1,color='red')\n",
      "75/33:\n",
      "ax = df_positividad[df_positividad.index.month >= 6].rolling(20).mean().plot(kind='line', title='Positividad',figsize=(10,4))\n",
      "ax.set_ylabel('Test Positivos / Total Tests')\n",
      "ax.set_xlabel('Fecha')\n",
      "ax.axhline(0.1,color='red')\n",
      "75/34: fallecidos.pivot_table(index='fecha_fallecimiento',columns='residencia_provincia_nombre',aggfunc='size')[['Buenos Aires','CABA']].plot.line()\n",
      "75/35:\n",
      "fechas_diagnostico = confirmados.query('residencia_provincia_nombre == \"Buenos Aires\"').groupby('fecha_diagnostico').agg('size').to_frame().reset_index()\n",
      "fechas_diagnostico.columns = ['fecha', 'casos']\n",
      "fechas_fallecidos = fallecidos.query('residencia_provincia_nombre == \"Buenos Aires\"').groupby('fecha_fallecimiento').agg('size').to_frame().reset_index()\n",
      "fechas_fallecidos.columns = ['fecha', 'fallecidos']\n",
      "75/36:\n",
      "fechas_ba = fechas_diagnostico.merge(fechas_fallecidos)\n",
      "fechas_ba['casos_ac'] = fechas_ba.casos.cumsum()\n",
      "fechas_ba['fa_ac'] = fechas_ba.fallecidos.cumsum()\n",
      "75/37: fechas_ba.set_index('fecha')['fa_ac'].map(np.log).plot.line()\n",
      "75/38: import numpy as no\n",
      "75/39: import numpy as np\n",
      "75/40:\n",
      "fechas_diagnostico = confirmados.query('residencia_provincia_nombre == \"Buenos Aires\"').groupby('fecha_diagnostico').agg('size').to_frame().reset_index()\n",
      "fechas_diagnostico.columns = ['fecha', 'casos']\n",
      "fechas_fallecidos = fallecidos.query('residencia_provincia_nombre == \"Buenos Aires\"').groupby('fecha_fallecimiento').agg('size').to_frame().reset_index()\n",
      "fechas_fallecidos.columns = ['fecha', 'fallecidos']\n",
      "75/41:\n",
      "fechas_ba = fechas_diagnostico.merge(fechas_fallecidos)\n",
      "fechas_ba['casos_ac'] = fechas_ba.casos.cumsum()\n",
      "fechas_ba['fa_ac'] = fechas_ba.fallecidos.cumsum()\n",
      "75/42: fechas_ba.set_index('fecha')['fa_ac'].map(np.log).plot.line()\n",
      "75/43: fechas_ba.set_index('fecha')[['casos_ac']].plot.line()\n",
      "75/44: confirmados.tail()\n",
      "75/45: confirmados.tail()\n",
      "75/46: confirmados.sort_values('fecha_diagnostico')\n",
      "75/47:\n",
      "pmuerte = []\n",
      "for edad, g in confirmados.query('fecha_diagnostico < \"2020-9-9\"').groupby('edad'):\n",
      "    pmuerte.append([edad, len(g.query('fallecido == \"SI\"'))/len(g)])\n",
      "75/48: pmuerte = pandas.DataFrame(pmuerte, columns=['edad','p'])\n",
      "75/49: pmuerte['p_roll'] = pmuerte.p.rolling(10).mean()\n",
      "75/50: pmuerte.set_index('edad').p_roll.plot.line()\n",
      "75/51: (confirmados.groupby('edad').agg('size') * pmuerte.set_index('edad').p).dropna().sum()\n",
      "75/52: confirmados\n",
      "75/53: confirmados.groupby('fecha_diagnostico')\n",
      "75/54: confirmados.groupby('fecha_diagnostico').agg('size')\n",
      "75/55: confirmados.groupby('fecha_diagnostico').agg('size').plot()\n",
      "75/56: confirmados.groupby('fecha_diagnostico').agg('size').cumsum().plot()\n",
      "75/57: confirmados\n",
      "75/58: confirmados.sort_values('edad_diagnostico')\n",
      "75/59: confirmados.sort_values('fecha_diagnostico')\n",
      "75/60: confirmados.sort_values('fecha_diagnostico').fecha_diagnostico\n",
      "75/61: confirmados.query('fecha_diagnostico > \"2019-09-30\"').sort_values('fecha_diagnostico').fecha_diagnostico\n",
      "75/62: confirmados.query('fecha_diagnostico > \"2019-09-30\"').groupby('fecha_diagnostico').agg('size').cumsum().plot()\n",
      "75/63: confirmados.query('fecha_diagnostico > \"2020-03-30\"').groupby('fecha_diagnostico').agg('size').cumsum().plot()\n",
      "75/64: confirmados.query('fecha_diagnostico > \"2020-03-01\"').groupby('fecha_diagnostico').agg('size').cumsum().plot()\n",
      "75/65: confirmados.query('fecha_diagnostico > \"2020-03-01\"').query('edad > 60').groupby('fecha_diagnostico').agg('size').cumsum().plot()\n",
      "75/66: confirmados.query('fecha_diagnostico > \"2020-03-01\"').query('edad > 70').groupby('fecha_diagnostico').agg('size').cumsum().plot()\n",
      "75/67: confirmados.query('fecha_diagnostico > \"2020-03-01\"').query('edad > 60').groupby('fecha_diagnostico').agg('size').cumsum().plot()\n",
      "76/1: import pandas as pd\n",
      "76/2: df = pd.from_Clipboard(sep='-')\n",
      "76/3: df = pd.read_Clipboard(sep='-')\n",
      "76/4: df = pd.read_clipboard(sep='-')\n",
      "76/5: df\n",
      "76/6: df.columns = ['a','b']\n",
      "76/7: for i in df.b.sort_values():print(i)\n",
      "76/8: df = pd.read_clipboard(sep='-')\n",
      "76/9: df.columns = ['a','b']\n",
      "76/10: for i in df.b.sort_values():print(i)\n",
      "76/11: df = pd.read_clipboard(sep='-')\n",
      "76/12: df.columns = ['a','b']\n",
      "76/13: df.b = db.b.str.capitalize()\n",
      "76/14: df.b = df.b.str.capitalize()\n",
      "76/15: for i in df.b:print(i)\n",
      "76/16: df.b = df.b.str.title()\n",
      "76/17: for i in df.b:print(i)\n",
      "77/1:\n",
      "import pandas\n",
      "from datetime import date\n",
      "77/2:\n",
      "# Load dataframe from source\n",
      "raw_df = pandas.read_csv(\"https://sisa.msal.gov.ar/datos/descargas/covid-19/files/Covid19Casos.csv\")\n",
      "77/3: raw_df.columns = raw_df.columns.str.lower()\n",
      "77/4: today = date.today()\n",
      "77/5: raw_df = raw_df[(raw_df.fecha_diagnostico.isna()) | (raw_df.fecha_diagnostico <= today.strftime(\"%Y-%m-%d\"))] #Sacar fechas posteriores al dia de hoy (mal ingresadas)\n",
      "77/6:\n",
      "for col in raw_df.columns:\n",
      "    if col.startswith('fecha'):\n",
      "        raw_df[col] = pandas.to_datetime(raw_df[col])\n",
      "77/7: time.time()\n",
      "77/8:\n",
      "import pandas\n",
      "from datetime import date\n",
      "77/9:\n",
      "confirmados = raw_df[raw_df['clasificacion_resumen'] == 'Confirmado']\n",
      "sospechosos = raw_df[raw_df['clasificacion_resumen'] == 'Sospechoso']\n",
      "fallecidos = confirmados.loc[confirmados['fallecido'] == \"SI\"]\n",
      "77/10:\n",
      "confirmados = raw_df[raw_df['clasificacion_resumen'] == 'Confirmado']\n",
      "sospechosos = raw_df[raw_df['clasificacion_resumen'] == 'Sospechoso']\n",
      "fallecidos = confirmados.loc[confirmados['fallecido'] == \"SI\"]\n",
      "77/11:\n",
      "prop_fallecidos = fallecidos['sexo'].value_counts()\n",
      "pc_fallecidos = prop_fallecidos / prop_fallecidos.sum() * 100\n",
      "77/12:\n",
      "print(f'Casos al {raw_df[\"ultima_actualizacion\"].max()}:')\n",
      "print('Confirmados:', len(confirmados))\n",
      "print('Sospechosos:', len(sospechosos))\n",
      "print('Fallecidos:', len(fallecidos))\n",
      "print(f'\\t{pc_fallecidos[\"F\"].round(1)} % Mujeres, {pc_fallecidos[\"M\"].round(1)} % Hombres')\n",
      "77/13:\n",
      "casos_x_provincia = (raw_df.pivot_table(index='residencia_provincia_nombre',\n",
      "                                                      columns='clasificacion_resumen', aggfunc='size')\n",
      "                                      .drop(columns='Sin Clasificar').fillna(0)\n",
      "                                      .sort_values('Confirmado',ascending=False)\n",
      "                                      .astype(int)\n",
      "                                      #.assign(total_testeado=lambda x: sum(x))\n",
      "                                  [['Confirmado','Sospechoso','Descartado']])\n",
      "\n",
      "fallecidos_x_provincia = (raw_df.loc[raw_df['fallecido'] == \"SI\"]\n",
      "                                      .pivot_table(index='residencia_provincia_nombre',\n",
      "                                                      columns='clasificacion_resumen', aggfunc='size')\n",
      "                                      #.drop(columns='Sin Clasificar')\n",
      "                                      .fillna(0)\n",
      "                                      .sort_values('Confirmado',ascending=False)\n",
      "                                      .astype(int)\n",
      "                                      #.assign(total_testeado=lambda x: sum(x))\n",
      "                                      [['Confirmado','Sospechoso','Descartado']])\n",
      "77/14:\n",
      "x_provincia = pandas.concat([casos_x_provincia, fallecidos_x_provincia],axis=1, keys=['Casos', 'Fallecidos']).fillna(0).astype(int)\n",
      "x_provincia\n",
      "77/15:\n",
      "top5_provincias_conf = x_provincia['Casos']['Confirmado'].sort_values(ascending=False).head(5)\n",
      "top5_provincias_fa = x_provincia['Fallecidos']['Confirmado'].sort_values(ascending=False).head(5)\n",
      "\n",
      "print('Confirmados:', top5_provincias_conf, sep='\\n')\n",
      "print()\n",
      "print('Fallecidos:', top5_provincias_fa, sep='\\n')\n",
      "77/16: df_topspro = raw_df.loc[raw_df['residencia_provincia_nombre'].isin(top5_provincias_fa.index.tolist())]\n",
      "77/17: pivot = df_topspro.loc[df_topspro['fecha_diagnostico'] >= \"2020-03\"].pivot_table(index= 'fecha_diagnostico', columns=['residencia_provincia_nombre', 'clasificacion_resumen'],aggfunc='size')\n",
      "77/18:\n",
      "dfs = []\n",
      "for provincia in top5_provincias_fa.index:\n",
      "    \n",
      "    dfs.append(\n",
      "        ((pivot[provincia][\"Confirmado\"] / (pivot[provincia][\"Confirmado\"] + pivot[provincia][\"Descartado\"])).\n",
      "        fillna(0).cumsum().to_frame())\n",
      "        \n",
      "    )\n",
      "df_positividad = pandas.concat(dfs,axis=1)\n",
      "df_positividad.columns = top5_provincias_fa.index.tolist()\n",
      "77/19: df_positividad\n",
      "77/20:\n",
      "ax = df_positividad.plot(kind='line', title='Positividad',figsize=(10,4))\n",
      "ax.set_ylabel('Test Positivos / Total Tests')\n",
      "ax.set_xlabel('Fecha')\n",
      "77/21:\n",
      "dfs = []\n",
      "for provincia in top5_provincias_fa.index:\n",
      "    \n",
      "    dfs.append(\n",
      "        ((pivot[provincia][\"Confirmado\"] / (pivot[provincia][\"Confirmado\"] + pivot[provincia][\"Descartado\"])).\n",
      "        fillna(0).to_frame())\n",
      "        \n",
      "    )\n",
      "df_positividad = pandas.concat(dfs,axis=1)\n",
      "df_positividad.columns = top5_provincias_fa.index.tolist()\n",
      "77/22:\n",
      "ax = df_positividad.rolling(20).mean().plot(kind='line', title='Positividad',figsize=(10,4))\n",
      "ax.set_ylabel('Test Positivos / Total Tests')\n",
      "ax.set_xlabel('Fecha')\n",
      "ax.axhline(0.1,color='red')\n",
      "77/23: df_positividad.rolling(10).mean()\n",
      "77/24: fallecidos.pivot_table(index='fecha_fallecimiento',columns='residencia_provincia_nombre',aggfunc='size')[['Buenos Aires','CABA']].plot.line()\n",
      "77/25:\n",
      "fechas_diagnostico = confirmados.query('residencia_provincia_nombre == \"Buenos Aires\"').groupby('fecha_diagnostico').agg('size').to_frame().reset_index()\n",
      "fechas_diagnostico.columns = ['fecha', 'casos']\n",
      "fechas_fallecidos = fallecidos.query('residencia_provincia_nombre == \"Buenos Aires\"').groupby('fecha_fallecimiento').agg('size').to_frame().reset_index()\n",
      "fechas_fallecidos.columns = ['fecha', 'fallecidos']\n",
      "77/26:\n",
      "fechas_ba = fechas_diagnostico.merge(fechas_fallecidos)\n",
      "fechas_ba['casos_ac'] = fechas_ba.casos.cumsum()\n",
      "fechas_ba['fa_ac'] = fechas_ba.fallecidos.cumsum()\n",
      "77/27: fechas_ba.set_index('fecha')['fa_ac'].map(np.log).plot.line()\n",
      "77/28: import numpy as np\n",
      "77/29:\n",
      "fechas_diagnostico = confirmados.query('residencia_provincia_nombre == \"Buenos Aires\"').groupby('fecha_diagnostico').agg('size').to_frame().reset_index()\n",
      "fechas_diagnostico.columns = ['fecha', 'casos']\n",
      "fechas_fallecidos = fallecidos.query('residencia_provincia_nombre == \"Buenos Aires\"').groupby('fecha_fallecimiento').agg('size').to_frame().reset_index()\n",
      "fechas_fallecidos.columns = ['fecha', 'fallecidos']\n",
      "77/30:\n",
      "fechas_ba = fechas_diagnostico.merge(fechas_fallecidos)\n",
      "fechas_ba['casos_ac'] = fechas_ba.casos.cumsum()\n",
      "fechas_ba['fa_ac'] = fechas_ba.fallecidos.cumsum()\n",
      "77/31: fechas_ba.set_index('fecha')['fa_ac'].map(np.log).plot.line()\n",
      "77/32: fechas_ba.set_index('fecha')[['casos_ac']].plot.line()\n",
      "77/33: confirmados.groupby('fecha_diagnostico').agg('size')[-20:]\n",
      "77/34: confirmados.tail()\n",
      "77/35: confirmados.sort_values('fecha_diagnostico')\n",
      "77/36: pivot\n",
      "77/37: confirmados.query('fecha_diagnostico == \"2020-08-29 \"')\n",
      "77/38:\n",
      "import datetime\n",
      "today = datetime.date.today()\n",
      "77/39: raw_df[raw_df.fecha_diagnostico <= today.strftime(\"%Y-%m-%d\")]\n",
      "77/40: import seaborn as sns\n",
      "77/41: confirmados.groupby('edad').agg(lambda x: len([i for i in x.fallecido if i == \"Si\"]) / len(x))\n",
      "77/42: confirmados.loc[confirmados.edad > 90, 'edad'] = 90\n",
      "77/43: (fallecidos.fecha_fallecimiento - fallecidos.fecha_diagnostico)[(fallecidos.fecha_fallecimiento - fallecidos.fecha_diagnostico) > pandas.Timedelta(0)].describe([.95])\n",
      "77/44:\n",
      "pmuerte = []\n",
      "for edad, g in confirmados.query('fecha_diagnostico < \"2020-9-9\"').groupby('edad'):\n",
      "    pmuerte.append([edad, len(g.query('fallecido == \"SI\"'))/len(g)])\n",
      "77/45: pmuerte = pandas.DataFrame(pmuerte, columns=['edad','p'])\n",
      "77/46: pmuerte['p_roll'] = pmuerte.p.rolling(10).mean()\n",
      "77/47: pmuerte.set_index('edad').p_roll.plot.line()\n",
      "77/48: (confirmados.groupby('edad').agg('size') * pmuerte.set_index('edad').p).dropna().sum()\n",
      "77/49:\n",
      "pmuerte = []\n",
      "for edad, g in confirmados.query('fecha_diagnostico < \"2020-9-20\"').groupby('edad'):\n",
      "    pmuerte.append([edad, len(g.query('fallecido == \"SI\"'))/len(g)])\n",
      "77/50: pmuerte = pandas.DataFrame(pmuerte, columns=['edad','p'])\n",
      "77/51: pmuerte['p_roll'] = pmuerte.p.rolling(10).mean()\n",
      "77/52: pmuerte.set_index('edad').p_roll.plot.line()\n",
      "77/53: (confirmados.groupby('edad').agg('size') * pmuerte.set_index('edad').p).dropna().sum()\n",
      "77/54: (fallecidos.fecha_fallecimiento - fallecidos.fecha_diagnostico)\n",
      "77/55: confirmados\n",
      "77/56: confirmados.residencia_provincia_nombre.value_counts()\n",
      "77/57: confirmados.tail()\n",
      "77/58: confirmados.residencia_departamento_nombre.value_counts()\n",
      "77/59: confirmados.residencia_departamento_nombre.value_counts()[:20]\n",
      "77/60: confirmados.residencia_departamento_nombre.value_counts()[:40]\n",
      "77/61:\n",
      "amba_pba = [\"Almirante Brown\",\"Avellaneda\",\"Berazategui\",\"Esteban Echeverría\",\"Ezeiza\",\"Florencio Varela\",\"General San Martín\"\n",
      "            ,\"Hurlingham\",\"Ituzaingó\",\"José C. Paz\",\"La Matanza\",\"Lanús\",\"Lomas de Zamora\",\"Malvinas Argentinas\",\"Merlo\", \"Moreno\",\"Morón\",\"Quilmes\",\"San Fernando\",\"San Isidro\",\"San Miguel\",\n",
      "            \"Tigre\",\"Tres de Febrero\",\"Vicente López\"]\n",
      "77/62: amba_pba\n",
      "77/63: confirmados['amba'] = confirmados.residencia_departamento_nombre.isin[amba_pba]\n",
      "77/64: confirmados['amba'] = confirmados.residencia_departamento_nombre.isin(amba_pba)\n",
      "77/65: confirmados.loc[confirmados.residencia_departamento_nombre == \"CABA\", \"amba\"] = True\n",
      "77/66: confirmados.amba.value_counts()\n",
      "77/67: confirmados.residencia_departamento_nombre.value_counts()\n",
      "77/68: confirmados.residencia_departamento_nombre.value_counts(~confirmados.residencia_departamento_nombre.value_counts().index.isin(amba_pba))\n",
      "77/69: confirmados.residencia_departamento_nombre.value_counts(confirmados.residencia_departamento_nombre.value_counts().index.isin(amba_pba))\n",
      "77/70: confirmados.residencia_departamento_nombre.value_counts()[confirmados.residencia_departamento_nombre.value_counts().index.isin(amba_pba)]\n",
      "77/71: confirmados.residencia_departamento_nombre.value_counts()[~confirmados.residencia_departamento_nombre.value_counts().index.isin(amba_pba)]\n",
      "77/72: confirmados.residencia_departamento_nombre.value_counts()[~confirmados.residencia_departamento_nombre.value_counts().index.isin(amba_pba)][:50]\n",
      "78/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "78/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "78/3:\n",
      "import pandas as pd\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "78/4: db = tablas['human_box1'].drop_duplicates([\"uniprot\",'mlo_loc'])\n",
      "78/5: db['gene_name'] = db.uniprot.map(tablas['gene_names_dict'])\n",
      "78/6: db.mlo_loc = db.mlo_loc.map(tablas['translate'])\n",
      "78/7:\n",
      "basesdatos = tablas['human_box1'].drop_duplicates([\"uniprot\",'web'])\n",
      "basesdatos['gene_name'] = basesdatos.uniprot.map(tablas['gene_names_dict'])\n",
      "ndbs = basesdatos.gene_name.value_counts().to_dict()\n",
      "78/8:\n",
      "pivot = db.pivot_table(index='gene_name',columns='mlo_loc',aggfunc='size')\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "78/9: pivot\n",
      "78/10: pivot.sort_values('suma',ascending=False)\n",
      "78/11: pivot.query('suma >= 5')\n",
      "78/12: pivot.query('suma >= 5').index.tolist()\n",
      "78/13: db = tablas['db'].query('organism == \"Homo sapiens\"')\n",
      "78/14: db\n",
      "78/15: db = tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates(['uniprot','web'])\n",
      "78/16: db\n",
      "78/17: db['gene_name'] = db.uniprot.str.strip().map(tablas['gene_names_dict'])\n",
      "78/18: db\n",
      "78/19: db[db.gene_names.isin(pivot.query('suma >= 5').index.tolist())]\n",
      "78/20: db[db.gene_name.isin(pivot.query('suma >= 5').index.tolist())]\n",
      "78/21: db[db.gene_name.isin(pivot.query('suma >= 5').index.tolist())].gene_name.value_counts()\n",
      "78/22: tablas['human_dis']\n",
      "78/23: tablas['human_dis'].query('mlo_loc == \"nuclear_pore_complex\"')\n",
      "78/24: tablas['human_dis'].query('mlo_loc == \"nuclear_pore_complex\"').sort_values('dc')\n",
      "78/25: tablas['human_dis']['gene_name'] = tablas['human_dis'].uniprot.map(tablas['gene_names_dict'])\n",
      "78/26: tablas['human_dis'].query('mlo_loc == \"nuclear_pore_complex\"').sort_values('dc')\n",
      "78/27: tablas['human_dis'].query('uniprot == \"P52948\"')\n",
      "78/28: plt\n",
      "78/29: plt.plot.bar([591,3209,930])\n",
      "78/30: plt.bar([591,3209,930])\n",
      "78/31: plt.bar(height=[591,3209,930])\n",
      "78/32: plt.bar([1,2,3],[591,3209,930])\n",
      "79/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "79/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "79/3: print(tablas['info'])\n",
      "79/4: tablas['human_dominios_box1']\n",
      "79/5: tablas['human_dominios_box1'].query('domain == \"PLD\"')\n",
      "79/6: tablas['human_dominios_box1'].query('domain == \"PLD\"').drop_duplicates([\"mlo_loc\",'uniprot']).mlo_loc.value_counts()\n",
      "79/7: tablas['human_lc']\n",
      "79/8: tablas['human_lc'].query('uniprot == \"Q8IZL8\"')\n",
      "79/9:\n",
      "tablas['human_lc'].query('uniprot == \"Q8IZL8\"').iloc[1\n",
      "                                                    ]\n",
      "79/10: tablas['human_lc'].query('uniprot == \"Q8IZL8\"').iloc[1].seq\n",
      "79/11:\n",
      "from collections import Counter\n",
      "print(Counter(tablas['human_lc'].query('uniprot == \"Q8IZL8\"').iloc[1].seq))\n",
      "79/12: len(\"EEEEEEEEEEEEEEEEEEEEE\")\n",
      "80/1: !pip install biopython\n",
      "80/2:\n",
      "from Bio import Entrez\n",
      "\n",
      "Entrez.email = \"forti@leloir.org.ar\"\n",
      "\n",
      "handle = Entrez.einfo() # or esearch, efetch, ...\n",
      "\n",
      "record = Entrez.read(handle)\n",
      "\n",
      "handle.close()\n",
      "80/3: handle = Entrez.esummary(db=\"snp\", id=\"rs1000005\")\n",
      "80/4: record = Entrez.read(handle)\n",
      "80/5: handle\n",
      "80/6: record\n",
      "80/7: info = record[0][\"TitleMainList\"][0]\n",
      "80/8: handle = Entrez.esummary(db=\"snp\", id=\"rs1000005\")\n",
      "80/9: record = Entrez.read(handle)\n",
      "80/10: record = Entrez.read(handle, validate=False)\n",
      "80/11: handle = Entrez.esummary(db=\"nlmcatalog\", id=\"101660833\")\n",
      "80/12: record = Entrez.read(handle)\n",
      "80/13: record\n",
      "80/14: handle = Entrez.esummary(db=\"snp\", id=\"rs1000005\")\n",
      "80/15: record = Entrez.read(handle)\n",
      "80/16: handle = Entrez.epost(db=\"snp\", id=\"rs1000005\")\n",
      "80/17: record\n",
      "80/18: handle = Entrez.epost(db=\"snp\", id=\"rs1000005\").read()\n",
      "80/19: handle\n",
      "80/20: print(handle)\n",
      "80/21: handle = Entrez.efetch(db=\"snp\", id=\"rs1000005\").read()\n",
      "81/1: import qgrid\n",
      "81/2: import qgrid2\n",
      "81/3: qgrid\n",
      "81/4: import qgrid\n",
      "81/5:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import qgrid\n",
      "randn = np.random.randn\n",
      "df_types = pd.DataFrame({\n",
      "    'A' : pd.Series(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n",
      "               '2013-01-05', '2013-01-06', '2013-01-07', '2013-01-08', '2013-01-09'],index=list(range(9)),dtype='datetime64[ns]'),\n",
      "    'B' : pd.Series(randn(9),index=list(range(9)),dtype='float32'),\n",
      "    'C' : pd.Categorical([\"washington\", \"adams\", \"washington\", \"madison\", \"lincoln\",\"jefferson\", \"hamilton\", \"roosevelt\", \"kennedy\"]),\n",
      "    'D' : [\"foo\", \"bar\", \"buzz\", \"bippity\",\"boppity\", \"foo\", \"foo\", \"bar\", \"zoo\"] })\n",
      "df_types['E'] = df_types['D'] == 'foo'\n",
      "qgrid_widget = qgrid.show_grid(df_types, show_toolbar=True)\n",
      "qgrid_widget\n",
      "82/1:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "import qgrid\n",
      "randn = np.random.randn\n",
      "df_types = pd.DataFrame({\n",
      "    'A' : pd.Series(['2013-01-01', '2013-01-02', '2013-01-03', '2013-01-04',\n",
      "               '2013-01-05', '2013-01-06', '2013-01-07', '2013-01-08', '2013-01-09'],index=list(range(9)),dtype='datetime64[ns]'),\n",
      "    'B' : pd.Series(randn(9),index=list(range(9)),dtype='float32'),\n",
      "    'C' : pd.Categorical([\"washington\", \"adams\", \"washington\", \"madison\", \"lincoln\",\"jefferson\", \"hamilton\", \"roosevelt\", \"kennedy\"]),\n",
      "    'D' : [\"foo\", \"bar\", \"buzz\", \"bippity\",\"boppity\", \"foo\", \"foo\", \"bar\", \"zoo\"] })\n",
      "df_types['E'] = df_types['D'] == 'foo'\n",
      "qgrid_widget = qgrid.show_grid(df_types, show_toolbar=True)\n",
      "qgrid_widget\n",
      "84/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "import altair as alt\n",
      "import numpy as np\n",
      "84/2:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "84/3:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "84/4: tablas['db_dis']\n",
      "84/5: tablas['db_dis'].drop_duplicates([\"uniprot\",'db'])\n",
      "84/6: tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"')\n",
      "84/7: tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').boxplot('dc','db')\n",
      "84/8: tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').boxplot('dc','db',figsize=(10,4))\n",
      "84/9: sns.boxplot(data=tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"'), x='db', y='dc')\n",
      "84/10: tablas['human_dis']\n",
      "84/11: tablas['human_lc']\n",
      "84/12: tablas['human_largos']\n",
      "84/13: tablas['largos']\n",
      "84/14: tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['largos'])\n",
      "84/15: tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['largos']).boxplot('largo','db')\n",
      "84/16: tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['largos']).boxplot('largo','db', ylim=(0,1000))\n",
      "84/17: tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['largos']).query('largo <= 1000').boxplot('largo','db', )\n",
      "84/18: tidy\n",
      "84/19: tablas['tidy']\n",
      "84/20: tablas['tidy'].columns\n",
      "84/21: tablas['tidy'][['lc_median']]\n",
      "84/22: tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['tidy'][['lc_median']])\n",
      "84/23: tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['tidy'][['uniprot', 'lc_median']])\n",
      "84/24: sns.scatterplot(data=tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['tidy'][['uniprot', 'lc_median']]),x='dc', y='lc_median')\n",
      "84/25: sns.scatterplot(data=tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['tidy'][['uniprot', 'lc_median']]),x='dc', y='lc_median', hue='db')\n",
      "84/26:\n",
      "sns.scatterplot(data=tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['tidy'][['uniprot', 'lc_median']]),x='dc', y='lc_median', hue='db')\n",
      "plt.ylim(0,250)\n",
      "84/27:\n",
      "sns.scatterplot(data=tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['tidy'][['uniprot', 'lc_median']]),x='dc', y='lc_median', hue='web')\n",
      "plt.ylim(0,250)\n",
      "84/28:\n",
      "_ = plt.subplots(figsize=(15,15))\n",
      "sns.scatterplot(data=tablas['db_dis'].drop_duplicates([\"uniprot\",'db']).query('org == \"Homo sapiens\"').merge(tablas['tidy'][['uniprot', 'lc_median']]),x='dc', y='lc_median', hue='web')\n",
      "plt.ylim(0,250)\n",
      "85/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "85/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "85/3: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptm_sites_swiss_prot.csv')\n",
      "85/4: boxes = tablas['human_boxes'][[\"uniprot\",'box']].drop_duplicates()\n",
      "85/5: unis = {b:boxes.query('box == @b').uniprot.tolist() for b in boxes.box.unique()}\n",
      "85/6:\n",
      "unis['sp'] = tablas['human_uniprots_sp']\n",
      "unis['usp'] = [u for u in tablas['human_uniprots_sp'] if u not in tablas['db'].uniprot.tolist()]\n",
      "85/7: [len(i) for i in unis.values()]\n",
      "85/8:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "85/9:\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "print(counts_ht_all)\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', 'sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "85/10:\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "85/11: todo\n",
      "85/12:\n",
      "ptms = ptms_sp.merge(boxes,how='left')\n",
      "ptms.box = ptms.box.fillna('sp')\n",
      "85/13: box_counts = ptms.drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "85/14: ptms['isdom'] = ptms.domain.notna()\n",
      "85/15:\n",
      "p = ptms.pivot_table(index='box',columns='isdom',aggfunc='size')#.apply(lambda x: x[True] / sum(x[True] + x[False]),1)\n",
      "p[True] / (p[False] + p[True])\n",
      "85/16: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_sites.csv')\n",
      "85/17:\n",
      "ptms = ptms_sp.merge(boxes,how='left')\n",
      "ptms.box = ptms.box.fillna('sp')\n",
      "85/18: box_counts = ptms.drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "85/19: ptms['isdom'] = ptms.domain.notna()\n",
      "85/20:\n",
      "p = ptms.pivot_table(index='box',columns='isdom',aggfunc='size')#.apply(lambda x: x[True] / sum(x[True] + x[False]),1)\n",
      "p[True] / (p[False] + p[True])\n",
      "85/21: p\n",
      "85/22:\n",
      "def is_in(pos, df, grupo= '', grupo_col='', start_col='start',end_col='end', ):\n",
      "\n",
      "    if grupo:\n",
      "        df = df[df[grupo_col] == grupo]\n",
      "    for s,e in zip(df[start_col],df[end_col] ):\n",
      "        if pos in pd.Interval(s,e, closed='both'):\n",
      "            return True\n",
      "    return False\n",
      "85/23: print(tablas['info'])\n",
      "85/24: zd = pd.read_csv(\"/home/fernando/git/mlo/todo/tablas/sp_zonas_dis\")\n",
      "85/25:\n",
      "ptms['is_dis'] = ptms[['pos','uniprot']].apply(\n",
      "                            lambda x: is_in(x.pos,zd,grupo=x.uniprot, grupo_col='uniprot'),\n",
      "                            axis=1 )\n",
      "91/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "91/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "91/3: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptm_sites_swiss_prot.csv')\n",
      "91/4:\n",
      "boxes = tablas['human_boxes'].copy()\n",
      "boxes.loc[boxes.box == \"box3\", 'box'] = 'box2'\n",
      "boxes.box = boxes.box.str.replace('box4','box3')\n",
      "boxes = boxes.drop_duplicates()\n",
      "91/5: unis = {b:boxes.query('box == @b').uniprot.tolist() for b in boxes.box.unique()}\n",
      "91/6:\n",
      "unis['sp'] = tablas['human_uniprots_sp']\n",
      "unis['usp'] = [u for u in tablas['human_uniprots_sp'] if u not in tablas['db'].uniprot.tolist()]\n",
      "91/7: [len(i) for i in unis.values()]\n",
      "91/8:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "91/9:\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "print(counts_ht_all)\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', 'sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "91/10:\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "91/11:\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "#plt.savefig('ptms_ht_boxes.svg')\n",
      "91/12:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "91/13:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "91/14: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptm_sites_swiss_prot.csv')\n",
      "91/15: boxes = tablas['human_boxes'].copy()\n",
      "91/16: unis = {b:boxes.query('box == @b').uniprot.tolist() for b in boxes.box.unique()}\n",
      "91/17:\n",
      "unis['sp'] = tablas['human_uniprots_sp']\n",
      "unis['usp'] = [u for u in tablas['human_uniprots_sp'] if u not in tablas['db'].uniprot.tolist()]\n",
      "91/18: [len(i) for i in unis.values()]\n",
      "91/19: boxes = tablas['human_boxes'][[\"uniprot\",'box']].drop_duplicates()\n",
      "91/20: unis = {b:boxes.query('box == @b').uniprot.tolist() for b in boxes.box.unique()}\n",
      "91/21:\n",
      "unis['sp'] = tablas['human_uniprots_sp']\n",
      "unis['usp'] = [u for u in tablas['human_uniprots_sp'] if u not in tablas['db'].uniprot.tolist()]\n",
      "91/22: [len(i) for i in unis.values()]\n",
      "91/23:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "91/24:\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "print(counts_ht_all)\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', 'sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "91/25:\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "91/26:\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "#plt.savefig('ptms_ht_boxes.svg')\n",
      "91/27: tablas.keys()\n",
      "91/28: tablas['colors_3b']\n",
      "91/29: tablas['colors_3b_script']\n",
      "91/30: colors = tablas['colors_3b']\n",
      "91/31:\n",
      "color.insert(0,'white')\n",
      "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n",
      "91/32: colors = tablas['colors_3b']\n",
      "91/33:\n",
      "colors.insert(0,'white')\n",
      "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n",
      "91/34:\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "#plt.savefig('ptms_ht_boxes.svg')\n",
      "91/35: todo\n",
      "91/36: todo\n",
      "91/37: todo.source.replace({'sp':'_sp'})\n",
      "91/38: todo.source = todo.source.replace({'sp':'_sp'})\n",
      "85/26:\n",
      "ptms['is_lc'] = ptms[['pos','uniprot']].apply(\n",
      "                            lambda x: is_in(x.pos,tablas['human_lc'].query('method == \"SEG_intermediate\"'),grupo=x.uniprot, grupo_col='uniprot'),\n",
      "                            axis=1 )\n",
      "91/39:\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "print(counts_ht_all)\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "91/40:\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "#plt.savefig('ptms_ht_boxes.svg')\n",
      "91/41: colors\n",
      "91/42:\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, cmap=['#597dbfff','w'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/43:\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, colors=['#597dbfff','w'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/44:\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, color=['#597dbfff','w'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/45:\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, colormap=['#597dbfff','w'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/46:\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, palette\n",
      "                 =['#597dbfff','w'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/47:\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','_sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, palette=['#597dbfff','w'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/48:\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','_sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, palette=['w','#597dbfff'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/49:\n",
      "_ = plt.subplots(figsize=(4,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','_sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, palette=['w','#597dbfff'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/50:\n",
      "_ = plt.subplots(figsize=(5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','_sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, palette=['w','#597dbfff'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/51:\n",
      "_ = plt.subplots(figsize=(4,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin(['box1','_sp','usp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True, palette=['w','#597dbfff'])\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "ax.set_ylabel(\"n PTMs\",fontsize=14)\n",
      "#plt.savefig('ptms_ht_box1.svg')\n",
      "91/52: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_reg_swiss_prot.csv')\n",
      "85/27: ptms\n",
      "85/28:\n",
      "p = ptms.pivot_table(index='box',columns='is_dis',aggfunc='size')#.apply(lambda x: x[True] / sum(x[True] + x[False]),1)\n",
      "p[True] / (p[False] + p[True])\n",
      "85/29: zd_box = zd.merge(boxes, how='left').fillna(\"sp\")\n",
      "85/30: largos = zd_box.groupby('box').agg('sum')[\"largo\"]\n",
      "85/31: p[True] / largos\n",
      "91/53:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.tight_layout();\n",
      "#plt.savefig('ptms_reg_boxes.svg')\n",
      "91/54:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=30);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/55:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=30);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/56:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=10);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/57:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/58:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/59:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "plt.yticks(figsize=15)\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/60:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "plt.y_ticks(figsize=15)\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/61:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "plt.ylabels(figsize=15)\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/62:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "plt.yticks(figsize=15)\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/63:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "plt.yticks(size=15)\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/64:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "plt.yticks(fontsize=15)\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/65:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "plt.yticks(fontsize=20)\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/66:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "plt.xticks(fontsize=20)\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/67:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=30 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/68:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=15 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/69:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/70:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "#plt.savefig('ptms_dis_boxes.svg')\n",
      "91/71:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=30);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/72:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "#plt.savefig('ptms_dis_boxes.svg')\n",
      "91/73:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=30);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/74: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_dis_swiss_prot.csv')\n",
      "91/75:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "#plt.savefig('ptms_dis_boxes.svg')\n",
      "91/76:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=30 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=30);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/77:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=15 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=30);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/78:\n",
      "\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/79:\n",
      "_ = plt.subplots(figsize=(4,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/80:\n",
      "_ = plt.subplots(figsize=(4,5))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/81:\n",
      "_ = plt.subplots(figsize=(4,2))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/82:\n",
      "_ = plt.subplots(figsize=(4,2))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/83:\n",
      "#_ = plt.subplots(figsize=(4,2))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/84:\n",
      "_ = plt.subplots(figsize=(4,6))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/85:\n",
      "_ = plt.subplots(figsize=(6,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/86:\n",
      "#_ = plt.subplots(figsize=(6,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/87:\n",
      "_ = plt.subplots(figsize=(5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/88:\n",
      "_ = plt.subplots(figsize=(4,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/89:\n",
      "_ = plt.subplots(figsize=(3,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/90:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_reg_b1.svg')\n",
      "91/91:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "#plt.savefig('ptms_dis_b1.svg')\n",
      "91/92:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "plt.savefig('new_ptms_reg_b1.svg')\n",
      "91/93:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "as.ylabel('n PTMS')\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_dis_b1.svg')\n",
      "91/94:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "ax.ylabel('n PTMS')\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_dis_b1.svg')\n",
      "91/95:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "ax.ylab('n PTMS')\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_dis_b1.svg')\n",
      "91/96:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "ax.set_ytick('n PTMS')\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_dis_b1.svg')\n",
      "91/97:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "plt.ylab('n PTMS')\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_dis_b1.svg')\n",
      "91/98:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "plt.ylabel('n PTMS')\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_dis_b1.svg')\n",
      "91/99:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_dis_b1.svg')\n",
      "91/100: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_reg_swiss_prot.csv')\n",
      "91/101:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.tight_layout();\n",
      "#plt.savefig('ptms_reg_boxes.svg')\n",
      "91/102:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "plt.savefig('new_ptms_reg_b1.svg')\n",
      "91/103:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "plt.tittle('Regulatories PTMs')\n",
      "plt.savefig('new_ptms_reg_b1.svg')\n",
      "91/104:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "plt.title('Regulatories PTMs')\n",
      "plt.savefig('new_ptms_reg_b1.svg')\n",
      "91/105:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "plt.title('Regulatories PTMs',fontsize=16)\n",
      "plt.savefig('new_ptms_reg_b1.svg')\n",
      "91/106:\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo[todo.source.isin([\"box1\",'_sp'])].sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=16 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=15);\n",
      "\n",
      "plt.savefig('new_ptms_reg_b1.svg')\n",
      "91/107: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_reg_swiss_prot.csv')\n",
      "91/108:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "91/109:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a['box3'] = 'Box 3 \\n (333)'\n",
      "91/110:\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.tight_layout();\n",
      "#plt.savefig('ptms_reg_boxes.svg')\n",
      "91/111: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_reg_swiss_prot.csv')\n",
      "91/112:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} \\n ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "91/113:\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.tight_layout();\n",
      "#plt.savefig('ptms_reg_boxes.svg')\n",
      "91/114:\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.tight_layout();\n",
      "#plt.savefig('ptms_reg_boxes.svg')\n",
      "91/115:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}/n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "#plt.savefig('ptms_dis_boxes.svg')\n",
      "91/116:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "#plt.savefig('ptms_dis_boxes.svg')\n",
      "91/117:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "#plt.savefig('ptms_dis_boxes.svg')\n",
      "91/118: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_dis_swiss_prot.csv')\n",
      "91/119:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "#plt.savefig('ptms_dis_boxes.svg')\n",
      "91/120:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "91/121:\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.tight_layout();\n",
      "#plt.savefig('ptms_reg_boxes.svg')\n",
      "91/122: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_reg_swiss_prot.csv')\n",
      "91/123:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.tight_layout();\n",
      "#plt.savefig('ptms_reg_boxes.svg')\n",
      "91/124:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.tight_layout();\n",
      "plt.savefig('new_ptms_reg_boxes.svg')\n",
      "91/125:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "plt.savefig('new_ptms_dis_boxes.svg')\n",
      "91/126:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "\n",
      "plt.savefig('new_ptms_reg_boxes.svg')\n",
      "91/127:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "\n",
      "plt.tight_layout();\n",
      "plt.savefig('new_ptms_reg_boxes.svg')\n",
      "91/128:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_reg_boxes.svg')\n",
      "91/129: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_dis_swiss_prot.csv')\n",
      "91/130:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "plt.savefig('new_ptms_dis_boxes.svg')\n",
      "91/131:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "_ = plt.subplots(figsize=(3.5,4))\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "#plt.savefig('new_ptms_dis_boxes.svg')\n",
      "91/132:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "_ = plt.subplots(figsize=(4,4))\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "#plt.savefig('new_ptms_dis_boxes.svg')\n",
      "91/133:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "_ = plt.subplots(figsize=(4.5,4))\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "#plt.savefig('new_ptms_dis_boxes.svg')\n",
      "91/134: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_dis_swiss_prot.csv')\n",
      "91/135:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "_ = plt.subplots(figsize=(4.5,4))\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "#ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=16 );\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "plt.savefig('new_ptms_dis_boxes.svg')\n",
      "91/136: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptms_reg_swiss_prot.csv')\n",
      "91/137:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "todo.source = todo.source.replace({'sp':'_sp'})\n",
      "\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "\n",
      "\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "_ = plt.subplots(figsize=(4.5,4))\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_reg_boxes.svg')\n",
      "91/138:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "91/139: todo.source = todo.source.replace({'sp':'_sp'})\n",
      "91/140:\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "print(counts_ht_all)\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]} ({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "91/141:\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "91/142:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "_ = plt.subplots(figsize=(4.5,4))\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],rotation=45, horizontalalignment='right',fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_boxes.svg')\n",
      "91/143:\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "print(counts_ht_all)\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "91/144: colors = tablas['colors_3b']\n",
      "91/145:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "_ = plt.subplots(figsize=(4.5,4))\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_boxes.svg')\n",
      "91/146:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "91/147: todo.source = todo.source.replace({'sp':'_sp'})\n",
      "91/148:\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "print(counts_ht_all)\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "91/149: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptm_sites_swiss_prot.csv')\n",
      "91/150: boxes = tablas['human_boxes'][[\"uniprot\",'box']].drop_duplicates()\n",
      "91/151: unis = {b:boxes.query('box == @b').uniprot.tolist() for b in boxes.box.unique()}\n",
      "91/152:\n",
      "unis['sp'] = tablas['human_uniprots_sp']\n",
      "unis['usp'] = [u for u in tablas['human_uniprots_sp'] if u not in tablas['db'].uniprot.tolist()]\n",
      "91/153: [len(i) for i in unis.values()]\n",
      "91/154:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "91/155:\n",
      "# drop usp:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "91/156:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "_ = plt.subplots(figsize=(4.5,4))\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_boxes.svg')\n",
      "91/157:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "91/158: todo.source = todo.source.replace({'sp':'_sp'})\n",
      "91/159:\n",
      "counts_ht_all = todo.drop_duplicates([\"uniprot\",'source']).source.value_counts()\n",
      "print(counts_ht_all)\n",
      "box_fancy = {'box1': 'Box 1', 'box2':'Box 2', 'box3':'Box 3', '_sp':'Swiss-Prot', 'usp':'Swiss-Prot No MLO'}\n",
      "labels_h_a = { k:f'{box_fancy[k]}\\n({v})' for k,v in counts_ht_all.to_dict().items()}\n",
      "labels_h_a\n",
      "91/160:\n",
      "todo = todo.query('source != \"usp\"')\n",
      "_ = plt.subplots(figsize=(4.5,4))\n",
      "\n",
      "ax = sns.boxplot(data=todo.sort_values('source'),x='source',y='counts',showfliers=False,notch=True)\n",
      "#a = ax.get_xticklabels()\n",
      "ax.set_xticklabels([labels_h_a[i.get_text()] for i in ax.get_xticklabels()],fontsize=14 );\n",
      "\n",
      "ax.set_yticklabels([int(i) for i in ax.get_yticks()],fontsize=14);\n",
      "plt.ylabel('n PTMS', fontsize=16)\n",
      "\n",
      "\n",
      "plt.savefig('new_ptms_boxes.svg')\n",
      "93/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "93/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "93/3: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptm_sites_swiss_prot.csv')\n",
      "93/4: boxes = tablas['human_boxes'][[\"uniprot\",'box']].drop_duplicates()\n",
      "93/5: unis = {b:boxes.query('box == @b').uniprot.tolist() for b in boxes.box.unique()}\n",
      "93/6:\n",
      "unis['sp'] = tablas['human_uniprots_sp']\n",
      "unis['usp'] = [u for u in tablas['human_uniprots_sp'] if u not in tablas['db'].uniprot.tolist()]\n",
      "93/7: [len(i) for i in unis.values()]\n",
      "93/8:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "93/9: todo\n",
      "93/10: ptms_sp\n",
      "93/11: *tablas.keys()\n",
      "93/12: *tablas.keys(),\n",
      "93/13:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "93/14:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "93/15: data = pd.read_csv('/home/fernando/Downloads/mobidb_result_.tsv')\n",
      "93/16: data = pd.read_csv('/home/fernando/Downloads/mobidb_result_.tsv', sep='\\t')\n",
      "93/17: len(data)\n",
      "93/18: *data.columns,\n",
      "93/19: data.columns\n",
      "93/20: len(data.acc.unique())\n",
      "93/21: tablas['human_db']\n",
      "93/22: tablas['human']\n",
      "93/23: *tablas.keys(),\n",
      "93/24: db = tablas.query('organism == \"Homo sapiens\"')\n",
      "93/25: db = tablas[\"db\"].query('organism == \"Homo sapiens\"')\n",
      "93/26: db\n",
      "93/27: sp = pd.read_csv(\"/home/fernando/git/mlo/todo/tablas/swiss_prot.tab\", sep='\\')\n",
      "93/28: sp = pd.read_csv(\"/home/fernando/git/mlo/todo/tablas/swiss_prot.tab\", sep='\\t')\n",
      "93/29: sp.columns\n",
      "93/30: data[data.acc.isin(sp.Entry)]\n",
      "93/31: data = data[data.acc.isin(sp.Entry)]\n",
      "93/32: data.feature.value_counts()\n",
      "93/33: data.feature.value_counts()[:30]\n",
      "93/34: data.query('acc == \"P04637\"')\n",
      "93/35: data\n",
      "93/36: data.query('feature == \"curated-disorder-idea\"')\n",
      "93/37: data.query('feature == \"curated-disorder-ideal\"')\n",
      "93/38: data.query('feature == \"curated-disorder-merge\"')\n",
      "93/39: data.query('acc == \"P62258\"')\n",
      "93/40: data.query('feature == \"prediction-disorder-th_50\"')\n",
      "93/41: data.query('feature == \"prediction-disorder-th_50\"').assign(lambda df: df['regions'] = df['start..end'].str.split(\",\"))\n",
      "93/42: data.query('feature == \"prediction-disorder-th_50\"').assign(regions = lambda df: f['start..end'].str.split(\",\"))\n",
      "93/43:\n",
      "data.query('feature == \"prediction-disorder-th_50\"').assign(regions = lambda df: d\n",
      "                                                            f['start..end'].str.split(\",\"))\n",
      "93/44: data.query('feature == \"prediction-disorder-th_50\"').assign(regions = lambda df: df['start..end'].str.split(\",\"))\n",
      "93/45: data.query('feature == \"prediction-disorder-th_50\"').assign(regions = lambda df: df['start..end'].str.split(\",\")).explode('regions')\n",
      "93/46: data.query('feature == \"prediction-disorder-th_50\"').assign(regions = lambda df: df['start..end'].str.split(\",\")).explode('regions').assign(start, end = lambda df: df.regions.str.split(\"..\", expand=True))\n",
      "93/47: data.query('feature == \"prediction-disorder-th_50\"').assign(regions = lambda df: df['start..end'].str.split(\",\")).explode('regions').assign((start, end) = lambda df: df.regions.str.split(\"..\", expand=True))\n",
      "93/48: data.query('feature == \"prediction-disorder-th_50\"').assign(regions = lambda df: df['start..end'].str.split(\",\")).explode('regions').assign(start = lambda df: df.regions.str.split(\"..\", expand=True).str[0],end = lambda df: df.regions.str.split(\"..\", expand=True).str[1])\n",
      "93/49: data.query('feature == \"prediction-disorder-th_50\"').assign(regions = lambda df: df['start..end'].str.split(\",\")).explode('regions').assign(start = lambda df: df.regions.str.split(\"..\", expand=True)[0],end = lambda df: df.regions.str.split(\"..\", expand=True)[1])\n",
      "93/50: df.regions.str.split(\"..\", expand=True)\n",
      "93/51: regions_dis = data.query('feature == \"prediction-disorder-th_50\"').assign(regions = lambda df: df['start..end'].str.split(\",\")).explode('regions')#.assign(start = lambda df: df.regions.str.split(\"..\", expand=True)[0],end = lambda df: df.regions.str.split(\"..\", expand=True)[1])\n",
      "93/52: regions_dis.regions.str.split(\"..\", expand=True)\n",
      "93/53: regions_dis\n",
      "93/54: regions_dis.regions.str.split(\"..\")\n",
      "93/55: regions_dis.regions[13]\n",
      "93/56: regions_dis.regions[13][0]\n",
      "93/57: regions_dis.regions[13].iloc[0]\n",
      "93/58: regions_dis.regions[13].iloc[0].split(\"..\")\n",
      "93/59: regions_dis\n",
      "93/60: regions_dis.regions\n",
      "93/61: regions_dis.regions.str.split(\"..\")\n",
      "93/62: regions_dis.regions.str.replace('..','__')\n",
      "93/63: regions_dis.regions.str.replace(r'..','__')\n",
      "93/64: regions_dis.regions.str.replace(r'.','__')\n",
      "93/65: regions_dis.regions.str.split(\".\",expand=True)\n",
      "93/66: regions_dis['start'], regions_dis['end'] = regions_dis.regions.str.split(\".\",expand=True).iloc[:,[0,2]]\n",
      "93/67: region_dis\n",
      "93/68: regions_dis\n",
      "93/69: regions_dis['start'], regions_dis[\"_\"] regions_dis['end'] = regions_dis.regions.str.split(\".\",expand=True)\n",
      "93/70: regions_dis['start'], regions_dis[\"_\"], regions_dis['end'] = regions_dis.regions.str.split(\".\",expand=True)\n",
      "93/71: regions_dis\n",
      "93/72: regions_dis.regions.str.split(\".\",expand=True)\n",
      "93/73: regions_dis.concat(regions_dis.regions.str.split(\".\",expand=True))\n",
      "93/74: pc.concat(regions_dis, regions_dis.regions.str.split(\".\",expand=True))\n",
      "93/75: pd.concat(regions_dis, regions_dis.regions.str.split(\".\",expand=True))\n",
      "93/76: pd.concat([regions_dis, regions_dis.regions.str.split(\".\",expand=True)])\n",
      "93/77: pd.concat([regions_dis, regions_dis.regions.str.split(\".\",expand=True)], axis=1)\n",
      "93/78: region_dis = pd.concat([regions_dis, regions_dis.regions.str.split(\".\",expand=True)], axis=1)\n",
      "93/79: regions_dis.start = regions_dis[0]\n",
      "93/80: regions_dis.start = regions_dis.iloc[:,0]\n",
      "93/81: regions_dis.end = regions_dis.iloc[:,1]\n",
      "93/82: regions_dis\n",
      "93/83: region_dis = pd.concat([regions_dis, regions_dis.regions.str.split(\".\",expand=True)], axis=1)\n",
      "93/84: regions_dis.start = regions_dis.loc[:,0]\n",
      "98/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "98/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "98/3:\n",
      "db = tablas[\"db\"].query('organism == \"Homo sapiens\"')\n",
      "sp = pd.read_csv(\"/home/fernando/git/mlo/todo/tablas/swiss_prot.tab\", sep='\\t')\n",
      "data = pd.read_csv('/home/fernando/Downloads/mobidb_result_.tsv', sep='\\t')\n",
      "data = data[data.acc.isin(sp.Entry)]\n",
      "98/4: dis = data.query('feature == \"prediction-disorder-th_50\"').assign(lambda df: df['regions'] = df['start..end'].str.split(\",\"))\n",
      "98/5: dis = data.query('feature == \"prediction-disorder-th_50\"').assign(regiones = lambda df: df['regions'] = df['start..end'].str.split(\",\"))\n",
      "98/6: dis = data.query('feature == \"prediction-disorder-th_50\"').assign(regiones = lambda df:  df['start..end'].str.split(\",\"))\n",
      "98/7: dis\n",
      "98/8: dis = dis.explode('regiones')\n",
      "98/9: dis\n",
      "98/10: dis.regiones.str.split('.')\n",
      "98/11: dis.regiones.str.split('..')\n",
      "98/12: dis.regiones.str.split('.')\n",
      "98/13: dis.regiones.str.replace('.',\"_\")\n",
      "98/14: dis.regiones.str.replace('.',\"_\").str.split('__')\n",
      "98/15: dis.regiones.str.replace('.',\"_\").str.split('__',expand=True)\n",
      "98/16: dis[['start', 'end']]dis.regiones.str.replace('.',\"_\").str.split('__',expand=True)\n",
      "98/17: dis[['start', 'end']] = dis.regiones.str.replace('.',\"_\").str.split('__',expand=True)\n",
      "98/18: dis\n",
      "98/19: dis['largo'] = dis.end - dis.start + 1\n",
      "98/20: dis[['start', 'end']] = dis.regiones.str.replace('.',\"_\").str.split('__',expand=True).applymap(int)\n",
      "98/21: dis['largo'] = dis.end - dis.start + 1\n",
      "98/22: dis\n",
      "98/23:\n",
      "def is_in(uniprot, pos, df):\n",
      "    df = df[df.uniprot == uniprot]\n",
      "    for i, row in df.iterrows():\n",
      "        if pos in pd.Interval(row.star, row.end, True):\n",
      "            return True\n",
      "    return False\n",
      "98/24: dis\n",
      "98/25: dis['uniprot'] = dis.acc\n",
      "98/26: dis\n",
      "98/27: is_in(\"P31946\", 7, dis)\n",
      "98/28:\n",
      "def is_in(uniprot, pos, df):\n",
      "    df = df[df.uniprot == uniprot]\n",
      "    for i, row in df.iterrows():\n",
      "        if pos in pd.Interval(row.start, row.end, True):\n",
      "            return True\n",
      "    return False\n",
      "98/29:\n",
      "def is_in(uniprot, pos, df):\n",
      "    df = df[df.uniprot == uniprot]\n",
      "    for i, row in df.iterrows():\n",
      "        if pos in pd.Interval(row.start, row.end, True):\n",
      "            return True\n",
      "    return False\n",
      "98/30: is_in(\"P31946\", 7, dis)\n",
      "98/31:\n",
      "def is_in(uniprot, pos, df):\n",
      "    df = df[df.uniprot == uniprot]\n",
      "    for i, row in df.iterrows():\n",
      "        if pos in pd.Interval(row.start, row.end, \"both\"):\n",
      "            return True\n",
      "    return False\n",
      "98/32: is_in(\"P31946\", 7, dis)\n",
      "98/33: is_in(\"P31946\", 8, dis)\n",
      "98/34: is_in(\"P31946\", 9, dis)\n",
      "98/35: is_in(\"P31946\", 10, dis)\n",
      "98/36: ptms_sp\n",
      "98/37: ptms_sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/ptm_sites_swiss_prot.csv')\n",
      "98/38: boxes = tablas['human_boxes'][[\"uniprot\",'box']].drop_duplicates()\n",
      "98/39: unis = {b:boxes.query('box == @b').uniprot.tolist() for b in boxes.box.unique()}\n",
      "98/40:\n",
      "unis['sp'] = tablas['human_uniprots_sp']\n",
      "unis['usp'] = [u for u in tablas['human_uniprots_sp'] if u not in tablas['db'].uniprot.tolist()]\n",
      "98/41: [len(i) for i in unis.values()]\n",
      "98/42: unis\n",
      "98/43: boxes\n",
      "98/44: ptms_sp\n",
      "98/45:\n",
      "is_in_list = []\n",
      "for i,row in ptms_sp.iterrows():\n",
      "    is_in_list.append(is_in(row.uniprot, row.pos, dis))\n",
      "98/46:\n",
      "is_in_list = []\n",
      "for i,row in ptms_sp.head().iterrows():\n",
      "    is_in_list.append(is_in(row.uniprot, row.pos, dis))\n",
      "98/47:\n",
      "is_in_list = []\n",
      "for i,row in ptms_sp.head(10000).iterrows():\n",
      "    is_in_list.append(is_in(row.uniprot, row.pos, dis))\n",
      "98/48:\n",
      "is_in_list = []\n",
      "for n, (i,row) in enumerate(ptms_sp.head(10000).iterrows()):\n",
      "    is_in_list.append(is_in(row.uniprot, row.pos, dis))\n",
      "    if not n % 50:\n",
      "        print(n, len(ptms_sp))\n",
      "98/49:\n",
      "is_in_list = []\n",
      "for n, (i,row) in enumerate(ptms_sp.iterrows()):\n",
      "    is_in_list.append(is_in(row.uniprot, row.pos, dis))\n",
      "    if not n % 1000:\n",
      "        print(n, len(ptms_sp))\n",
      "100/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "import localcider\n",
      "100/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "100/3:\n",
      "def max_polisize(seq):\n",
      "    # Calcula mayor largo de secuencia de aminoacidos iguales\n",
      "    last = seq[0]\n",
      "    c = 0\n",
      "    zonas = []\n",
      "\n",
      "    for char in seq[1:]:\n",
      "        if char == last:\n",
      "            c +=1\n",
      "        else:\n",
      "            zonas.append(c)\n",
      "            c = 0\n",
      "        last = char\n",
      "    zonas.append(c)\n",
      "    return max(zonas)\n",
      "100/4:\n",
      "lc = tablas['human_lc_box1'].query('method == \"SEG_intermediate\"').copy()\n",
      "\n",
      "db = tablas['human_box1'][['uniprot','mlo_loc']].drop_duplicates()\n",
      "100/5: lc_db = lc.merge(tablas['human_box1'][['uniprot','mlo_loc']], how='left').drop_duplicates()\n",
      "100/6: plaac = tablas['human_dominios_box1'].query('source == \"plaac\"')\n",
      "100/7:\n",
      "# PLAAC\n",
      "plaac['gene_name'] = plaac.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "print('cantidad de proteinas con PLD:',len(plaac.uniprot.unique()))\n",
      "100/8:\n",
      "plaac_control = pd.read_csv(\"/home/fernando/datos/plaac_swiss_prot_human.tsv\",sep='\\t')\n",
      "\n",
      "\n",
      "plaac_control[\"uniprot\"] = plaac_control.SEQid.str.split(\"|\").str[1]\n",
      "\n",
      "plaac_control = plaac_control[~plaac_control.COREscore.isna()]\n",
      "100/9:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      "100/10:\n",
      "base = sp.copy()\n",
      "base['clase'] = 'no_lc'\n",
      "100/11:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "100/12: base.loc[base.uniprot.isin(lc_control.uniprot),'clase'] = 'lc'\n",
      "100/13: base.loc[base.uniprot.isin(plaac_control.uniprot), 'clase'] = 'featured'\n",
      "100/14: db = tablas['human_boxes'][['uniprot','box']].drop_duplicates()\n",
      "100/15:\n",
      "final = base.merge(db, how='left')\n",
      "final.box = final.box.fillna(\"control\")\n",
      "100/16: pivot = final.pivot_table(index='box',columns='clase',aggfunc='size')\n",
      "100/17:\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot.suma\n",
      "pivot = pivot.fillna(0)\n",
      "100/18: pivot['suma'] = pivot.featured + pivot.lc\n",
      "100/19: pivot.index = pivot.index.map(tablas['translate'])\n",
      "100/20: pivot\n",
      "100/21: dv\n",
      "100/22: db\n",
      "100/23: db = tablas['human_boxes'][['uniprot','box']].drop_duplicates()\n",
      "100/24:\n",
      "final = base.merge(db, how='left')\n",
      "final.box = final.box.fillna(\"control\")\n",
      "100/25: final\n",
      "100/26: pivot = final.pivot_table(index='box',columns='clase',aggfunc='size')\n",
      "100/27: pivot\n",
      "100/28:\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot.suma\n",
      "pivot = pivot.fillna(0)\n",
      "100/29: pivot\n",
      "100/30: pivot['suma'] = pivot.featured + pivot.lc\n",
      "100/31: pivot\n",
      "100/32: lc_db\n",
      "100/33: lc_db[['uniprot','seq']]\n",
      "100/34:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq','mlo_loc']]\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "100/35: seqs\n",
      "100/36: lc_db\n",
      "100/37: lc.merge(boxes)\n",
      "100/38: lc.merge(tablas[\"human_boxes\"])\n",
      "100/39:\n",
      "lc_db = lc.merge(tablas[\"human_boxes\"])\n",
      "lc_db['mlo_loc'] = lc_db.box\n",
      "100/40:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq','mlo_loc']]\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "100/41: seqs = seqs.explode('mlo_loc').drop_duplicates()\n",
      "100/42: seqs\n",
      "100/43: seqs = seqs.groupby('mlo_loc').agg(lambda x: ''.join(x))\n",
      "100/44:\n",
      "sp = sp[~sp.seq.str.contains(\"U\")]\n",
      "seqs.loc['control','seq'] = \"\".join(sp.drop_duplicates().seq)\n",
      "100/45:\n",
      "aas = \"\".join(sp[:10].seq.values)\n",
      "aas = list(set(aas))\n",
      "100/46: from localcider.sequenceParameters import SequenceParameters\n",
      "100/47: seqs\n",
      "100/48: from collections import Counter\n",
      "100/49:\n",
      "freqs = {}\n",
      "for mlo,seq in seqs.seq.items():\n",
      "    counts = Counter(seq)\n",
      "    freq = {k:v/len(seq) for k,v in counts.items()}\n",
      "    freqs[mlo] = freq\n",
      "100/50: freq_df = pd.DataFrame.from_dict(freqs)\n",
      "100/51:\n",
      "for col in freq_df.columns:\n",
      "    freq_df[col] = (freq_df[col] - freq_df.control) / freq_df.control\n",
      "100/52: freq_df[\"suma\"] = freq_df.sum(1)\n",
      "100/53:\n",
      "tablas['translate']['suma'] = 'suma'\n",
      "tablas['translate']['control'] = 'Swiss-Prot'\n",
      "100/54:\n",
      "#_ = plt.subplots(figsize=(8,12))\n",
      "ax = sns.heatmap(freq_df[pivot.fillna(0).sort_values('suma',ascending=False).index.tolist() + [\"suma\"]].sort_values('suma',ascending=False).T.drop('suma'),cmap='RdBu_r',  center=0,\n",
      "            square=True, linewidths=.5)#, cbar_kws={\"shrink\": .2})\n",
      "plt.yticks(fontsize=13);\n",
      "#plt.savefig('lc_heatmap_colormap.svg')\n",
      "106/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "106/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "106/3: tablas['db'].query('organism = \"Homo sapiens\"')\n",
      "106/4: tablas['db'].query('organism == \"Homo sapiens\"')\n",
      "106/5: tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'web'])\n",
      "106/6: for i in tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'web']).query('web == \"phasepdb\"').unique():print(i)\n",
      "106/7: for i in tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'web']).query('web == \"phasepdb\"').uniprot.unique():print(i)\n",
      "106/8: for i in tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'web']).query('web == \"phapsepro\"').uniprot.unique():print(i)\n",
      "106/9: %clean\n",
      "106/10: clean\n",
      "106/11: !clean\n",
      "106/12: for i in tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'web']).query('web == \"phasepro\"').uniprot.unique():print(i)\n",
      "106/13: for i in tablas['db'].query('organism == \"Homo sapiens\"').drop_duplicates([\"uniprot\",'web']).query('web == \"drllps\"').uniprot.unique():print(i)\n",
      "107/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "107/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "107/3: tablas['db']\n",
      "107/4: tablas['db'].drop_duplicates([\"uniprot\",'web'])\n",
      "107/5: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('web == \"phasepdb\"').uniprot.unique():print(i)\n",
      "107/6: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('web == \"phasepro\"').uniprot.unique():print(i)\n",
      "107/7: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('web == \"drllps\"').uniprot.unique():print(i)\n",
      "107/8: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org != \"Homo sapiens\"').query('web == \"phasepdb\"').uniprot.unique():print(i)\n",
      "107/9: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org != \"Homo sapiens\"').query('web == \"phasepro\"').uniprot.unique():print(i)\n",
      "107/10: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org != \"Homo sapiens\"').query('web == \"drllps\"').uniprot.unique():print(i)\n",
      "107/11: tablas['db']\n",
      "107/12: tablas['db'].org_short.value_counts()\n",
      "107/13: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org != \"other\"').query('web == \"drllps\"').uniprot.unique():print(i)\n",
      "107/14: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org != \"other\"').query('web == \"phasepdb\"').uniprot.unique():print(i)\n",
      "107/15: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org != \"other\"').query('web == \"phasepro\"').uniprot.unique():print(i)\n",
      "107/16: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org != \"other\"').query('web == \"drllps\"').uniprot.unique():print(i)\n",
      "107/17: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org == \"other\"').query('web == \"drllps\"').uniprot.unique():print(i)\n",
      "107/18: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org_short == \"other\"').query('web == \"phasepdb\"').uniprot.unique():print(i)\n",
      "107/19: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org_short == \"other\"').query('web == \"phasepro\"').uniprot.unique():print(i)\n",
      "107/20: for i in tablas['db'].drop_duplicates([\"uniprot\",'web']).query('org_short == \"other\"').query('web == \"drllps\"').uniprot.unique():print(i)\n",
      "113/1: import pandas as pd\n",
      "113/2: lista = pd.read_html('https://edition.cnn.com/election/2020/results/state/pennsylvania/president')\n",
      "113/3: lista\n",
      "113/4: import pandas as pd\n",
      "113/5: pd.read_html('https://edition.cnn.com/election/2020/results/state/georgia')\n",
      "113/6: from bs4 import BeautifulSoup\n",
      "113/7:\n",
      "with open('https://edition.cnn.com/election/2020/results/state/georgia', 'r') as f:\n",
      "\n",
      "    contents = f.read()\n",
      "113/8: resp = req.get('https://edition.cnn.com/election/2020/results/state/georgia')\n",
      "113/9: import requests as req\n",
      "113/10: resp = req.get('https://edition.cnn.com/election/2020/results/state/georgia')\n",
      "113/11: soup = BeautifulSoup(resp.text, 'html')\n",
      "113/12:\n",
      "print(soup.title)\n",
      "print(soup.title.text)\n",
      "print(soup.title.parent)\n",
      "113/13: soup = BeautifulSoup(resp.text, 'lxml')\n",
      "113/14:\n",
      "print(soup.title)\n",
      "print(soup.title.text)\n",
      "print(soup.title.parent)\n",
      "113/15: soup\n",
      "113/16: soup.children()\n",
      "113/17: soup.children\n",
      "113/18: for i in soup.children:print(i)\n",
      "113/19: soup.title\n",
      "113/20:    print(f'HTML: {soup.h2}, name: {soup.h2.name}, text: {soup.h2.text}')\n",
      "113/21:\n",
      "for child in soup.recursiveChildGenerator():\n",
      "\n",
      "    if child.name:\n",
      "\n",
      "        print(child.name)\n",
      "113/22:\n",
      "root_childs = [e.name for e in root.children if e.name is not None]\n",
      "print(root_childs)\n",
      "113/23: root = soup.html\n",
      "113/24:\n",
      "root_childs = [e.name for e in root.children if e.name is not None]\n",
      "print(root_childs)\n",
      "113/25: for i in root:print(i)\n",
      "113/26: for i in list(root[:10]):print(i)\n",
      "113/27: for i in list(root)[:20]:print(i)\n",
      "113/28: for i in list(root)[:10]:print(i)\n",
      "113/29: list(root)[0]\n",
      "113/30: list(root)[1]\n",
      "114/1: import pandas as pd\n",
      "114/2: pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "114/3: dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "114/4: dfs[0]\n",
      "114/5: dfs[0].iloc[0,4]\n",
      "114/6: dfs[0].iloc[0,3]\n",
      "114/7: int(dfs[0].iloc[0,3])\n",
      "114/8: dfs[0].iloc[0,3].strip()\n",
      "114/9: dfs[0].iloc[0,3].replace('+','').replace(',')\n",
      "114/10: dfs[0].iloc[0,3].replace('+','').replace(',','')\n",
      "114/11: int(dfs[0].iloc[0,3].replace('+','').replace(',',''))\n",
      "114/12: a = []\n",
      "114/13: a[-1]\n",
      "114/14: lista = [[0],[15000]]\n",
      "114/15: lista[0]\n",
      "114/16: lista = [[0,15000]]\n",
      "114/17: lista[0]\n",
      "114/18: lista[0][0]\n",
      "114/19: dfs[0].State\n",
      "114/20: dfs[0]\n",
      "114/21: DF = dfs[0]\n",
      "114/22: DF['total_diferencia'] = DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "114/23: old_DF = dfs[0]\n",
      "114/24: dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "114/25: DF = dfs[0]\n",
      "114/26: DF['total_diferencia'] = DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "114/27: DF\n",
      "114/28: old_DF['total_diferencia'] = old_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "114/29: DF['cambio_votos'] = old_DF.total_diferencia - DF.total_diferencia\n",
      "114/30: DF['news'] = DF['cambio_votos'].map(lambda x: '' if cambio_votos == 0 else 'CAMBIO')\n",
      "114/31: DF\n",
      "114/32: old_DF\n",
      "114/33: dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "114/34: dfs[0]\n",
      "114/35:\n",
      "import pandas as pd\n",
      "now = datetime.datetime.now()\n",
      "dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "114/36:\n",
      "import pandas as pd\n",
      "import datetime\n",
      "now = datetime.datetime.now()\n",
      "dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "114/37: wo\n",
      "114/38: now\n",
      "114/39: dfs\n",
      "114/40: dfs[0]\n",
      "114/41: dfs[0].columns.droplevel()\n",
      "114/42: df = dfs[0]\n",
      "114/43: df\n",
      "114/44:\n",
      "now = datetime.datetime.now()\n",
      "DF = dfs[0]\n",
      "DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "\n",
      "DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "#DF.columns = ['state', 'xa','votes','total','lider']\n",
      "114/45: DF\n",
      "114/46:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = dfs[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "while True:\n",
      "    time.sleep(30)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    new_DF['cambio_votos'] = old_DF.total_diferencia - new_DF.total_diferencia\n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF)\n",
      "        old_DF = new_DF.copy()\n",
      "114/47: import time\n",
      "114/48:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = dfs[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "while True:\n",
      "    time.sleep(30)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    new_DF['cambio_votos'] = old_DF.total_diferencia - new_DF.total_diferencia\n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF)\n",
      "        old_DF = new_DF.copy()\n",
      "114/49:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = dfs[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "while True:\n",
      "    time.sleep(30)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    new_DF['cambio_votos'] = old_DF.total_diferencia - new_DF.total_diferencia\n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF)\n",
      "        old_DF = new_DF.copy()\n",
      "    else:print(\".\")\n",
      "114/50:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = dfs[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "print(OLD_df.query('state != \"Alaska\"'))\n",
      "while True:\n",
      "    time.sleep(30)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    new_DF['cambio_votos'] = old_DF.total_diferencia - new_DF.total_diferencia\n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        old_DF = new_DF.copy()\n",
      "    else:print(\".\")\n",
      "114/51:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "print(OLD_df.query('state != \"Alaska\"'))\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    new_DF['cambio_votos'] = old_DF.total_diferencia - new_DF.total_diferencia\n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        old_DF = new_DF.copy()\n",
      "    else:print(\".\")\n",
      "114/52:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "print(OLD_df.query('state != \"Alaska\"'))\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    new_DF['cambio_votos'] = old_DF.total_diferencia - new_DF.total_diferencia\n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        old_DF = new_DF.copy()\n",
      "    else:print(\".\")\n",
      "114/53:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "print(old_DF.query('state != \"Alaska\"'))\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    new_DF['cambio_votos'] = old_DF.total_diferencia - new_DF.total_diferencia\n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        old_DF = new_DF.copy()\n",
      "    else:print(\".\")\n",
      "114/54: print('\\a')\n",
      "114/55: print('\\a')\n",
      "114/56: from subprocess import call\n",
      "114/57: call(['echo', u'\\x07'])\n",
      "114/58: call(['echo', u'\\x07'])\n",
      "114/59:  print('\\007')\n",
      "114/60:\n",
      "import subprocess\n",
      "subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "subprocess.call(['spd-say', '\"your process has finished\"'])\n",
      "114/61:\n",
      "import os\n",
      "duration = 1  # seconds\n",
      "freq = 440  # Hz\n",
      "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
      "114/62:\n",
      "import os\n",
      "duration = 1  # seconds\n",
      "freq = 440  # Hz\n",
      "os.system('play -nq -t alsa synth {} sine {}'.format(duration, freq))\n",
      "114/63:\n",
      "import subprocess\n",
      "subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "subprocess.call(['spd-say', '\"your process has finished\"'])\n",
      "114/64:  print('\\007')\n",
      "114/65: call(['echo', u'\\x07'])\n",
      "114/66: print('\\a')\n",
      "114/67:\n",
      "import subprocess\n",
      "subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "subprocess.call(['spd-say', '\"New Update\"'])\n",
      "117/1: import pandas\n",
      "117/2: import pandas as pd\n",
      "117/3:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "print(old_DF.query('state != \"Alaska\"'))\n",
      "117/4:\n",
      "import pandas as pd\n",
      "import datetime\n",
      "import time\n",
      "117/5:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "print(old_DF.query('state != \"Alaska\"'))\n",
      "117/6:\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    new_DF['cambio_votos'] = old_DF.total_diferencia - new_DF.total_diferencia\n",
      "117/7:\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    new_DF['cambio_votos'] = old_DF.total_diferencia - new_DF.total_diferencia\n",
      "    print(new_DF)\n",
      "117/8:\n",
      "import pandas as pd\n",
      "import datetime\n",
      "import time\n",
      "import subprocess\n",
      "117/9: dfs\n",
      "117/10: dfs_dict = {'old': old_DF}\n",
      "117/11: _old_ = old_DF\n",
      "117/12:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        for i,row in new_DF.query('cambio_votos > 0'):\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', '\"New Update in {}, difference is {}\" and the change is{}'.format(row.state, row.total_diferencia, row.cambio_votos)])\n",
      "            print(new_DF)\n",
      "            dfs_dict['old'] = new_DF.copy()\n",
      "117/13:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        for i,row in new_DF.query('cambio_votos > 0'):\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}, difference is {} and the change is{}'.format(row.state, row.total_diferencia, row.cambio_votos)])\n",
      "            print(new_DF)\n",
      "            dfs_dict['old'] = new_DF.copy()\n",
      "117/14:  'New Update in {}, difference is {} and the change is{}'.format(2,3, 5)\n",
      "117/15:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}, difference is {} and the change is {}'.format(row.state, row.total_diferencia, row.cambio_votos)])\n",
      "            print(new_DF)\n",
      "            dfs_dict['old'] = new_DF.copy()\n",
      "117/16: dfs_dict['old'] = _old_.copy()\n",
      "117/17:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "            print(new_DF)\n",
      "            dfs_dict['old'] = new_DF.copy()\n",
      "117/18: dfs_dict['old'] = _old_.copy()\n",
      "117/19:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "        print(new_DF)\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "117/20:\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "117/21:\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "117/22:\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "117/23:\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "print()\n",
      "117/24:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "117/25: dfs_dict['old'] = _old_.copy()\n",
      "117/26:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        print(new_DF.query('state != \"Alaska\"'))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "117/27: display(HTML(new_DF.to_html()))\n",
      "117/28:\n",
      "from IPython.display import display, HTML\n",
      "\n",
      "display(HTML(new_DF.to_html()))\n",
      "117/29: dfs_dict['old'] = _old_.copy()\n",
      "117/30:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "115/1: import pandas as pd\n",
      "115/2: pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "115/3: pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/05/us-election-2020-live-results-donald-trump-joe-biden-presidential-votes-arizona-nevada-pennsylvania-georgia')[0]\n",
      "117/31: _old_f\n",
      "117/32: _old_\n",
      "117/33: _old_.iloc[0,4]\n",
      "117/34: _old_\n",
      "117/35:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    if new_DF.iloc[0,4] == \"Biden\":\n",
      "        subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "        subprocess.call(['spd-say', 'Georgia is now Blue'])\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "115/4: pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "120/1:\n",
      "import pandas as pd\n",
      "import datetime\n",
      "import time\n",
      "import subprocess\n",
      "120/2:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "print(old_DF.query('state != \"Alaska\"'))\n",
      "120/3: old_DF\n",
      "120/4:\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF\n",
      "120/5:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "print(old_DF.query('state != \"Alaska\"'))\n",
      "120/6: dfs_dict = {'old': old_DF}\n",
      "120/7:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/8:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/9: from IPython.display import display, HTML\n",
      "120/10:\n",
      "display(HTML(old_DF.query('state != \"Alaska\"').to_html()))\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "\n",
      "    new_DF['cambio_votos'] = old.total_diferencia - new_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/11:\n",
      "import pandas as pd\n",
      "import datetime\n",
      "import time\n",
      "import subprocess\n",
      "120/12:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "print(old_DF.query('state != \"Alaska\"'))\n",
      "120/13: dfs_dict = {'old': old_DF}\n",
      "120/14:\n",
      "display(HTML(old_DF.query('state != \"Alaska\"').to_html()))\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "\n",
      "    new_DF['cambio_votos'] = new.total_diferencia - old_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos != 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/15:\n",
      "display(HTML(old_DF.query('state != \"Alaska\"').to_html()))\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos != 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/16:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "print(old_DF.query('state != \"Alaska\"'))\n",
      "120/17:\n",
      "display(HTML(old_DF.query('state != \"Alaska\"').to_html()))\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos != 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/18: dfs_dict = {'old': old_DF}\n",
      "120/19:\n",
      "display(HTML(old_DF.query('state != \"Alaska\"').to_html()))\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "    \n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos != 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/20:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = new.total_diferencia - old_DF.total_diferencia\n",
      "    if new_DF.iloc[0,4] == \"Biden\":\n",
      "        subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "        subprocess.call(['spd-say', 'Georgia is now Blue'])\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/21:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = new.total_diferencia - old_DF.total_diferencia\n",
      "    if new_DF.iloc[0,4] == \"Biden\":\n",
      "        subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "        subprocess.call(['spd-say', 'Georgia is now Blue'])\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/22:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = new_SD.total_diferencia - old_DF.total_diferencia\n",
      "    if new_DF.iloc[0,4] == \"Biden\":\n",
      "        subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "        subprocess.call(['spd-say', 'Georgia is now Blue'])\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/23:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "    if new_DF.iloc[0,4] == \"Biden\":\n",
      "        subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "        subprocess.call(['spd-say', 'Georgia is now Blue'])\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/24:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "print(old_DF.query('state != \"Alaska\"'))\n",
      "120/25:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "    if new_DF.iloc[0,4] == \"Biden\":\n",
      "        subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "        subprocess.call(['spd-say', 'Georgia is now Blue'])\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "        new_DF['cambio_votos'] = 0\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/26:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "    #if new_DF.iloc[0,4] == \"Biden\":\n",
      "    #    subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "    #    subprocess.call(['spd-say', 'Georgia is now Blue'])\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "        new_DF['cambio_votos'] = 0\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/27:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "    #if new_DF.iloc[0,4] == \"Biden\":\n",
      "    #    subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "    #    subprocess.call(['spd-say', 'Georgia is now Blue'])\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "120/28: dfs_dict\n",
      "120/29:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    if new_DF[['state', 'total'].equals(old_DF[['state', 'total']]):\n",
      "              pass\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "120/30:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    if new_DF[['state', 'total'].equals(old_DF[['state', 'total']]):\n",
      "            pass\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "120/31:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    if new_DF[['state', 'total'].equals(old_DF[['state', 'total']]):\n",
      "        pass\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "120/32:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    if new_DF[['state', 'total'].equals(old_DF[['state', 'total']]):\n",
      "              pass\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "120/33:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    if new_DF[['state', 'total'].equals(old_DF[['state', 'total']]):\n",
      "        pass\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "120/34:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    if new_DF[['state', 'total']].equals(old_DF[['state', 'total']]):\n",
      "        pass\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "120/35: new_DF[['state', 'total']].equals(old_DF[['state', 'total']])\n",
      "120/36: new_DF[['state', 'total']]\n",
      "120/37: old_DF[['state', 'total']]\n",
      "120/38:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    if new_DF[['state', 'total']].equals(old_DF[['state', 'total']]):\n",
      "        pass\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() != 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "120/39: dfs_dict['old']\n",
      "120/40: new_DF\n",
      "120/41: dfs_dict['old'].equals(new_DF)\n",
      "120/42:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    if not new_DF[['state', 'total']].equals(old_DF[['state', 'total']]):\n",
      "\n",
      "        new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "\n",
      "        if new_DF['cambio_votos'].sum() != 0:\n",
      "            display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "            for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "                subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "                subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "120/43:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    print(not new_DF[['state', 'total']].equals(old_DF[['state', 'total']]))\n",
      "    if not new_DF[['state', 'total']].equals(old_DF[['state', 'total']]):\n",
      "\n",
      "        new_DF['cambio_votos'] = new_DF.total_diferencia - old_DF.total_diferencia\n",
      "\n",
      "        if new_DF['cambio_votos'].sum() != 0:\n",
      "            display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "            for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "                subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "                subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "    print(dfs_dict['old'])\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "    print(dfs_dict['old'])\n",
      "120/44:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    print(not new_DF[['state', 'total']].equals(old_DF[['state', 'total']]))\n",
      "    if not new_DF[['state', 'total']].equals(old_DF[['state', 'total']]):\n",
      "\n",
      "        new_DF['cambio_votos'] = new_DF.total_diferencia - old.total_diferencia\n",
      "\n",
      "        if new_DF['cambio_votos'].sum() != 0:\n",
      "            display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "            for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "                subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "                subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "    dfs_dict['old'] = new_DF.copy()\n",
      "120/45: display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "120/46:\n",
      "now = datetime.datetime.now()\n",
      "old_DF = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')[0]\n",
      "old_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "old_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "old_DF['total_diferencia'] = old_DF.iloc[:,3].astype(int)#.str.replace('+','').str.replace(',','').astype(int)\n",
      "print(old_DF.query('state != \"Alaska\"'))\n",
      "120/47:\n",
      "while True:\n",
      "    time.sleep(15)\n",
      "    new_dfs = pd.read_html('https://www.theguardian.com/us-news/ng-interactive/2020/nov/03/us-election-2020-live-results-donald-trump-joe-biden-who-won-presidential-republican-democrat')\n",
      "    new_DF= new_dfs[0]\n",
      "    new_DF['x'] = str(now.hour) + \":\" + str(now.minute)\n",
      "    new_DF.columns = ['state', 'xa','votes','total','lider','x']\n",
      "    new_DF['total_diferencia'] = new_DF.iloc[:,3]#.str.replace('+','').str.replace(',','').astype(int)\n",
      "    old = dfs_dict[\"old\"].copy()\n",
      "    new_DF['cambio_votos'] = new_DF.total_diferencia - old.total_diferencia\n",
      "\n",
      "    if new_DF['cambio_votos'].sum() > 0:\n",
      "        display(HTML(new_DF.query('state != \"Alaska\"').to_html()))\n",
      "        for i,row in new_DF.query('cambio_votos > 0').iterrows():\n",
      "            subprocess.call(['speech-dispatcher'])        #start speech dispatcher\n",
      "            subprocess.call(['spd-say', 'New Update in {}'.format(row.state)])\n",
      "\n",
      "        dfs_dict['old'] = new_DF.copy()\n",
      "126/1: import configparser\n",
      "126/2:\n",
      "def redCOYUNTURA_1Habilitado():\n",
      "    usuario = \"test\n",
      "    if usuario:\n",
      "        config.read( usuario)\n",
      "        red = config['redCOYUNTURA_1']\n",
      "        if red['habilitado'] == 'T':\n",
      "            return(True)\n",
      "        else:\n",
      "            return(False)\n",
      "    else:\n",
      "        return(False)\n",
      "126/3:\n",
      "def redCOYUNTURA_1Habilitado():\n",
      "    usuario = \"test\"\n",
      "    \n",
      "    if usuario:\n",
      "        config.read( usuario)\n",
      "        red = config['redCOYUNTURA_1']\n",
      "        if red['habilitado'] == 'T':\n",
      "            return(True)\n",
      "        else:\n",
      "            return(False)\n",
      "    else:\n",
      "        return(False)\n",
      "126/4: redCOYUNTURA_1habilitad()\n",
      "126/5: redCOYUNTURA_1habilitado()\n",
      "126/6: redCOYUNTURA_1Habilitado()\n",
      "126/7: config = configparser.ConfigParser()\n",
      "126/8: redCOYUNTURA_1Habilitado()\n",
      "127/1: import pandas as pd\n",
      "127/2: permisos = pd.read_csv('permisos.csv')\n",
      "127/3: permisos.to_dict()\n",
      "127/4: permisos.to_dict(orient='index')\n",
      "127/5: permisos.to_dict(orient='index')[\"test\"]\n",
      "127/6: permisos.set_index(\"user\").to_dict(orient='index')[\"test\"]\n",
      "128/1: from pubmed_lookup import PubMedLookup\n",
      "128/2: email = ''\n",
      "128/3: url = 'https://pubmed.ncbi.nlm.nih.gov/29961577/'\n",
      "128/4: lookup = PubMedLookup(url, email)\n",
      "128/5: url = 'https://pubmed.ncbi.nlm.nih.gov/29961577'\n",
      "128/6: lookup = PubMedLookup(url, email)\n",
      "128/7: url = '29961577'\n",
      "128/8: lookup = PubMedLookup(url, email)\n",
      "128/9: publication\n",
      "128/10: pub = Publication(lookup)\n",
      "128/11: from pubmed_lookup import Publication\n",
      "128/12: pub = Publication(lookup)\n",
      "128/13: pub\n",
      "128/14: pub.title\n",
      "128/15: pub.abstract\n",
      "128/16: rep(pub.abstract)\n",
      "128/17: Proteins such as FUS phase separate to form liquid-like condensates that can harden into less dynamic structures. However, how these properties emerge from the collective interactions of many amino acids remains largely unknown. Here, we use extensive mutagenesis to identify a sequence-encoded molecular grammar underlying the driving forces of phase separation of proteins in the FUS family and test aspects of this grammar in cells. Phase separation is primarily governed by multivalent interactions among tyrosine residues from prion-like domains and arginine residues from RNA-binding domains, which are modulated by negatively charged residues. Glycine residues enhance the fluidity, whereas glutamine and serine residues promote hardening. We develop a model to show that the measured saturation concentrations of phase separation are inversely proportional to the product of the numbers of arginine and tyrosine residues. These results suggest it is possible to predict phase-separation properties based on amino acid sequences.\n",
      "128/18: pub.cite_mini()\n",
      "128/19: pub.cite()\n",
      "133/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "import localcider\n",
      "133/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "133/3: tablas['db']\n",
      "133/4: tablas['db'].query(\"organism == 'Homo sapiens'\")\n",
      "133/5: tablas['tidy']\n",
      "133/6: tablas['tidy'].columns\n",
      "133/7:\n",
      "tablas['db'].query(\"organism == 'Homo sapiens'\").merge(tablas['tidy']['gene_name', 'dc', 'domain',\n",
      "       'dis_zonas_list', 'dis_max', 'dis_median', 'dis_cantidad',\n",
      "       'lc_zonas_list', 'lc_max', 'lc_median', 'lc_cantidad', 'human'])\n",
      "133/8:\n",
      "tablas['db'].query(\"organism == 'Homo sapiens'\").merge(tablas['tidy'][['gene_name', 'dc', 'domain',\n",
      "       'dis_zonas_list', 'dis_max', 'dis_median', 'dis_cantidad',\n",
      "       'lc_zonas_list', 'lc_max', 'lc_median', 'lc_cantidad', 'human']])\n",
      "133/9:\n",
      "tablas['db'].query(\"organism == 'Homo sapiens'\").merge(tablas['tidy'][['uniprot','gene_name', 'dc', 'domain',\n",
      "       'dis_zonas_list', 'dis_max', 'dis_median', 'dis_cantidad',\n",
      "       'lc_zonas_list', 'lc_max', 'lc_median', 'lc_cantidad', 'human']])\n",
      "133/10:\n",
      "tablas['db'].query(\"organism == 'Homo sapiens'\").merge(tablas['tidy'][['uniprot','gene_name', 'dc', 'domain',\n",
      "       'dis_zonas_list', 'dis_max', 'dis_median', 'dis_cantidad',\n",
      "       'lc_zonas_list', 'lc_max', 'lc_median', 'lc_cantidad', 'human']]).drop_duplicates()\n",
      "133/11:\n",
      "tablas['db'].query(\"organism == 'Homo sapiens'\").merge(tablas['tidy'][['uniprot','gene_name', 'dc', 'domain',\n",
      "       'dis_zonas_list', 'dis_max', 'dis_median', 'dis_cantidad',\n",
      "       'lc_zonas_list', 'lc_max', 'lc_median', 'lc_cantidad', 'human']]).drop_duplicates(['uniprot'])\n",
      "133/12:\n",
      "tabla = tablas['db'].query(\"organism == 'Homo sapiens'\").merge(tablas['tidy'][['uniprot','gene_name', 'dc', 'domain',\n",
      "       'dis_zonas_list', 'dis_max', 'dis_median', 'dis_cantidad',\n",
      "       'lc_zonas_list', 'lc_max', 'lc_median', 'lc_cantidad', 'human']]).drop_duplicates(['uniprot'])\n",
      "133/13: tabla.merge(tablas['boxes_human'])\n",
      "133/14: tabla.merge(tablas['human_boxes'])\n",
      "133/15: tabla\n",
      "133/16: tablas['human_boxes'].merge(tabla)\n",
      "133/17: tablas['human_boxes']#.merge(tabla)\n",
      "133/18: tablas['human_boxes'].uniprot.value_counts()#.merge(tabla)\n",
      "133/19: tablas['human_boxes'].query('uniprot == \"Q15233\"')\n",
      "133/20: tablas.query('uniprot == \"Q15233\"')\n",
      "133/21: tabla.query('uniprot == \"Q15233\"')\n",
      "133/22: tabla.query('uniprot == \"Q15233\"').merge(tablas['human_boxes'])\n",
      "133/23: tabla.query('uniprot == \"Q15233\"').merge(tablas['human_boxes'], how='left')\n",
      "133/24: tabla.query('uniprot == \"Q15233\"')#.merge(tablas['human_boxes'], how='left')\n",
      "133/25: tabla.query('uniprot == \"Q15233\"').merge(tablas['human_boxes'], how='outer')\n",
      "133/26: tabla.query('uniprot == \"Q15233\"').merge(tablas['human_boxes'], how='rigth')\n",
      "133/27: tabla.query('uniprot == \"Q15233\"').merge(tablas['human_boxes'], how='right')\n",
      "133/28: tablas['human_boxes']\n",
      "133/29: tablas['human_boxes'].merge(tabla.query('uniprot == \"Q15233\"'))\n",
      "133/30: tablas['human_boxes'].query('uniprot == \"Q15233\"')\n",
      "133/31: tablas['human_boxes'].merge(tabla).query('uniprot == \"Q15233\"')\n",
      "133/32: tablas['human_boxes'].merge(tabla, how='outer').query('uniprot == \"Q15233\"')\n",
      "133/33: tabla\n",
      "133/34:\n",
      "tabla = tablas['tidy'][['uniprot','gene_name', 'dc', 'domain',\n",
      "       'dis_zonas_list', 'dis_max', 'dis_median', 'dis_cantidad',\n",
      "       'lc_zonas_list', 'lc_max', 'lc_median', 'lc_cantidad', 'human']]).drop_duplicates(['uniprot']\n",
      "133/35:\n",
      "tabla = tablas['tidy'][['uniprot','gene_name', 'dc', 'domain',\n",
      "       'dis_zonas_list', 'dis_max', 'dis_median', 'dis_cantidad',\n",
      "       'lc_zonas_list', 'lc_max', 'lc_median', 'lc_cantidad', 'human']].drop_duplicates(['uniprot'])\n",
      "133/36: tabla.query('uniprot == \"Q15233\"').merge(tablas['human_boxes'])\n",
      "133/37: tabla = tabla.merge(tablas['human_boxes'])\n",
      "133/38: tabla.query('dc >= 0.7')\n",
      "133/39: tabla.query('dc >= 0.7').query('box == \"box1\"')\n",
      "133/40: tablas['db']\n",
      "133/41: tablas['boxes']\n",
      "133/42: tablas['db']\n",
      "133/43: tablas['db'].query('org == \"Homo sapiens\"')\n",
      "133/44:\n",
      "print(len(tablas['db'].query('org == \"Homo sapiens\"')))\n",
      "for box, grupo in tablas['db'].query('org == \"Homo sapiens\"'):\n",
      "    print('entries', box, len(grupo))\n",
      "    print('proteins', box, len(grupo.drop_duplicates(\"uniprot\"))\n",
      "133/45:\n",
      "print(len(tablas['db'].query('org == \"Homo sapiens\"')))\n",
      "for box, grupo in tablas['db'].query('org == \"Homo sapiens\"'):\n",
      "    print('entries', box, len(grupo))\n",
      "    print('proteins', box, len(grupo.drop_duplicates(\"uniprot\")))\n",
      "133/46:\n",
      "print(len(tablas['db'].query('org == \"Homo sapiens\"')))\n",
      "for box, grupo in tablas['db'].query('org == \"Homo sapiens\"').groupby('org'):\n",
      "    print('entries', box, len(grupo))\n",
      "    print('proteins', box, len(grupo.drop_duplicates(\"uniprot\")))\n",
      "133/47:\n",
      "print(len(tablas['db'].query('org == \"Homo sapiens\"')))\n",
      "for box, grupo in tablas['db'].query('org == \"Homo sapiens\"').groupby('box'):\n",
      "    print('entries', box, len(grupo))\n",
      "    print('proteins', box, len(grupo.drop_duplicates(\"uniprot\")))\n",
      "133/48:\n",
      "print(len(tablas['db'].query('org == \"Homo sapiens\"')))\n",
      "for box, grupo in tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes']).groupby('box'):\n",
      "    print('entries', box, len(grupo))\n",
      "    print('proteins', box, len(grupo.drop_duplicates(\"uniprot\")))\n",
      "133/49: len(tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes']).drop_duplicates(\"uniprot\"))\n",
      "133/50:\n",
      "print(len(tablas['db'].query('org == \"Homo sapiens\"')))\n",
      "for box, grupo in tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes']).groupby('web'):\n",
      "    print('entries', box, len(grupo))\n",
      "    print('proteins', box, len(grupo.drop_duplicates(\"uniprot\")))\n",
      "133/51: tabla = tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes'])\n",
      "133/52: tabla.pivot_table(index='uniprot',column='box',aggfunc='size')\n",
      "133/53: tabla.pivot_table(index='uniprot',columns='box',aggfunc='size')\n",
      "133/54: tabla.drop_duplicates([\"uniprot\",'box']).pivot_table(index='uniprot',columns='box',aggfunc='size')\n",
      "133/55: tabla.drop_duplicates([\"uniprot\",'box']).pivot_table(index='web',columns='box',aggfunc='size')\n",
      "133/56: tabla.drop_duplicates([\"uniprot\",'box']).pivot_table(index='web',columns='box',aggfunc='size',margins=True)\n",
      "133/57: tabla.drop_duplicates([\"uniprot\",'box']).pivot_table(index='web',columns='box',aggfunc='size',margins=True)\n",
      "133/58: tabla.drop_duplicates([\"uniprot\",'box']).pivot_table(index='web',columns='box',aggfunc='size')\n",
      "133/59: tabla.drop_duplicates([\"uniprot\",'web','box']).pivot_table(index='web',columns='box',aggfunc='size')\n",
      "133/60: tabla.query('web == \"drllps\"').drop_duplicates(\"uniprot\")\n",
      "133/61: len(tabla.query('web == \"drllps\"').drop_duplicates(\"uniprot\"))\n",
      "133/62: len(tabla.query('web == \"drllps\"').query(\"box == 'box1'\").drop_duplicates(\"uniprot\"))\n",
      "133/63: tabla.query('web == \"drllps\"').query(\"box == 'box1'\").drop_duplicates(\"uniprot\")\n",
      "133/64: tabla.query('web == \"drllps\"').query(\"box == 'box1'\").drop_duplicates(\"uniprot\").box.value_counts()\n",
      "133/65: tabla.query('web == \"drllps\"').query(\"box == 'box1'\").drop_duplicates(\"uniprot\")#.box.value_counts()\n",
      "133/66: tabla.query('web == \"drllps\"').query(\"box == 'box1'\").drop_duplicates(\"uniprot\").db.value_counts()#.box.value_counts()\n",
      "133/67: tabla.query('web == \"drllps\"').query(\"box == 'box1'\")\n",
      "133/68: tabla.query('web == \"drllps\"').query(\"box == 'box1'\").query('db != \"drllps_scaffolds\"')\n",
      "133/69: tablas['human_boxes']\n",
      "133/70: tablas['human_boxes'].value_counts()\n",
      "133/71: tablas['human_boxes'].box.value_counts()\n",
      "134/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "import localcider\n",
      "134/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "134/3:\n",
      "import pandas as pd\n",
      "from collections import Counter\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "def calcular_frecuencias(df, agrup=True, agrup_col='mlo', seq_colname='seq'):\n",
      "    \"\"\"\n",
      "    Entrada: DataFrame con las secuencias, y columnas necesarias para agrupar si es necesario\n",
      "    Salida: Tabla con frecuencias de aminoacidos\n",
      "    \"\"\"\n",
      "    aas = ['A', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'K', 'L',\n",
      "           'M', 'N', 'P', 'Q', 'R', 'S', 'T', 'V', 'W', 'Y']\n",
      "\n",
      "    df = df.drop_duplicates().dropna()\n",
      "\n",
      "    # Si hay que agrupar por una columna (default es por MLO)\n",
      "    if agrup and agrup_col in list(df.columns):\n",
      "\n",
      "        df_all = df.groupby('mlo').agg(\"\".join)['seq'].reset_index()\n",
      "\n",
      "        counts = df_all.seq.map(Counter).apply(pd.Series)[aas]\n",
      "        counts['total'] = counts.apply(sum, axis='columns')\n",
      "        counts = counts.div(counts.total, axis=0)\n",
      "\n",
      "        suma_aa_df = pd.concat([df_all, counts], axis='columns')\n",
      "        suma_aa_df = suma_aa_df.drop(columns=['seq', 'total'])\n",
      "\n",
      "        human_frecuencias_melt = pd.melt(suma_aa_df, id_vars=['mlo'])\n",
      "        return human_frecuencias_melt\n",
      "\n",
      "    # SI no hay que agrupar por una columna -> solo frecuencias\n",
      "    else:\n",
      "        if agrup_col not in list(df.columns):\n",
      "            print(\"No existe la columna agrupar, calculando frecuencias generales\")\n",
      "        all_seq = \"\".join(df.seq.to_list())\n",
      "        counts = Counter(all_seq)\n",
      "        total_aas = sum([v for k, v in counts.items() if k in aas])\n",
      "        salida = {k: v / total_aas for k, v in counts.items() if k in aas}\n",
      "        return salida\n",
      "\n",
      "\n",
      "def dif_freq(f1, f2, n1=\"f1\", n2='f2', grupo='mlo'):\n",
      "    \"\"\"\n",
      "    Entrada: dos DataFrames de frecuencias de aminocadidos para cada grupo.\n",
      "    Salida: Un DataFrame con la diferencia de las frecuencias para cada aminoacido para cada grupo.\n",
      "    F1 - F2\n",
      "    \"\"\"\n",
      "    # si ambos son dataframes:\n",
      "    if isinstance(f1, pd.DataFrame):\n",
      "        f1.columns = [grupo, 'aminoacid', 'freq_' + n1]\n",
      "    else:\n",
      "        f1 = pd.DataFrame.from_dict(f1, orient='index').reset_index()\n",
      "        f1.columns = ['aminoacid', 'freq_' + n1]\n",
      "\n",
      "    if isinstance(f2, pd.DataFrame):\n",
      "        f2.columns = [grupo, 'aminoacid', 'freq_' + n2]\n",
      "    else:\n",
      "        f2 = pd.DataFrame.from_dict(f2, orient='index').reset_index()\n",
      "        f2.columns = ['aminoacid', 'freq_' + n2]\n",
      "\n",
      "    df = f1.merge(f2)\n",
      "    df['dif_freq'] = df['freq_' + n1] - df['freq_' + n2]\n",
      "    return df\n",
      "\n",
      "def align_yaxis(ax1, v1, ax2, v2):\n",
      "    \"\"\"adjust ax2 ylimit so that v2 in ax2 is aligned to v1 in ax1\"\"\"\n",
      "    _, y1 = ax1.transData.transform((0, v1))\n",
      "    _, y2 = ax2.transData.transform((0, v2))\n",
      "    inv = ax2.transData.inverted()\n",
      "    _, dy = inv.transform((0, 0)) - inv.transform((0, y1-y2))\n",
      "    miny, maxy = ax2.get_ylim()\n",
      "    ax2.set_ylim((miny+dy)*2, (maxy+dy)*2)\n",
      "\n",
      "def plot_dif_freq(target,base, mfdf,control, string_list=[\"A\", \"B\"], orden=[], asc=True):\n",
      "\n",
      "    diferencias = dif_freq(target,base)\n",
      "    diferencias['dif_freq_norm'] = diferencias.dif_freq / diferencias.freq_f2\n",
      "\n",
      "    colores_xaxis = {\n",
      "        'A': 'orange', 'I': 'orange', 'L': 'orange', 'P': 'orange', 'V': 'orange',\n",
      "        'N': 'red', 'Q': 'red',\n",
      "        'R': 'green', 'H': 'green', 'K': 'green',\n",
      "        'D': 'firebrick', 'E': 'firebrick',\n",
      "        'Y': 'blue', 'W': 'blue', 'F': 'blue',\n",
      "        'C': 'brown', 'M': 'brown',\n",
      "        'S': 'black', 'T': 'black'\n",
      "    }\n",
      "\n",
      "    to_plot = diferencias.pivot_table(index='aminoacid', columns='mlo', values='dif_freq_norm')\n",
      "    if not orden:\n",
      "        orden = list(to_plot.mean(axis=1).sort_values(ascending= not asc).index)\n",
      "\n",
      "    ax1 = to_plot.reindex(reversed(orden)).plot.bar(figsize=(10,4), cmap='tab10', rot=0, width=0.75 ,title=\"{} VS {}\".format(string_list[0], string_list[1]))\n",
      "\n",
      "    for xtic in ax1.get_xticklabels():\n",
      "        if xtic.get_text() in colores_xaxis.keys():  # Change color if exist else not\n",
      "            xtic.set_color(colores_xaxis[xtic.get_text()])\n",
      "    ax1.set_ylabel(\"(FrLC - FrSP) / FrSP\");\n",
      "\n",
      "\n",
      "\n",
      "    ax2 = ax1.twinx()\n",
      "    mfdf.reindex(reversed(orden)).plot.bar(alpha=(0.2), color='r', width=0.43, ax=ax2, legend=string_list[0])\n",
      "    control.reindex(reversed(orden)).plot.bar(alpha=(0.3), width=0.96, ax=ax2, label=string_list[1])\n",
      "\n",
      "    ax2.legend(string_list,loc='upper right',bbox_to_anchor=(0.95, 1.20))\n",
      "    align_yaxis(ax1, 0, ax2, 0)\n",
      "    #ax1.legend(loc='upper left',bbox_to_anchor=(0, 1.15), facecolor='white', framealpha=0)\n",
      "    ax1.legend(loc='lower center', bbox_to_anchor=(0.5,-0.50),\n",
      "          ncol=3, fancybox=True, shadow=True);\n",
      "    \n",
      "    #plt.savefig('graficos/{}'.format(\"_vs_\".join(string_list)))\n",
      "134/4:\n",
      "# cargar tablas de frecuencias control de uniprot\n",
      "todo = pd.read_csv('/home/fernando/datos/mlo/human_proteome.tab', sep='\\t')\n",
      "todo.columns = ['uniprot', 'org', 'seq']\n",
      "134/5: rep(tablas.keys())\n",
      "134/6: repp(tablas.keys())\n",
      "134/7: print(tablas.keys())\n",
      "134/8: print(*tablas.keys())\n",
      "134/9: print(*tablas.keys(),)\n",
      "134/10: *tablas.keys()\n",
      "134/11: *tablas.keys(),\n",
      "134/12:\n",
      "# Cargar los datos LC del control Swiss Prot\n",
      "from Bio import SeqIO\n",
      "import re\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "134/13: lc_control\n",
      "134/14:\n",
      "lc = tablas['human_lc_box1'].query('method == \"SEG_intermediate\"').copy()\n",
      "\n",
      "db = tablas['human_box1'][['uniprot','mlo_loc']].drop_duplicates()\n",
      "134/15: lc_db = lc.merge(tablas['human_box1'][['uniprot','mlo_loc']], how='left').drop_duplicates()\n",
      "134/16:\n",
      "lc_db = lc.merge(tablas[\"human_boxes\"])\n",
      "lc_db['mlo_loc'] = lc_db.box\n",
      "134/17:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq','mlo_loc']]\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "134/18: seqs = seqs.explode('mlo_loc').drop_duplicates()\n",
      "134/19: seqs\n",
      "134/20: lc_db = lc.merge(tablas['human_box1'][['uniprot','mlo_loc']], how='left').drop_duplicates()\n",
      "134/21:\n",
      "#lc_db = lc.merge(tablas[\"human_boxes\"])\n",
      "lc_db['mlo_loc'] = lc_db.box\n",
      "134/22:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq','mlo_loc']]\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "134/23:\n",
      "#lc_db = lc.merge(tablas[\"human_boxes\"])\n",
      "#lc_db['mlo_loc'] = lc_db.box\n",
      "134/24:\n",
      "seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq','mlo_loc']]\n",
      "seqs['seq'] = seqs.seq.map(\"\".join)\n",
      "134/25: seqs = seqs.explode('mlo_loc').drop_duplicates()\n",
      "134/26: seqs = seqs.groupby('mlo_loc').agg(lambda x: ''.join(x))\n",
      "134/27:\n",
      "sp = sp[~sp.seq.str.contains(\"U\")]\n",
      "seqs.loc['control','seq'] = \"\".join(sp.drop_duplicates().seq)\n",
      "134/28:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      "134/29:\n",
      "sp = sp[~sp.seq.str.contains(\"U\")]\n",
      "seqs.loc['control','seq'] = \"\".join(sp.drop_duplicates().seq)\n",
      "134/30:\n",
      "aas = \"\".join(sp[:10].seq.values)\n",
      "aas = list(set(aas))\n",
      "134/31: from localcider.sequenceParameters import SequenceParameters\n",
      "134/32: seqs\n",
      "134/33: from collections import Counter\n",
      "134/34:\n",
      "freqs = {}\n",
      "for mlo,seq in seqs.seq.items():\n",
      "    counts = Counter(seq)\n",
      "    freq = {k:v/len(seq) for k,v in counts.items()}\n",
      "    freqs[mlo] = freq\n",
      "134/35: freq_df = pd.DataFrame.from_dict(freqs)\n",
      "134/36:\n",
      "for col in freq_df.columns:\n",
      "    freq_df[col] = (freq_df[col] - freq_df.control) / freq_df.control\n",
      "134/37: freq_df\n",
      "134/38:\n",
      "    colores_xaxis = {\n",
      "        'A': 'orange', 'I': 'orange', 'L': 'orange', 'P': 'orange', 'V': 'orange',\n",
      "        'N': 'red', 'Q': 'red',\n",
      "        'R': 'green', 'H': 'green', 'K': 'green',\n",
      "        'D': 'firebrick', 'E': 'firebrick',\n",
      "        'Y': 'blue', 'W': 'blue', 'F': 'blue',\n",
      "        'C': 'brown', 'M': 'brown',\n",
      "        'S': 'black', 'T': 'black'\n",
      "    }\n",
      "\n",
      "    to_plot = diferencias.pivot_table(index='aminoacid', columns='mlo', values='dif_freq_norm')\n",
      "    if not orden:\n",
      "        orden = list(to_plot.mean(axis=1).sort_values(ascending= not asc).index)\n",
      "\n",
      "    ax1 = to_plot.reindex(reversed(orden)).plot.bar(figsize=(10,4), cmap='tab10', rot=0, width=0.75 ,title=\"{} VS {}\".format(string_list[0], string_list[1]))\n",
      "\n",
      "    for xtic in ax1.get_xticklabels():\n",
      "        if xtic.get_text() in colores_xaxis.keys():  # Change color if exist else not\n",
      "            xtic.set_color(colores_xaxis[xtic.get_text()])\n",
      "    ax1.set_ylabel(\"(FrLC - FrSP) / FrSP\");\n",
      "\n",
      "\n",
      "\n",
      "    ax2 = ax1.twinx()\n",
      "    mfdf.reindex(reversed(orden)).plot.bar(alpha=(0.2), color='r', width=0.43, ax=ax2, legend=string_list[0])\n",
      "    control.reindex(reversed(orden)).plot.bar(alpha=(0.3), width=0.96, ax=ax2, label=string_list[1])\n",
      "\n",
      "    ax2.legend(string_list,loc='upper right',bbox_to_anchor=(0.95, 1.20))\n",
      "    align_yaxis(ax1, 0, ax2, 0)\n",
      "    #ax1.legend(loc='upper left',bbox_to_anchor=(0, 1.15), facecolor='white', framealpha=0)\n",
      "    ax1.legend(loc='lower center', bbox_to_anchor=(0.5,-0.50),\n",
      "          ncol=3, fancybox=True, shadow=True);\n",
      "134/39:\n",
      "colores_xaxis = {\n",
      "    'A': 'orange', 'I': 'orange', 'L': 'orange', 'P': 'orange', 'V': 'orange',\n",
      "    'N': 'red', 'Q': 'red',\n",
      "    'R': 'green', 'H': 'green', 'K': 'green',\n",
      "    'D': 'firebrick', 'E': 'firebrick',\n",
      "    'Y': 'blue', 'W': 'blue', 'F': 'blue',\n",
      "    'C': 'brown', 'M': 'brown',\n",
      "    'S': 'black', 'T': 'black'\n",
      "}\n",
      "134/40: to_plot = freq_df.copy()\n",
      "134/41: to_plot.reindex(reversed(orden)).plot.bar(figsize=(10,4), cmap='tab10', rot=0, width=0.75 ,title=\"{} VS {}\".format(string_list[0], string_list[1]))\n",
      "134/42: to_plot.plot.bar(figsize=(10,4), cmap='tab10', rot=0, width=0.75 ,title=\"{} VS {}\".format(string_list[0], string_list[1]))\n",
      "134/43: to_plot.plot.bar(figsize=(10,4), cmap='tab10', rot=0, width=0.75 )#,title=\"{} VS {}\".format(string_list[0], string_list[1]))\n",
      "134/44: freq_df\n",
      "134/45: freq_df.mean(1)\n",
      "134/46: freq_df.mean(1).sort_values()\n",
      "134/47: freq_df.mean(1).sort_values(ascending=False)\n",
      "134/48: freq_df.mean(1).sort_values(ascending=False).index.tolist()\n",
      "134/49: orden = freq_df.mean(1).sort_values(ascending=False).index.tolist()\n",
      "134/50:\n",
      "mlo_seqs = lc_db.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list)[['seq']]\n",
      "mlo_seqs['seq'] = mlo_seqs.seq.map(\"\".join)\n",
      "134/51: mlo_seqs\n",
      "134/52: mlo_seqs.drop_duplicates()\n",
      "134/53: \"\".join(mlo_seqs.drop_duplicates().seq)\n",
      "134/54: mlo_freqs = Counter(\"\".join(mlo_seqs.drop_duplicates().seq))\n",
      "134/55:\n",
      "mlo_freqs = Counter(\"\".join(mlo_seqs.drop_duplicates().seq))\n",
      "mlo_freqs = {k:v/len(\"\".join(mlo_seqs.drop_duplicates().seq for k,v in mlo_freqs.items()}\n",
      "134/56:\n",
      "mlo_freqs = Counter(\"\".join(mlo_seqs.drop_duplicates().seq))\n",
      "mlo_freqs = {k:v/len(\"\".join(mlo_seqs.drop_duplicates().seq) for k,v in mlo_freqs.items()}\n",
      "134/57:\n",
      "mlo_freqs = Counter(\"\".join(mlo_seqs.drop_duplicates().seq))\n",
      "mlo_freqs = {k:v/len(\"\".join(mlo_seqs.drop_duplicates().seq)) for k,v in mlo_freqs.items() }\n",
      "134/58: mlo_freqs\n",
      "134/59: control_freqs = {k:v/len(\"\".join(sp.drop_duplicates().seq)) for k,v in Counter(\"\".join(sp.drop_duplicates().seq)).items()}\n",
      "134/60:\n",
      "\n",
      "ax1 = to_plot.reindex(reversed(orden)).plot.bar(figsize=(10,4), cmap='tab10', rot=0, width=0.75 ,title=\"{} VS {}\".format(string_list[0], string_list[1]))\n",
      "\n",
      "for xtic in ax1.get_xticklabels():\n",
      "    if xtic.get_text() in colores_xaxis.keys():  # Change color if exist else not\n",
      "        xtic.set_color(colores_xaxis[xtic.get_text()])\n",
      "ax1.set_ylabel(\"(FrLC - FrSP) / FrSP\");\n",
      "\n",
      "\n",
      "\n",
      "ax2 = ax1.twinx()\n",
      "mlo_freqs.reindex(reversed(orden)).plot.bar(alpha=(0.2), color='r', width=0.43, ax=ax2, legend=string_list[0])\n",
      "control_freqs.reindex(reversed(orden)).plot.bar(alpha=(0.3), width=0.96, ax=ax2, label=string_list[1])\n",
      "\n",
      "ax2.legend(string_list,loc='upper right',bbox_to_anchor=(0.95, 1.20))\n",
      "align_yaxis(ax1, 0, ax2, 0)\n",
      "#ax1.legend(loc='upper left',bbox_to_anchor=(0, 1.15), facecolor='white', framealpha=0)\n",
      "ax1.legend(loc='lower center', bbox_to_anchor=(0.5,-0.50),\n",
      "      ncol=3, fancybox=True, shadow=True);\n",
      "134/61: string_list = ['LC zones', 'Control']\n",
      "134/62:\n",
      "\n",
      "ax1 = to_plot.reindex(reversed(orden)).plot.bar(figsize=(10,4), cmap='tab10', rot=0, width=0.75 ,title=\"{} VS {}\".format(string_list[0], string_list[1]))\n",
      "\n",
      "for xtic in ax1.get_xticklabels():\n",
      "    if xtic.get_text() in colores_xaxis.keys():  # Change color if exist else not\n",
      "        xtic.set_color(colores_xaxis[xtic.get_text()])\n",
      "ax1.set_ylabel(\"(FrLC - FrSP) / FrSP\");\n",
      "\n",
      "\n",
      "\n",
      "ax2 = ax1.twinx()\n",
      "mlo_freqs.reindex(reversed(orden)).plot.bar(alpha=(0.2), color='r', width=0.43, ax=ax2, legend=string_list[0])\n",
      "control_freqs.reindex(reversed(orden)).plot.bar(alpha=(0.3), width=0.96, ax=ax2, label=string_list[1])\n",
      "\n",
      "ax2.legend(string_list,loc='upper right',bbox_to_anchor=(0.95, 1.20))\n",
      "align_yaxis(ax1, 0, ax2, 0)\n",
      "#ax1.legend(loc='upper left',bbox_to_anchor=(0, 1.15), facecolor='white', framealpha=0)\n",
      "ax1.legend(loc='lower center', bbox_to_anchor=(0.5,-0.50),\n",
      "      ncol=3, fancybox=True, shadow=True);\n",
      "135/1: import PyPDF2\n",
      "135/2: pdfFileObj = open('/media/fernando/Datos/Archivos%20Format/ubuntu_23-9/papers/1-s2.0-S0022283618309112-main.pdf', 'rb')\n",
      "135/3: pdfFileObj = open('/media/fernando/Datos/Archivos Format/ubuntu_23-9/papers/1-s2.0-S0022283618309112-main.pdf', 'rb')\n",
      "135/4: pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "135/5: pdfReader.extractText()\n",
      "135/6: pageObj.extractText()\n",
      "135/7: pageObj = pdfReader.getPage(0)\n",
      "135/8: pageObj.extractText()\n",
      "135/9: pageObj.extractText().find('stress')\n",
      "135/10: pageObj.extractText().findall('stress')\n",
      "136/1: improt PyPDF2\n",
      "136/2: import PyPDF2\n",
      "136/3: import os\n",
      "136/4: files = os.lisdir(\"/media/fernando/Datos/Archivos Format/ubuntu_23-9/papers/papers/mlo/\")\n",
      "136/5: files = os.listdir(\"/media/fernando/Datos/Archivos Format/ubuntu_23-9/papers/papers/mlo/\")\n",
      "136/6: files\n",
      "136/7: files = os.listdir(\"/media/fernando/Datos/Archivos Format/ubuntu_23-9/papers/todo/\")\n",
      "136/8: files\n",
      "136/9: base = '/media/fernando/Datos/Archivos Format/ubuntu_23-9/papers/todo/'\n",
      "136/10: \"ASd\".find('B')\n",
      "136/11: \"B\" in \"ASd\"\n",
      "136/12: \"d\" in \"ASd\"\n",
      "136/13: \"s\" in \"ASd\"\n",
      "136/14: \"s\" in \"ASd\".lower()\n",
      "136/15:\n",
      "def buscar(target,files)\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + arhivo:\n",
      "        pdfFileObj = open('ruta', 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in pdfReader.numPages:\n",
      "            page =  pdfReader.getPage(0)\n",
      "            page.extractText()\n",
      "            if target.lower() in page.lower():\n",
      "                salida = True\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(archivos, salida) if isin]\n",
      "136/16:\n",
      "def buscar(target,files):\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + arhivo:\n",
      "        pdfFileObj = open('ruta', 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in pdfReader.numPages:\n",
      "            page =  pdfReader.getPage(0)\n",
      "            page.extractText()\n",
      "            if target.lower() in page.lower():\n",
      "                salida = True\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(archivos, salida) if isin]\n",
      "136/17:\n",
      "def buscar(target,files):\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + arhivo\n",
      "        pdfFileObj = open('ruta', 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in pdfReader.numPages:\n",
      "            page =  pdfReader.getPage(0)\n",
      "            page.extractText()\n",
      "            if target.lower() in page.lower():\n",
      "                salida = True\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(archivos, salida) if isin]\n",
      "136/18: buscar('polymers', files)\n",
      "136/19:\n",
      "def buscar(target,files):\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + archivo\n",
      "        pdfFileObj = open('ruta', 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in pdfReader.numPages:\n",
      "            page =  pdfReader.getPage(0)\n",
      "            page.extractText()\n",
      "            if target.lower() in page.lower():\n",
      "                salida = True\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(archivos, salida) if isin]\n",
      "136/20: buscar('polymers', files)\n",
      "136/21:\n",
      "def buscar(target,files):\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + archivo\n",
      "        pdfFileObj = open(ruta, 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in pdfReader.numPages:\n",
      "            page =  pdfReader.getPage(0)\n",
      "            page.extractText()\n",
      "            if target.lower() in page.lower():\n",
      "                salida = True\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(archivos, salida) if isin]\n",
      "136/22: buscar('polymers', files)\n",
      "136/23:\n",
      "def buscar(target,files):\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + archivo\n",
      "        pdfFileObj = open(ruta, 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in range(pdfReader.numPages):\n",
      "            page =  pdfReader.getPage(0)\n",
      "            page.extractText()\n",
      "            if target.lower() in page.lower():\n",
      "                salida = True\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(archivos, salida) if isin]\n",
      "136/24: buscar('polymers', files)\n",
      "136/25:\n",
      "def buscar(target,files):\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + archivo\n",
      "        pdfFileObj = open(ruta, 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in range(pdfReader.numPages):\n",
      "            page =  pdfReader.getPage(0)\n",
      "            texto = page.extractText()\n",
      "            if target.lower() in texto.lower():\n",
      "                salida = True\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(archivos, salida) if isin]\n",
      "136/26: buscar('polymers', files)\n",
      "136/27:\n",
      "def buscar(target,files):\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + archivo\n",
      "        pdfFileObj = open(ruta, 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in range(pdfReader.numPages):\n",
      "            page =  pdfReader.getPage(0)\n",
      "            texto = page.extractText()\n",
      "            if target.lower() in texto.lower():\n",
      "                salida = True\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(files, salida) if isin]\n",
      "136/28: buscar('polymers', files)\n",
      "136/29:\n",
      "def buscar(target,files):\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + archivo\n",
      "        pdfFileObj = open(ruta, 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in range(pdfReader.numPages):\n",
      "            page =  pdfReader.getPage(0)\n",
      "            texto = page.extractText()\n",
      "            if target.lower() in texto.lower():\n",
      "                salida = True\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(files, output) if isin]\n",
      "136/30: buscar('polymers', files)\n",
      "136/31: buscar('NCL', files)\n",
      "136/32:\n",
      "def buscar(target,files):\n",
      "    output = []\n",
      "    for archivo in files:\n",
      "        salida = False\n",
      "        ruta = base + archivo\n",
      "        pdfFileObj = open(ruta, 'rb')\n",
      "        pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
      "        for page in range(pdfReader.numPages):\n",
      "            page =  pdfReader.getPage(0)\n",
      "            texto = page.extractText()\n",
      "            if target.lower() in texto.lower():\n",
      "                salida = True\n",
      "                break\n",
      "        output.append(salida)\n",
      "    return [paper for paper, isin in zip(files, output) if isin]\n",
      "136/33: buscar('NCL', files)\n",
      "136/34: buscar('SNRPB', files)\n",
      "136/35: buscar('RSMB', files)\n",
      "137/1: import pandas as pd\n",
      "137/2: db = pd.read_csv('assets/db.csv')\n",
      "137/3: df\n",
      "137/4: db\n",
      "137/5: db.set_index('paper_id')\n",
      "137/6: db = db.set_index('paper_id')\n",
      "137/7: salida = []\n",
      "137/8:\n",
      "for row in db.iterrows():\n",
      "    salida.append(row.to_dict())\n",
      "137/9: db\n",
      "137/10: db.to_dict()\n",
      "137/11: db.to_dict(orients='records')\n",
      "137/12: db.to_dict(orient='records')\n",
      "137/13: \"LA CASA DE ACA\".split()\n",
      "137/14:  db.abstract.map(lambda x: \"\".join(x.split()[:100]))\n",
      "137/15: db\n",
      "137/16: db = pd.read_csv('data/db.csv')\n",
      "137/17:  db.abstract.map(lambda x: \"\".join(x.split()[:100]))\n",
      "137/18:  db.abstract.map(lambda x: \" \".join(x.split()[:100]))\n",
      "138/1: import pandas as pd\n",
      "138/2: p = pd.read_csv('pregunta_P54.csv')\n",
      "139/1: import pandas as pd\n",
      "139/2: p = pd.read_csv('pregunta_P54.csv')\n",
      "139/3: p\n",
      "139/4: p = pd.read_csv('cruce_pregunta_P54.csv')\n",
      "139/5: p\n",
      "139/6: columnas = p.columns.tolist()\n",
      "139/7: columnas\n",
      "139/8: columnas.remove(\"No sabe\")\n",
      "139/9: columnas\n",
      "139/10: columnas = p.columns.tolist()\n",
      "139/11: col_rev = list(reversed(columnas))\n",
      "139/12: col_rev\n",
      "139/13: col_rev.append(col_rev.pop('No sabe'))\n",
      "139/14: col_rev.remove('No sabe').append('No sabe')\n",
      "139/15: col_rev.remove('No sabe')\n",
      "139/16: col_rev = list(reversed(columnas))\n",
      "139/17: col_rev.remove('No sabe')\n",
      "139/18: col_rev.append('No sabe')\n",
      "139/19: col_rev\n",
      "139/20: df\n",
      "139/21: df = p\n",
      "139/22: df\n",
      "139/23:\n",
      "if \"No sabe\" not in df.columns.tolist():\n",
      "    df = df.iloc[:, ::-1] # reordenar inversamente las columnas\n",
      "140/1: \"AS/ASSA/SADA/'\n",
      "140/2: \"AS/ASSA/SADA/\"\n",
      "140/3: \"AS/ASSA/SADA/\".replace('/','')\n",
      "140/4: \"AS/ASSA/SADA/\".strip()\n",
      "140/5: \"AS/ASSA/SADA/\".split(\"/\")\n",
      "140/6: [i in \"AS/ASSA/SADA/\".split(\"/\") if len(i)]\n",
      "140/7: [i for i in \"AS/ASSA/SADA/\".split(\"/\") if len(i)]\n",
      "140/8: [i for i in \"AS/ASSA/SADA/\".split(\"/\") if len(i)][-1]\n",
      "140/9: from werkzeug.security import check_password_hash\n",
      "141/1:\n",
      "from plotly.offline import init_notebook_mode, iplot\n",
      "from plotly.graph_objs import *\n",
      "141/2:\n",
      "init_notebook_mode(connected=True)         # initiate notebook for offline plot\n",
      "\n",
      "trace0 = Scatter(\n",
      "     x=[1, 2, 3, 4],\n",
      "     y=[10, 15, 13, 17]\n",
      ")\n",
      "trace1 = Scatter(\n",
      "     x=[1, 2, 3, 4],\n",
      "     y=[16, 5, 11, 9]\n",
      ")\n",
      "data = Data([trace0, trace1])\n",
      "\n",
      "iplot(data)               # use plotly.offline.iplot for offline plot\n",
      "\n",
      "enter image description here\n",
      "141/3:\n",
      "init_notebook_mode(connected=True)         # initiate notebook for offline plot\n",
      "\n",
      "trace0 = Scatter(\n",
      "     x=[1, 2, 3, 4],\n",
      "     y=[10, 15, 13, 17]\n",
      ")\n",
      "trace1 = Scatter(\n",
      "     x=[1, 2, 3, 4],\n",
      "     y=[16, 5, 11, 9]\n",
      ")\n",
      "data = Data([trace0, trace1])\n",
      "\n",
      "iplot(data)               # use plotly.offline.iplot for offline plot\n",
      "141/4:\n",
      "import chart_studio.plotly as py\n",
      "import plotly.figure_factory as ff\n",
      "import pandas as pd\n",
      "\n",
      "df = pd.read_csv(\"https://raw.githubusercontent.com/plotly/datasets/master/school_earnings.csv\")\n",
      "\n",
      "table = ff.create_table(df)\n",
      "py.iplot(table, filename='jupyter-table1')\n",
      "141/5:\n",
      "import plotly.express as px\n",
      "data_canada = px.data.gapminder().query(\"country == 'Canada'\")\n",
      "fig = px.bar(data_canada, x='year', y='pop')\n",
      "fig.show()\n",
      "145/1:\n",
      "import plotly.express as px\n",
      "data_canada = px.data.gapminder().query(\"country == 'Canada'\")\n",
      "fig = px.bar(data_canada, x='year', y='pop')\n",
      "fig.show()\n",
      "145/2: fig['data'][0]['y'][0] = 10785584\n",
      "145/3: fig\n",
      "145/4: fig['data'][0]['xaxis'] = [1952, 1957, 1962, -1967, -1972, -1977, -1982, 1987, 1992, 1997, 2002, 2007]\n",
      "145/5: array= fig['data'][0]['xaxis']\n",
      "145/6: array\n",
      "145/7: fig\n",
      "145/8: df = pd.read_csv('data.csv')\n",
      "145/9: import pandas as pd\n",
      "145/10: df = pd.read_csv('data.csv')\n",
      "145/11: df\n",
      "148/1:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "data_canada = px.data.gapminder().query(\"country == 'Canada'\")\n",
      "fig = px.bar(data_canada, x='year', y='pop')\n",
      "fig.show()\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "df = px.data.tips()\n",
      "fig = px.bar(df, x=\"total_bill\", y=\"base\", color='day', orientation='h',\n",
      "             hover_data=[\"tip\", \"size\"],\n",
      "             height=400,\n",
      "             title='Restaurant bills')\n",
      "fig.show()\n",
      "148/2: df\n",
      "148/3: df = pd.read_csv('data.csv')\n",
      "148/4: df\n",
      "148/5: df.melt()\n",
      "148/6: df[['base'],'cantidad1','cantidad2'].melt()\n",
      "148/7: df[['base','cantidad1','cantidad2']].melt()\n",
      "148/8: df[['base','cantidad1','cantidad2']].melt(id_vars='base')\n",
      "148/9: df_melt = df[['base','cantidad1','cantidad2']].melt(id_vars='base')\n",
      "150/1:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data_canada = px.data.gapminder().query(\"country == 'Canada'\")\n",
      "fig = px.bar(data_canada, x='year', y='pop')\n",
      "fig.show()\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "df['cantidad1'] = df.cantidad1 * -1\n",
      "\n",
      "df_melt = df[['base','cantidad1','cantidad2']].melt(id_vars='base')\n",
      "\n",
      "fig = px.bar(df_melt, x=\"value\", y=\"base\", color='variable', orientation='h')\n",
      "\n",
      "vals = np.concatenate((fig['data'][0]['x'], fig['data'][1]['x']))\n",
      "\n",
      "layout=dict(                xaxis=dict(\n",
      "                tickvals=vals,\n",
      "                ticktext=[abs(i)for i in vals],\n",
      "                tickmode='array'\n",
      "                )        )\n",
      "\n",
      "fig.update_layout(layout)\n",
      "\n",
      "fig.update_xaxes(\n",
      "    ticktext=[\"End of Q1\", \"End of Q2\", \"End of Q3\"],\n",
      "    tickvals=[-2000, 0, 2000],\n",
      ").show()\n",
      "\n",
      "fig.update_layout(layout).show()\n",
      "\n",
      "fig['data'][0]['x'] = fig['data'][0]['x'] * -1\n",
      "\n",
      "a = df.cantidad1.min()\n",
      "b = df.cantidad2.max()\n",
      "\n",
      "(b - a) / 8\n",
      "\n",
      "df.plot.bar(stacked=True)\n",
      "152/1:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data_canada = px.data.gapminder().query(\"country == 'Canada'\")\n",
      "fig = px.bar(data_canada, x='year', y='pop')\n",
      "fig.show()\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "df['cantidad1'] = df.cantidad1 * -1\n",
      "\n",
      "df_melt = df[['base','cantidad1','cantidad2']].melt(id_vars='base')\n",
      "\n",
      "fig = px.bar(df_melt, x=\"value\", y=\"base\", color='variable', orientation='h')\n",
      "\n",
      "vals = np.concatenate((fig['data'][0]['x'], fig['data'][1]['x']))\n",
      "\n",
      "layout=dict(                xaxis=dict(\n",
      "                tickvals=vals,\n",
      "                ticktext=[abs(i)for i in vals],\n",
      "                tickmode='array'\n",
      "                )        )\n",
      "\n",
      "fig.update_layout(layout)\n",
      "\n",
      "fig.update_xaxes(\n",
      "    ticktext=labels = [str(int(abs(i))) for i in df.plot.bar(stacked=True).get_yticks()],\n",
      "    tickvals=df.plot.bar(stacked=True).get_yticks(),\n",
      ").show()\n",
      "152/2: df2\n",
      "152/3: import pandas as pd\n",
      "152/4: df2\n",
      "152/5: df2 = pd.read_csv('data2.csv')\n",
      "152/6: df2\n",
      "154/1:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data_canada = px.data.gapminder().query(\"country == 'Canada'\")\n",
      "fig = px.bar(data_canada, x='year', y='pop')\n",
      "fig.show()\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "df['cantidad1'] = df.cantidad1 * -1\n",
      "\n",
      "df_melt = df[['base','cantidad1','cantidad2']].melt(id_vars='base')\n",
      "\n",
      "fig = px.bar(df_melt, x=\"value\", y=\"base\", color='variable', orientation='h', text='x',textposition='auto')\n",
      "\n",
      "vals = np.concatenate((fig['data'][0]['x'], fig['data'][1]['x']))\n",
      "\n",
      "layout=dict(                xaxis=dict(\n",
      "                tickvals=vals,\n",
      "                ticktext=[abs(i)for i in vals],\n",
      "                )        )\n",
      "\n",
      "fig.update_layout(layout).show()\n",
      "\n",
      "import plotly.express as px\n",
      "import seaborn as sns\n",
      "import matplotlib\n",
      "[matplotlib.colors.to_hex(i) for i in colors]\n",
      "\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"Topico_\"  + df2.topic.astype(str)\n",
      "fig = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",\n",
      "             facet_row=\"stance\", color_discrete_sequence=custom_colors,title=['A','B','C'])\n",
      "\n",
      "(fig\n",
      ".update_layout(dict(plot_bgcolor='rgba(0,0,0,0)'))\n",
      ".update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
      ".update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
      ").show()\n",
      "154/2:\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"Topico_\"  + df2.topic.astype(str)\n",
      "fig = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",\n",
      "             facet_row=\"stance\", color_discrete_sequence=custom_colors)\n",
      "\n",
      "(fig\n",
      ".update_layout(dict(plot_bgcolor='rgba(0,0,0,0)'))\n",
      ".update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
      ".update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
      ").show()\n",
      "156/1:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "data_canada = px.data.gapminder().query(\"country == 'Canada'\")\n",
      "fig = px.bar(data_canada, x='year', y='pop')\n",
      "fig.show()\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "df['cantidad1'] = df.cantidad1 * -1\n",
      "\n",
      "df_melt = df[['base','cantidad1','cantidad2']].melt(id_vars='base')\n",
      "\n",
      "fig = px.bar(df_melt, x=\"value\", y=\"base\", color='variable', orientation='h', text='x',textposition='auto')\n",
      "\n",
      "vals = np.concatenate((fig['data'][0]['x'], fig['data'][1]['x']))\n",
      "\n",
      "layout=dict(                xaxis=dict(\n",
      "                tickvals=vals,\n",
      "                ticktext=[abs(i)for i in vals],\n",
      "                )        )\n",
      "\n",
      "fig.update_layout(layout).show()\n",
      "\n",
      "import plotly.express as px\n",
      "import seaborn as sns\n",
      "import matplotlib\n",
      "\n",
      "\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"Topico_\"  + df2.topic.astype(str)\n",
      "fig = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",\n",
      "             facet_row=\"stance\", color_discrete_sequence=custom_colors)\n",
      "\n",
      "(fig\n",
      ".update_layout(dict(plot_bgcolor='rgba(0,0,0,0)'))\n",
      ".update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
      ".update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
      ").show()\n",
      "156/2:\n",
      "\n",
      "import plotly.express as px\n",
      "import seaborn as sns\n",
      "import matplotlib\n",
      "\n",
      "\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"Topico_\"  + df2.topic.astype(str)\n",
      "fig = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",\n",
      "             facet_row=\"stance\", color_discrete_sequence=custom_colors)\n",
      "\n",
      "(fig\n",
      ".update_layout(dict(plot_bgcolor='rgba(0,0,0,0)'))\n",
      ".update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
      ".update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey')\n",
      ").show()\n",
      "158/1:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "\n",
      "df.columns = ['nombre', '1er Nivel', '2do Nivel', 'ct']\n",
      "df['1er Nivel'] = df['1er Nivel'] * -1\n",
      "df_melt = df[['nombre','1er Nivel','2do Nivel']].melt(id_vars='nombre')\n",
      "df_melt['value_pos'] = df_melt['value'].map(abs).astype(int)\n",
      "\n",
      "df_melt = df_melt.sort_values('nombre')\n",
      "df_melt['colors'] = paired_colors[:len(df_melt)]\n",
      "\n",
      "fig = px.bar(df_melt, x=\"value\", y=\"nombre\", color='colors', orientation='h', hover_data=['value_pos'],\n",
      "color_discrete_sequence=paired_colors\n",
      ")\n",
      "fig.update_traces(hovertemplate='%{y} <br> %{customdata[0]} tweets')\n",
      "fig.show()\n",
      "\n",
      "colores = [\"#980c13\", \"#d92523\", \"#fd8c00\", \"#ffa600\", \"#084a91\", \"#2e7ebc\"]\n",
      "\n",
      "partido = {'A.Fernandez':'Kirchnerismo', 'CFK':'Kirchnerismo', 'Macri':'Cambiemos', 'SM':'Peronismo'}\n",
      "colores_partidos = {'Kirchnerismo':[\"#980c13\", \"#d92523\"], 'Peronismo': [\"#084a91\", \"#2e7ebc\"], 'Cambiemos':[\"#fd8c00\", \"#ffa600\"]}\n",
      "nivel = {'1er Nivel':0, '2do Nivel':1}\n",
      "\n",
      "\n",
      "data = []\n",
      "for i,row in df_melt.iterrows():\n",
      "    print(colores_partidos[partido[row.nombre]][nivel[row.variable]])\n",
      "    data.append(\n",
      "        go.Bar(y=[row.nombre],\n",
      "               x=[row.value],\n",
      "               orientation='h',\n",
      "               hover_data=f'{row.nombre}<br>{row.value_pos} tweets<br>{row.variable}'\n",
      "               marker={'color':colores_partidos[partido[row.nombre]][nivel[row.variable]]})\n",
      "    )\n",
      "\n",
      "go.Figure({'data':data}).update_layout(barmode='relative').update_xaxes( zerolinecolor='black').show()\n",
      "\n",
      "\n",
      "\n",
      "vals = np.concatenate((fig['data'][0]['x'], fig['data'][1]['x']))\n",
      "\n",
      "layout=dict(                xaxis=dict(\n",
      "                tickvals=vals,\n",
      "                ticktext=[abs(i)for i in vals],\n",
      "                )        )\n",
      "\n",
      "fig.update_layout(layout).update_xaxes(zerolinecolor='black',).show()\n",
      "\n",
      "import plotly.express as px\n",
      "import seaborn as sns\n",
      "import matplotlib\n",
      "\n",
      "\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"Topico_\"  + df2.topic.astype(str)\n",
      "fig = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",\n",
      "             facet_row=\"stance\", color_discrete_sequence=custom_colors)\n",
      "\n",
      "\n",
      "(fig\n",
      ".update_layout(dict(plot_bgcolor='rgba(0,0,0,0)'))\n",
      ".update_yaxes(matches=None)\n",
      ".for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
      "\n",
      ".update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black',)\n",
      ".update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black',)\n",
      ".update_layout(showlegend=False)\n",
      ")\n",
      "for y,a in zip([1, 0.33, 0.66], fig['layout']['annotations']):\n",
      "    a['text'] = a['text'].capitalize()\n",
      "    a['y'] = y\n",
      "    a['x'] = 0.49\n",
      "    a['xanchor'] = 'center'\n",
      "    a['textangle'] = 0\n",
      "    a['font'] = {'size':20}\n",
      "\n",
      "fig.show()\n",
      "\n",
      "\n",
      "aa  = []\n",
      "fig = px.scatter(px.data.tips(), x=\"total_bill\", y=\"tip\", facet_col=\"smoker\")\n",
      "fig.for_each_annotation(lambda a: aa.append(a.update(text=a.text.split(\"=\")[-1])))\n",
      "fig.show()\n",
      "158/2:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "\n",
      "df.columns = ['nombre', '1er Nivel', '2do Nivel', 'ct']\n",
      "df['1er Nivel'] = df['1er Nivel'] * -1\n",
      "df_melt = df[['nombre','1er Nivel','2do Nivel']].melt(id_vars='nombre')\n",
      "df_melt['value_pos'] = df_melt['value'].map(abs).astype(int)\n",
      "\n",
      "df_melt = df_melt.sort_values('nombre')\n",
      "df_melt['colors'] = paired_colors[:len(df_melt)]\n",
      "\n",
      "fig = px.bar(df_melt, x=\"value\", y=\"nombre\", color='colors', orientation='h', hover_data=['value_pos'],\n",
      "color_discrete_sequence=paired_colors\n",
      ")\n",
      "fig.update_traces(hovertemplate='%{y} <br> %{customdata[0]} tweets')\n",
      "fig.show()\n",
      "\n",
      "colores = [\"#980c13\", \"#d92523\", \"#fd8c00\", \"#ffa600\", \"#084a91\", \"#2e7ebc\"]\n",
      "\n",
      "partido = {'A.Fernandez':'Kirchnerismo', 'CFK':'Kirchnerismo', 'Macri':'Cambiemos', 'SM':'Peronismo'}\n",
      "colores_partidos = {'Kirchnerismo':[\"#980c13\", \"#d92523\"], 'Peronismo': [\"#084a91\", \"#2e7ebc\"], 'Cambiemos':[\"#fd8c00\", \"#ffa600\"]}\n",
      "nivel = {'1er Nivel':0, '2do Nivel':1}\n",
      "\n",
      "\n",
      "data = []\n",
      "for i,row in df_melt.iterrows():\n",
      "    print(colores_partidos[partido[row.nombre]][nivel[row.variable]])\n",
      "    data.append(\n",
      "        go.Bar(y=[row.nombre],\n",
      "               x=[row.value],\n",
      "               orientation='h',\n",
      "               hover_data=f'{row.nombre}<br>{row.value_pos} tweets<br>{row.variable}',\n",
      "               marker={'color':colores_partidos[partido[row.nombre]][nivel[row.variable]]})\n",
      "    )\n",
      "\n",
      "go.Figure({'data':data}).update_layout(barmode='relative').update_xaxes( zerolinecolor='black').show()\n",
      "158/3:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "\n",
      "df.columns = ['nombre', '1er Nivel', '2do Nivel', 'ct']\n",
      "df['1er Nivel'] = df['1er Nivel'] * -1\n",
      "df_melt = df[['nombre','1er Nivel','2do Nivel']].melt(id_vars='nombre')\n",
      "df_melt['value_pos'] = df_melt['value'].map(abs).astype(int)\n",
      "\n",
      "df_melt = df_melt.sort_values('nombre')\n",
      "df_melt['colors'] = paired_colors[:len(df_melt)]\n",
      "\n",
      "fig = px.bar(df_melt, x=\"value\", y=\"nombre\", color='colors', orientation='h', hover_data=['value_pos'],\n",
      "color_discrete_sequence=paired_colors\n",
      ")\n",
      "fig.update_traces(hovertemplate='%{y} <br> %{customdata[0]} tweets')\n",
      "fig.show()\n",
      "\n",
      "colores = [\"#980c13\", \"#d92523\", \"#fd8c00\", \"#ffa600\", \"#084a91\", \"#2e7ebc\"]\n",
      "\n",
      "partido = {'A.Fernandez':'Kirchnerismo', 'CFK':'Kirchnerismo', 'Macri':'Cambiemos', 'SM':'Peronismo'}\n",
      "colores_partidos = {'Kirchnerismo':[\"#980c13\", \"#d92523\"], 'Peronismo': [\"#084a91\", \"#2e7ebc\"], 'Cambiemos':[\"#fd8c00\", \"#ffa600\"]}\n",
      "nivel = {'1er Nivel':0, '2do Nivel':1}\n",
      "\n",
      "\n",
      "data = []\n",
      "for i,row in df_melt.iterrows():\n",
      "    print(colores_partidos[partido[row.nombre]][nivel[row.variable]])\n",
      "    data.append(\n",
      "        go.Bar(y=[row.nombre],\n",
      "               x=[row.value],\n",
      "               orientation='h',\n",
      "               hover_data=f'{row.nombre}<br>{row.value_pos} tweets<br>{row.variable}',\n",
      "               marker={'color':colores_partidos[partido[row.nombre]][nivel[row.variable]]})\n",
      "    )\n",
      "\n",
      "go.Figure({'data':data}).update_layout(barmode='relative').update_xaxes( zerolinecolor='black').show()\n",
      "158/4:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "\n",
      "df.columns = ['nombre', '1er Nivel', '2do Nivel', 'ct']\n",
      "df['1er Nivel'] = df['1er Nivel'] * -1\n",
      "df_melt = df[['nombre','1er Nivel','2do Nivel']].melt(id_vars='nombre')\n",
      "df_melt['value_pos'] = df_melt['value'].map(abs).astype(int)\n",
      "\n",
      "df_melt = df_melt.sort_values('nombre')\n",
      "df_melt['colors'] = paired_colors[:len(df_melt)]\n",
      "\n",
      "fig = px.bar(df_melt, x=\"value\", y=\"nombre\", color='colors', orientation='h', hover_data=['value_pos'],\n",
      "color_discrete_sequence=paired_colors\n",
      ")\n",
      "fig.update_traces(hovertemplate='%{y} <br> %{customdata[0]} tweets')\n",
      "fig.show()\n",
      "\n",
      "colores = [\"#980c13\", \"#d92523\", \"#fd8c00\", \"#ffa600\", \"#084a91\", \"#2e7ebc\"]\n",
      "\n",
      "partido = {'A.Fernandez':'Kirchnerismo', 'CFK':'Kirchnerismo', 'Macri':'Cambiemos', 'SM':'Peronismo'}\n",
      "colores_partidos = {'Kirchnerismo':[\"#980c13\", \"#d92523\"], 'Peronismo': [\"#084a91\", \"#2e7ebc\"], 'Cambiemos':[\"#fd8c00\", \"#ffa600\"]}\n",
      "nivel = {'1er Nivel':0, '2do Nivel':1}\n",
      "\n",
      "\n",
      "data = []\n",
      "for i,row in df_melt.iterrows():\n",
      "    print(colores_partidos[partido[row.nombre]][nivel[row.variable]])\n",
      "    data.append(\n",
      "        go.Bar(y=[row.nombre],\n",
      "               x=[row.value],\n",
      "               orientation='h',\n",
      "               hover_data=f'{row.nombre}<br>{row.value_pos} tweets<br>{row.variable}',\n",
      "               marker={'color':colores_partidos[partido[row.nombre]][nivel[row.variable]]})\n",
      "    )\n",
      "\n",
      "go.Figure({'data':data}).update_layout(barmode='relative').update_xaxes( zerolinecolor='black').show()\n",
      "160/1:\n",
      "import plotly.express as px\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "import plotly.graph_objects as go\n",
      "import seaborn as sns\n",
      "\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "\n",
      "df.columns = ['nombre', '1er Nivel', '2do Nivel', 'ct']\n",
      "df['1er Nivel'] = df['1er Nivel'] * -1\n",
      "df_melt = df[['nombre','1er Nivel','2do Nivel']].melt(id_vars='nombre')\n",
      "df_melt['value_pos'] = df_melt['value'].map(abs).astype(int)\n",
      "\n",
      "df_melt = df_melt.sort_values('nombre')\n",
      "df_melt['colors'] = paired_colors[:len(df_melt)]\n",
      "\n",
      "fig = px.bar(df_melt, x=\"value\", y=\"nombre\", color='colors', orientation='h', hover_data=['value_pos'],\n",
      "color_discrete_sequence=paired_colors\n",
      ")\n",
      "fig.update_traces(hovertemplate='%{y} <br> %{customdata[0]} tweets')\n",
      "fig.show()\n",
      "\n",
      "colores = [\"#980c13\", \"#d92523\", \"#fd8c00\", \"#ffa600\", \"#084a91\", \"#2e7ebc\"]\n",
      "\n",
      "partido = {'A.Fernandez':'Kirchnerismo', 'CFK':'Kirchnerismo', 'Macri':'Cambiemos', 'SM':'Peronismo'}\n",
      "colores_partidos = {'Kirchnerismo':[\"#980c13\", \"#d92523\"], 'Peronismo': [\"#084a91\", \"#2e7ebc\"], 'Cambiemos':[\"#fd8c00\", \"#ffa600\"]}\n",
      "nivel = {'1er Nivel':0, '2do Nivel':1}\n",
      "\n",
      "\n",
      "data = []\n",
      "for i,row in df_melt.iterrows():\n",
      "    print(colores_partidos[partido[row.nombre]][nivel[row.variable]])\n",
      "    data.append(\n",
      "        go.Bar(y=[row.nombre],\n",
      "               x=[row.value],\n",
      "               orientation='h',\n",
      "               hover_data=f'{row.nombre}<br>{row.value_pos} tweets<br>{row.variable}',\n",
      "               marker={'color':colores_partidos[partido[row.nombre]][nivel[row.variable]]})\n",
      "    )\n",
      "\n",
      "go.Figure({'data':data}).update_layout(barmode='relative').update_xaxes( zerolinecolor='black').show()\n",
      "\n",
      "\n",
      "\n",
      "vals = np.concatenate((fig['data'][0]['x'], fig['data'][1]['x']))\n",
      "\n",
      "layout=dict(                xaxis=dict(\n",
      "                tickvals=vals,\n",
      "                ticktext=[abs(i)for i in vals],\n",
      "                )        )\n",
      "\n",
      "fig.update_layout(layout).update_xaxes(zerolinecolor='black',).show()\n",
      "\n",
      "import plotly.express as px\n",
      "import seaborn as sns\n",
      "import matplotlib\n",
      "\n",
      "\n",
      "\n",
      "paired_colors = [matplotlib.colors.to_hex(i) for i in sns.color_palette(\"Paired\",10)]\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"Topico_\"  + df2.topic.astype(str)\n",
      "fig = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",\n",
      "             facet_row=\"stance\", color_discrete_sequence=custom_colors)\n",
      "\n",
      "\n",
      "(fig\n",
      ".update_layout(dict(plot_bgcolor='rgba(0,0,0,0)'))\n",
      ".update_yaxes(matches=None)\n",
      ".for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
      "\n",
      ".update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black',)\n",
      ".update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black',)\n",
      ".update_layout(showlegend=False)\n",
      ")\n",
      "for y,a in zip([1, 0.33, 0.66], fig['layout']['annotations']):\n",
      "    a['text'] = a['text'].capitalize()\n",
      "    a['y'] = y\n",
      "    a['x'] = 0.49\n",
      "    a['xanchor'] = 'center'\n",
      "    a['textangle'] = 0\n",
      "    a['font'] = {'size':20}\n",
      "\n",
      "fig.show()\n",
      "\n",
      "\n",
      "aa  = []\n",
      "fig = px.scatter(px.data.tips(), x=\"total_bill\", y=\"tip\", facet_col=\"smoker\")\n",
      "fig.for_each_annotation(lambda a: aa.append(a.update(text=a.text.split(\"=\")[-1])))\n",
      "fig.show()\n",
      "167/1:\n",
      "import os\n",
      "import pandas as pd\n",
      "167/2: ruta = \"/home/fernando/git/redes/data_old\"\n",
      "167/3: os.listdir(ruta)\n",
      "167/4: os.listfiles(ruta)\n",
      "167/5: os.listfile(ruta)\n",
      "167/6: os.listdir(ruta)\n",
      "167/7:\n",
      "aysa = '/home/fernando/git/redes/data_old/AYSA'\n",
      "c1 = '/home/fernando/git/redes/data_old/COYUNTURA_1'\n",
      "c2 = '/home/fernando/git/redes/data_old/COYUNTURA_2'\n",
      "politca = '/home/fernando/git/redes/data_old/POLITICA'\n",
      "167/8:\n",
      "dict(aysa = '/home/fernando/git/redes/data_old/AYSA'\n",
      "c1 = '/home/fernando/git/redes/data_old/COYUNTURA_1'\n",
      "c2 = '/home/fernando/git/redes/data_old/COYUNTURA_2'\n",
      "politca = '/home/fernando/git/redes/data_old/POLITICA')\n",
      "167/9:\n",
      "dict(\"aysa\" = '/home/fernando/git/redes/data_old/AYSA'\n",
      "\"c1\" = '/home/fernando/git/redes/data_old/COYUNTURA_1'\n",
      "\"c2\" = '/home/fernando/git/redes/data_old/COYUNTURA_2'\n",
      "\"politca\" = '/home/fernando/git/redes/data_old/POLITICA')\n",
      "167/10:\n",
      "dict(aysa = '/home/fernando/git/redes/data_old/AYSA',\n",
      "c1 = '/home/fernando/git/redes/data_old/COYUNTURA_1',\n",
      "c2 = '/home/fernando/git/redes/data_old/COYUNTURA_2',\n",
      "politca = '/home/fernando/git/redes/data_old/POLITICA')\n",
      "167/11:\n",
      "files = dict(aysa = '/home/fernando/git/redes/data_old/AYSA',\n",
      "c1 = '/home/fernando/git/redes/data_old/COYUNTURA_1',\n",
      "c2 = '/home/fernando/git/redes/data_old/COYUNTURA_2',\n",
      "politca = '/home/fernando/git/redes/data_old/POLITICA')\n",
      "167/12: {k:os.listdir(v) for k,v in files.items()}\n",
      "167/13: archivos = {k:os.listdir(v) for k,v in files.items()}\n",
      "167/14: archivos\n",
      "167/15:\n",
      "files = dict(aysa = '/home/fernando/git/redes/data_old/AYSA',\n",
      "c1 = '/home/fernando/git/redes/data_old/COYUNTURA_1',\n",
      "c2 = '/home/fernando/git/redes/data_old/COYUNTURA_2',\n",
      "politca = '/home/fernando/git/redes/data_old/POLITICA',\n",
      "base =  '/home/fernando/git/redes/data_old')\n",
      "167/16: archivos = {k:os.listdir(v) for k,v in files.items()}\n",
      "167/17: archivos\n",
      "167/18:\n",
      "for archivo in archivos['base']:\n",
      "    print(archivo)\n",
      "167/19:\n",
      "lista = []\n",
      "for k,v in arhivos.items():\n",
      "    lista.extend(v)\n",
      "167/20:\n",
      "lista = []\n",
      "for k,v in archivos.items():\n",
      "    lista.extend(v)\n",
      "167/21: lista\n",
      "167/22: pd.Series(lista)\n",
      "167/23: pd.Series(lista).value_counts()\n",
      "167/24: pd.Series(lista).value_counts()[:40]\n",
      "167/25: pd.Series(lista).value_counts()[pd.Series(lista).value_counts()< 5]\n",
      "168/1: import pandas as pd\n",
      "168/2: pd.read_csv('data_jc.csv')\n",
      "168/3: db = pd.read_csv('data_jc.csv')\n",
      "168/4: db = pd.read_csv('data_jc.csv')\n",
      "168/5: db\n",
      "168/6: ndata = ['22/11/2020', 'Fer', \"2020\", 'Nar', 'Titulo', 'comentario', 'link', 'doi', '', '']\n",
      "168/7: ndata\n",
      "168/8: db.append(ndata)\n",
      "168/9: db.append(pd.Series(ndata))\n",
      "168/10: db.append(pd.Series(ndata), ignore_index=True)\n",
      "168/11: db\n",
      "168/12: db\n",
      "168/13: db[len(db)-1]\n",
      "168/14: db.iloc[len(db)-1]\n",
      "168/15:\n",
      "JC Date                                                       20/11/2020\n",
      "Found by                                                             Fer\n",
      "Year                                                                2020\n",
      "Journal                                      Briefings in Bioinformatics\n",
      "Title                  MloDisDB: a manually curated database of the r...\n",
      "Notes                                      Auto explaining, para Alvaro.\n",
      "Link or DOI                                                          DOI\n",
      "Presentado?                                                          NaN\n",
      "Presentation Slides                                                  NaN\n",
      "168/16: db.iloc[len(db)-1]\n",
      "168/17: clave = ['JC Date', 'Found by', 'Year', 'Journal', 'Title', 'Notes', 'Link or DOI', 'Presentado?', 'Presentation Slides']\n",
      "168/18: dict(zip(clave, ndata))\n",
      "168/19: new = dict(zip(clave, ndata))\n",
      "168/20: new\n",
      "168/21: db.append(new)\n",
      "168/22: db.append(new, ignore_index=True)\n",
      "168/23: clave\n",
      "168/24:  from datetime import datetime\n",
      "168/25: now = datetime.now()\n",
      "168/26: now\n",
      "168/27: now.strftime(\"%d/%m/%Y\")\n",
      "169/1: import pandas as pd\n",
      "169/2: db = pd.read_csv('/home/fernando/git/biblio/database.csv')\n",
      "169/3: db\n",
      "169/4: db.iloc[0]\n",
      "169/5: db = pd.read_csv('/home/fernando/git/biblio/data/db.csv')\n",
      "169/6: db.iloc[0]\n",
      "169/7: db = pd.read_csv('/home/fernando/git/biblio/data/db.csv')\n",
      "169/8: db.iloc[0]\n",
      "169/9: db\n",
      "169/10: db.head()\n",
      "169/11: db.append(db.head())\n",
      "169/12: db\n",
      "169/13:\n",
      "    nd = {'paper_id':_id + 1,\n",
      "    'date':datetime.now().strftime(\"%d/%m/%Y\"),\n",
      "    'owner':current_user.username,\n",
      "    'title':title,\n",
      "    'link': journal,\n",
      "    'doi':title,\n",
      "    'main_author': authors,\n",
      "    'abstract': link,\n",
      "    'comentarios':comentario,\n",
      "    'keywords':tags,\n",
      "    'presentado':'',\n",
      "    'presentacion':''}\n",
      "169/14: data\n",
      "169/15: db\n",
      "169/16:\n",
      "nd = {'owner':current_user.username,\n",
      "'title':title,\n",
      "'link': journal,\n",
      "'doi':title}\n",
      "169/17:\n",
      "nd = {\n",
      "'title':title,\n",
      "'link': journal,\n",
      "'doi':title}\n",
      "169/18:\n",
      "nd = {\n",
      "'title':3\n",
      ",\n",
      "'link': 2,\n",
      "'doi':1}\n",
      "169/19: db\n",
      "169/20: db.append(pd.Series(nd))\n",
      "169/21: db.append(pd.Series(nd), ignore_index=true)\n",
      "169/22: db.append(pd.Series(nd), ignore_index=True)\n",
      "169/23:\n",
      "nd = {\n",
      "'date':3\n",
      ",\n",
      "'link': 2,\n",
      "'doi':1}\n",
      "169/24: db.append(pd.Series(nd), ignore_index=True)\n",
      "169/25: db\n",
      "169/26: db.append(pd.Series(nd), ignore_index=True)\n",
      "174/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "import localcider\n",
      "174/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "174/3: tablas['lc']\n",
      "174/4: tablas['human_lc_box1']\n",
      "174/5: tablas['human_lc_box1'].query('method == \"SEG_intermediate\"')\n",
      "174/6: lc = tablas['human_lc_box1'].query('method == \"SEG_intermediate\"')\n",
      "174/7: lc.loc[0,4]\n",
      "174/8: lc.iloc[0,4]\n",
      "174/9: eval(lc.iloc[0,4])\n",
      "174/10: lc.iloc[:,4].map(eval)\n",
      "174/11: lc.iloc[:,4].map(eval)[8]\n",
      "174/12: lc.iloc[:,4].map(eval)[8][0]\n",
      "174/13: lc\n",
      "174/14: lc['entropia'] = lc.entropia.map(eval)\n",
      "174/15: lc\n",
      "174/16: lc['meane'] = pd.entropia.map(np.mean(entropia))\n",
      "174/17: lc['meane'] = pd.entropia.map(np.mean(\"entropia\"))\n",
      "174/18: lc['meane'] = lc.entropia.map(lambda x: np.mean(x))\n",
      "174/19: lc\n",
      "174/20: lc.sort_values(['entropia'])\n",
      "174/21: tablas['db']\n",
      "174/22: tablas['db'].query('uniprot == \"O00401\"')\n",
      "174/23: tablas['db'].query('uniprot == \"O60500\"')\n",
      "174/24: lc.query('uniprot == \"O60500\"')\n",
      "175/1:\n",
      "import plotly.express as px\n",
      "import dash_core_components as dcc\n",
      "import dash_html_components as html\n",
      "import pandas as pd\n",
      "import math\n",
      "import dash\n",
      "\n",
      "#     Falta afinar algunas cosas y pasarlos un poco en limpio. Y agregar en el primero los numeros a la derecha\n",
      "\n",
      "# # # Grafico de cantidad de Tweets\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "df.columns = ['nombre', '1er Nivel', '2do Nivel', 'ct']\n",
      "df['1er Nivel'] = df['1er Nivel'] * -1  # Columna con negativos para ponerlos a las barras a la izquierda\n",
      "df_melt = df[['nombre','1er Nivel','2do Nivel']].melt(id_vars='nombre')\n",
      "df_melt['value_pos'] = df_melt['value'].map(abs).astype(int)\n",
      "df_melt = df_melt.sort_values('nombre')\n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#980c13', '#d92523', '#980c13', '#d92523', '#084a91', '#2e7ebc', '#fd8c00', '#ffa600']\n",
      "175/2:\n",
      "import plotly.express as px\n",
      "import dash_core_components as dcc\n",
      "import dash_html_components as html\n",
      "import pandas as pd\n",
      "import math\n",
      "import dash\n",
      "\n",
      "#     Falta afinar algunas cosas y pasarlos un poco en limpio. Y agregar en el primero los numeros a la derecha\n",
      "\n",
      "# # # Grafico de cantidad de Tweets\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "df.columns = ['nombre', '1er Nivel', '2do Nivel', 'ct']\n",
      "df['1er Nivel'] = df['1er Nivel'] * -1  # Columna con negativos para ponerlos a las barras a la izquierda\n",
      "df_melt = df[['nombre','1er Nivel','2do Nivel']].melt(id_vars='nombre')\n",
      "df_melt['value_pos'] = df_melt['value'].map(abs).astype(int)\n",
      "df_melt = df_melt.sort_values('nombre')\n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#980c13', '#d92523', '#980c13', '#d92523', '#084a91', '#2e7ebc', '#fd8c00', '#ffa600']\n",
      "175/3: df_melt\n",
      "175/4: fig = go.Figure()\n",
      "175/5: import plotly.graph_objects as go\n",
      "175/6: fig = go.Figure()\n",
      "175/7: df_melt\n",
      "175/8:\n",
      "fig.add_trace(go.Bar(df_melt.query('variable == \"2do nivel\"'),\n",
      "    y = 'nombre', x = 'value', orientation='h',\n",
      "))\n",
      "175/9:\n",
      "fig.add_trace(go.Bar(data=df_melt.query('variable == \"2do nivel\"'),\n",
      "    y = 'nombre', x = 'value', orientation='h',\n",
      "))\n",
      "175/10:\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do nivel\"')['value'], orientation='h',\n",
      "))\n",
      "175/11:  df_melt.query('variable == \"2do nivel\"')[\"nombre\"]\n",
      "175/12: df_melt\n",
      "175/13:\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "177/1:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "179/1:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "177/2:\n",
      "import plotly.express as px\n",
      "import dash_core_components as dcc\n",
      "import dash_html_components as html\n",
      "import pandas as pd\n",
      "import math\n",
      "\n",
      "import dash\n",
      "#     Falta afinar algunas cosas y pasarlos un poco en limpio. Y agregar en el primero los numeros a la derecha\n",
      "# # # Grafico de cantidad de Tweets\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "df.columns = ['nombre', '1er Nivel', '2do Nivel', 'ct']\n",
      "df['1er Nivel'] = df['1er Nivel'] * -1  # Columna con negativos para ponerlos a las barras a la izquierda\n",
      "df_melt = df[['nombre','1er Nivel','2do Nivel']].melt(id_vars='nombre')\n",
      "df_melt['value_pos'] = df_melt['value'].map(abs).astype(int)\n",
      "df_melt = df_melt.sort_values('nombre')\n",
      "\n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "\n",
      "colores = ['#980c13', '#d92523', '#980c13', '#d92523', '#084a91', '#2e7ebc', '#fd8c00', '#ffa600']\n",
      "177/3:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "175/14:\n",
      "#import plotly.express as px\n",
      "\n",
      "import dash_core_components as dcc\n",
      "import dash_html_components as html\n",
      "import pandas as pd\n",
      "import math\n",
      "import dash\n",
      "\n",
      "import plotly.graph_objects as go\n",
      "#     Falta afinar algunas cosas y pasarlos un poco en limpio. Y agregar en el primero los numeros a la derecha\n",
      "# # # Grafico de cantidad de Tweets\n",
      "\n",
      "df = pd.read_csv('data.csv')\n",
      "df.columns = ['nombre', '1er Nivel', '2do Nivel', 'ct']\n",
      "df['1er Nivel'] = df['1er Nivel'] * -1  # Columna con negativos para ponerlos a las barras a la izquierda\n",
      "df_melt = df[['nombre','1er Nivel','2do Nivel']].melt(id_vars='nombre')\n",
      "df_melt['value_pos'] = df_melt['value'].map(abs).astype(int)\n",
      "df_melt = df_melt.sort_values('nombre')\n",
      "\n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "\n",
      "colores = ['#980c13', '#d92523', '#980c13', '#d92523', '#084a91', '#2e7ebc', '#fd8c00', '#ffa600']\n",
      "175/15:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "175/16: df_melt\n",
      "175/17:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable != \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "175/18:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable != \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.update_layout(barmode='stack')\n",
      "fig.show()\n",
      "175/19:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable != \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.update_layout(barmode='relative')\n",
      "fig.show()\n",
      "175/20:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable != \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.update_layout(barmode='relative')\n",
      "fig.update_yaxes(title=None,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_traces(hovertemplate='%{y} <br>%{customdata[1]} <br>%{customdata[0]} tweets<extra></extra>')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "min_break, max_break, step = nice_bounds(df_melt.value.min(), df_melt.value.max()) # Calcular los breaks\n",
      "vals = list(range(min_break, max_break, step))\n",
      "layout=dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "fig.update_layout(layout)\n",
      "177/4:\n",
      "# funciones para calcular los breaks del eje X, como hay valores negativos no se puede dejar el automatico de plotly\n",
      "\n",
      "def nice_number(value, round_=False):\n",
      "    exponent = math.floor(math.log(value, 10))\n",
      "    fraction = value / 10 ** exponent\n",
      "    if round_:\n",
      "        if fraction < 1.5:\n",
      "            nice_fraction = 1.\n",
      "        elif fraction < 3.:\n",
      "            nice_fraction = 2.\n",
      "        elif fraction < 7.:\n",
      "            nice_fraction = 5.\n",
      "        else:\n",
      "            nice_fraction = 10.\n",
      "    else:\n",
      "        if fraction <= 1:\n",
      "            nice_fraction = 1.\n",
      "        elif fraction <= 2:\n",
      "            nice_fraction = 2.\n",
      "        elif fraction <= 5:\n",
      "            nice_fraction = 5.\n",
      "        else:\n",
      "            nice_fraction = 10.\n",
      "    return nice_fraction * 10 ** exponent\n",
      "\n",
      "def nice_bounds(axis_start, axis_end, num_ticks=10):\n",
      "    axis_width = axis_end - axis_start\n",
      "    if axis_width == 0:\n",
      "        nice_tick = 0\n",
      "    else:\n",
      "        nice_range = nice_number(axis_width)\n",
      "        nice_tick = nice_number(nice_range / (num_ticks - 1), round_=True)\n",
      "        axis_start = math.floor(axis_start / nice_tick) * nice_tick\n",
      "        axis_end = math.ceil(axis_end / nice_tick) * nice_tick\n",
      "    return int(axis_start), int(axis_end), int(nice_tick)\n",
      "175/21:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable != \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.update_layout(barmode='relative')\n",
      "fig.update_yaxes(title=None,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_traces(hovertemplate='%{y} <br>%{customdata[1]} <br>%{customdata[0]} tweets<extra></extra>')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "min_break, max_break, step = nice_bounds(df_melt.value.min(), df_melt.value.max()) # Calcular los breaks\n",
      "vals = list(range(min_break, max_break, step))\n",
      "layout=dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "fig.update_layout(layout)\n",
      "177/5:\n",
      "def nice_bounds(axis_start, axis_end, num_ticks=10):\n",
      "    axis_width = axis_end - axis_start\n",
      "    if axis_width == 0:\n",
      "        nice_tick = 0\n",
      "    else:\n",
      "        nice_range = nice_number(axis_width)\n",
      "        nice_tick = nice_number(nice_range / (num_ticks - 1), round_=True)\n",
      "        axis_start = math.floor(axis_start / nice_tick) * nice_tick\n",
      "        axis_end = math.ceil(axis_end / nice_tick) * nice_tick\n",
      "    return int(axis_start), int(axis_end), int(nice_tick)\n",
      "177/6:\n",
      "def nice_number(value, round_=False):\n",
      "    exponent = math.floor(math.log(value, 10))\n",
      "    fraction = value / 10 ** exponent\n",
      "    if round_:\n",
      "        if fraction < 1.5:\n",
      "            nice_fraction = 1.\n",
      "        elif fraction < 3.:\n",
      "            nice_fraction = 2.\n",
      "        elif fraction < 7.:\n",
      "            nice_fraction = 5.\n",
      "        else:\n",
      "            nice_fraction = 10.\n",
      "    else:\n",
      "        if fraction <= 1:\n",
      "            nice_fraction = 1.\n",
      "        elif fraction <= 2:\n",
      "            nice_fraction = 2.\n",
      "        elif fraction <= 5:\n",
      "            nice_fraction = 5.\n",
      "        else:\n",
      "            nice_fraction = 10.\n",
      "    return nice_fraction * 10 ** exponent\n",
      "175/22:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable != \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.update_layout(barmode='relative')\n",
      "fig.update_yaxes(title=None,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_traces(hovertemplate='%{y} <br>%{customdata[1]} <br>%{customdata[0]} tweets<extra></extra>')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "min_break, max_break, step = nice_bounds(df_melt.value.min(), df_melt.value.max()) # Calcular los breaks\n",
      "vals = list(range(min_break, max_break, step))\n",
      "layout=dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "fig.update_layout(layout)\n",
      "175/23: nice_bounds()\n",
      "175/24:\n",
      "\n",
      "# funciones para calcular los breaks del eje X, como hay valores negativos no se puede dejar el automatico de plotly\n",
      "def nice_number(value, round_=False):\n",
      "    exponent = math.floor(math.log(value, 10))\n",
      "    fraction = value / 10 ** exponent\n",
      "    if round_:\n",
      "        if fraction < 1.5:\n",
      "            nice_fraction = 1.\n",
      "        elif fraction < 3.:\n",
      "            nice_fraction = 2.\n",
      "        elif fraction < 7.:\n",
      "            nice_fraction = 5.\n",
      "        else:\n",
      "            nice_fraction = 10.\n",
      "    else:\n",
      "        if fraction <= 1:\n",
      "            nice_fraction = 1.\n",
      "        elif fraction <= 2:\n",
      "            nice_fraction = 2.\n",
      "        elif fraction <= 5:\n",
      "            nice_fraction = 5.\n",
      "        else:\n",
      "            nice_fraction = 10.\n",
      "    return nice_fraction * 10 ** exponent\n",
      "\n",
      "def nice_bounds(axis_start, axis_end, num_ticks=10):\n",
      "    axis_width = axis_end - axis_start\n",
      "    if axis_width == 0:\n",
      "        nice_tick = 0\n",
      "    else:\n",
      "        nice_range = nice_number(axis_width)\n",
      "        nice_tick = nice_number(nice_range / (num_ticks - 1), round_=True)\n",
      "        axis_start = math.floor(axis_start / nice_tick) * nice_tick\n",
      "        axis_end = math.ceil(axis_end / nice_tick) * nice_tick\n",
      "\n",
      "    return int(axis_start), int(axis_end), int(nice_tick)\n",
      "175/25: nice_bounds()\n",
      "175/26:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable != \"2do Nivel\"')['value'], orientation='h',\n",
      "))\n",
      "\n",
      "fig.update_layout(barmode='relative')\n",
      "fig.update_yaxes(title=None,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_traces(hovertemplate='%{y} <br>%{customdata[1]} <br>%{customdata[0]} tweets<extra></extra>')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "min_break, max_break, step = nice_bounds(df_melt.value.min(), df_melt.value.max()) # Calcular los breaks\n",
      "vals = list(range(min_break, max_break, step))\n",
      "layout=dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "fig.update_layout(layout)\n",
      "175/27:\n",
      "fig = go.Figure()\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',labels={'value':'Cantidad de Tweets',}, hover_data=['value_pos','variable'],\n",
      "color_discrete_sequence=colores,width=1000, height=400\n",
      ")\n",
      "175/28:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "labels={'value':'Cantidad de Tweets',}, hover_data=['value_pos','variable'],\n",
      "color_discrete_sequence=colores,width=1000, height=400))\n",
      "175/29:\n",
      "fig = go.Figure()\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      ", hovertemplate = '%{y} <br>%{customdata[1]} <br>%{value_pos} tweets<extra></extra>'))\n",
      "175/30:\n",
      "\n",
      "\n",
      "\n",
      "fig = go.Figure()\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      ", hovertemplate = '%{y} <br>%{customdata[1]} <br>%{value_pos} tweets'))\n",
      "175/31:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "hovertemplate = '%{y} <br>%{customdata[1]} <br>%{value_pos} tweets'))\n",
      "175/32:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "hovertemplate = '%{y} <br>%{variable} <br>%{value_pos} tweets'))\n",
      "175/33:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "hovertemplate = '%{y} <br>%{variable} <br>%{valores} tweets', valores = df_melt.query('variable == \"2do Nivel\"')[\"variable\"]))\n",
      "175/34:\n",
      "fig = go.Figure()\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "hovertemplate = '%{y} <br>%{variable} <br>%{valores} tweets', valores = df_melt.query('variable == \"2do Nivel\"')[\"variable\"]))\n",
      "175/35:\n",
      "valores =  df_melt.query('variable == \"2do Nivel\"')['value_pos']\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "hovertemplate = '%{y} <br>%{variable} <br>%{valores} tweets', valores = df_melt.query('variable == \"2do Nivel\"')[\"variable\"]))\n",
      "175/36:\n",
      "valores =  df_melt.query('variable == \"2do Nivel\"')['value_pos']\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"], x = df_melt.query('variable == \"2do Nivel\"')['value'], orientation='h',\n",
      "hovertemplate = '%{y} <br>%{variable} <br>%{valores} tweets', ))\n",
      "175/37:\n",
      "valores =  df_melt.query('variable == \"2do Nivel\"')['value_pos']\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"2do Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"2do Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>%{variable} <br>%{text} tweets', ))\n",
      "175/38:\n",
      "valores =  df_melt.query('variable == \"2do Nivel\"')['value_pos']\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"2do Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"2do Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br> 2do Nivel <br>%{text} tweets', ))\n",
      "175/39:\n",
      "valores =  df_melt.query('variable == \"2do Nivel\"')['value_pos']\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"2do Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"2do Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>2do Nivel <br>%{text} tweets', ))\n",
      "175/40:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"2do Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"2do Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>2do Nivel <br>%{text} tweets', ))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"2do Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"1er Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>1er Nivel <br>%{text} tweets', ))\n",
      "175/41:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"2do Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"2do Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>2do Nivel <br>%{text} tweets', ))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"1er Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"1er Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>1er Nivel <br>%{text} tweets', ))\n",
      "175/42:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"2do Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"2do Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>2do Nivel <br>%{text} tweets', ))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"1er Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"1er Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>1er Nivel <br>%{text} tweets', ))\n",
      "\n",
      "fig.update_layout(barmode='relative')\n",
      "175/43:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"2do Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"2do Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>2do Nivel <br>%{text} tweets', ))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"1er Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"1er Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>1er Nivel <br>%{text} tweets', ))\n",
      "\n",
      "fig.update_layout(barmode='relative')\n",
      "fig.update_yaxes(title=None,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "min_break, max_break, step = nice_bounds(df_melt.value.min(), df_melt.value.max()) # Calcular los breaks\n",
      "vals = list(range(min_break, max_break, step))\n",
      "layout=dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "fig.update_layout(layout)\n",
      "175/44:\n",
      "fig = go.Figure()\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable == \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"2do Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"2do Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>2do Nivel <br>%{text} tweets<extra></extra>', ))\n",
      "\n",
      "fig.add_trace(go.Bar(y = df_melt.query('variable != \"2do Nivel\"')[\"nombre\"],\n",
      " x = df_melt.query('variable == \"1er Nivel\"')['value'],\n",
      " orientation='h',\n",
      " text = df_melt.query('variable == \"1er Nivel\"')['value_pos'],\n",
      "hovertemplate = '%{y} <br>1er Nivel <br>%{text} tweets<extra></extra>', ))\n",
      "\n",
      "fig.update_layout(barmode='relative')\n",
      "fig.update_yaxes(title=None,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "min_break, max_break, step = nice_bounds(df_melt.value.min(), df_melt.value.max()) # Calcular los breaks\n",
      "vals = list(range(min_break, max_break, step))\n",
      "layout=dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "fig.update_layout(layout)\n",
      "175/45:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "    df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "    # Ver si pasarlo como un diccionario\n",
      "    colores = ['#fd8c00', '#ffa600', #MV \n",
      "               '#084a91', '#2e7ebc', #SM\n",
      "               '#fd8c00', '#ffa600', #MM\n",
      "               '#980c13', '#d92523', #AK\n",
      "               '#980c13', '#d92523', #CFK\n",
      "               '#980c13', '#d92523'] #AF\n",
      "\n",
      "    fig = go.Figure(\n",
      "        data=go.Bar(x=df_melt[\"value\"], \n",
      "                    y=df_melt[\"nombre\"], \n",
      "                     orientation='h', \n",
      "                     marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                     customdata=df_melt['value_pos'],\n",
      "                     hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                     )\n",
      "        )\n",
      "    fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "    fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "    fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "    x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "    for _,row in df.iterrows():\n",
      "        fig.add_annotation(\n",
      "            xref=\"x\",\n",
      "            yref=\"y\",\n",
      "            x=x_anot,\n",
      "            y=row.nombre,\n",
      "            text=row.ct,\n",
      "            showarrow=False,\n",
      "        )\n",
      "    fig.add_annotation(\n",
      "            xref=\"x\",\n",
      "            yref=\"paper\",\n",
      "            x=-2000,\n",
      "            y=1.2,\n",
      "            text=\"Primer nivel\",\n",
      "            showarrow=False,\n",
      "        )\n",
      "    fig.add_annotation(\n",
      "            xref=\"x\",\n",
      "            yref=\"paper\",\n",
      "            x=2000,\n",
      "            y=1.2,\n",
      "            text=\"Segundo nivel\",\n",
      "            showarrow=False,\n",
      "        )\n",
      "    fig.add_annotation(\n",
      "            xref=\"x\",\n",
      "            yref=\"paper\",\n",
      "            x=max(df['2do Nivel'])+1000,\n",
      "            y=1.2,\n",
      "            text=\"Ratio 1°/2° nivel\",\n",
      "            showarrow=False,\n",
      "        )\n",
      "\n",
      "    component = html.Div(\n",
      "        dcc.Graph(figure=fig,\n",
      "                  config={'displayModeBar': False})\n",
      "    )\n",
      "175/46:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=-2000,\n",
      "        y=1.2,\n",
      "        text=\"Primer nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=2000,\n",
      "        y=1.2,\n",
      "        text=\"Segundo nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=max(df['2do Nivel'])+1000,\n",
      "        y=1.2,\n",
      "        text=\"Ratio 1°/2° nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "component = html.Div(\n",
      "    dcc.Graph(figure=fig,\n",
      "                config={'displayModeBar': False})\n",
      ")\n",
      "175/47: fig.show()\n",
      "175/48:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "175/49: fig.show()\n",
      "175/50:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')\n",
      "175/51:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "175/52: fig.show()\n",
      "175/53:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=-2000,\n",
      "        y=1.2,\n",
      "        text=\"Primer nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=2000,\n",
      "        y=1.2,\n",
      "        text=\"Segundo nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=max(df['2do Nivel'])+1000,\n",
      "        y=1.2,\n",
      "        text=\"Ratio 1°/2° nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.show()\n",
      "175/54:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "fig.show()\n",
      "175/55:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "175/56: fig.show()\n",
      "175/57:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "fig.show()\n",
      "175/58:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=-2000,\n",
      "        y=1.2,\n",
      "        text=\"Primer nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=2000,\n",
      "        y=1.2,\n",
      "        text=\"Segundo nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "fig.show()\n",
      "175/59:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=-2000,\n",
      "        y=1.2,\n",
      "        text=\"Primer nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "\n",
      "fig.show()\n",
      "175/60:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "\n",
      "\n",
      "fig.show()\n",
      "175/61:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "for _,row in df.iterrows():\n",
      "    pass\n",
      "\n",
      "\n",
      "fig.show()\n",
      "175/62:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=-2000,\n",
      "        y=1.2,\n",
      "        text=\"Primer nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=2000,\n",
      "        y=1.2,\n",
      "        text=\"Segundo nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=max(df['2do Nivel'])+1000,\n",
      "        y=1.2,\n",
      "        text=\"Ratio 1°/2° nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.show()\n",
      "175/63: x_anot\n",
      "175/64: fig.update_layout(dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] )))\n",
      "175/65:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=-2000,\n",
      "        y=1.2,\n",
      "        text=\"Primer nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=2000,\n",
      "        y=1.2,\n",
      "        text=\"Segundo nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=max(df['2do Nivel'])+1000,\n",
      "        y=1.2,\n",
      "        text=\"Ratio 1°/2° nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.show()\n",
      "175/66:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=-2000,\n",
      "        y=1.2,\n",
      "        text=\"Primer nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=2000,\n",
      "        y=1.2,\n",
      "        text=\"Segundo nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=max(df['2do Nivel'])+1000,\n",
      "        y=1.2,\n",
      "        text=\"Ratio 1°/2° nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "fig.show()\n",
      "175/67:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=-2000,\n",
      "        y=1.2,\n",
      "        text=\"Primer nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=2000,\n",
      "        y=1.2,\n",
      "        text=\"Segundo nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=max(df['2do Nivel'])+1000,\n",
      "        y=1.2,\n",
      "        text=\"Ratio 1°/2° nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.update_layout(dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20)))\n",
      "fig.show()\n",
      "177/7: 2\n",
      "175/68:\n",
      "df_melt = df_melt.sort_values('nombre', ascending=False)\n",
      "    \n",
      "df_melt['colors'] = [str(i) for i in range(len(df_melt))] # borrar proximamente\n",
      "# Ver si pasarlo como un diccionario\n",
      "colores = ['#fd8c00', '#ffa600', #MV \n",
      "            '#084a91', '#2e7ebc', #SM\n",
      "            '#fd8c00', '#ffa600', #MM\n",
      "            '#980c13', '#d92523', #AK\n",
      "            '#980c13', '#d92523', #CFK\n",
      "            '#980c13', '#d92523'] #AF\n",
      "\n",
      "fig = go.Figure(\n",
      "    data=go.Bar(x=df_melt[\"value\"], \n",
      "                y=df_melt[\"nombre\"], \n",
      "                    orientation='h', \n",
      "                    marker=dict(color=[colores[int(i)] for i in df_melt['colors']]),\n",
      "                    customdata=df_melt['value_pos'],\n",
      "                    hovertemplate='%{y} <br>%{customdata} tweets<extra></extra>',\n",
      "                    )\n",
      "    )\n",
      "fig.update_yaxes(fixedrange=True, title=None ,showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_xaxes(fixedrange=True, title='Cantidad de Tweets', showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      "fig.update_layout(showlegend=False, plot_bgcolor='rgba(0,0,0,0)')   \n",
      "x_anot = df['2do Nivel'].max() + df['2do Nivel'].max() * 0.1\n",
      "dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20))\n",
      "for _,row in df.iterrows():\n",
      "    fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"y\",\n",
      "        x=x_anot,\n",
      "        y=row.nombre,\n",
      "        text=row.ct,\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=-2000,\n",
      "        y=1.2,\n",
      "        text=\"Primer nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=2000,\n",
      "        y=1.2,\n",
      "        text=\"Segundo nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.add_annotation(\n",
      "        xref=\"x\",\n",
      "        yref=\"paper\",\n",
      "        x=max(df['2do Nivel'])+1000,\n",
      "        y=1.2,\n",
      "        text=\"Ratio 1°/2° nivel\",\n",
      "        showarrow=False,\n",
      "    )\n",
      "fig.update_layout(dict(xaxis=dict(tickvals=vals, ticktext=[abs(i)for i in vals] ), font=dict(size=20)))\n",
      "fig.show(config={'responsive':True})\n",
      "175/69:\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"Tópico \"  + df2.topic.astype(str)\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "fig2 = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",labels={'x_label':'Topicos', 'estandarizado': 'Nivel de asociación'},\n",
      "             facet_row=\"stance\",width=1200, height= 1000,color_discrete_sequence=custom_colors, hover_data=['stance'])\n",
      "\n",
      "(fig2\n",
      ".update_layout(dict(plot_bgcolor='rgba(0,0,0,0)'))\n",
      ".update_yaxes(matches=None)\n",
      ".for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
      ".update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      ".update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      ".update_traces(hovertemplate='%{customdata[0]} <br>%{x} <br>%{y}<extra></extra>')\n",
      ".update_layout(showlegend=False, font=dict(size=18))\n",
      ")\n",
      "\n",
      "fig.show()\n",
      "175/70:\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"Tópico \"  + df2.topic.astype(str)\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "fig2 = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",labels={'x_label':'Topicos', 'estandarizado': 'Nivel de asociación'},\n",
      "             facet_row=\"stance\",width=1200, height= 1000,color_discrete_sequence=custom_colors, hover_data=['stance'])\n",
      "\n",
      "(fig2\n",
      ".update_layout(dict(plot_bgcolor='rgba(0,0,0,0)'))\n",
      ".update_yaxes(matches=None)\n",
      ".for_each_annotation(lambda a: a.update(text=a.text.split(\"=\")[-1]))\n",
      ".update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      ".update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgrey', zerolinecolor='black')\n",
      ".update_traces(hovertemplate='%{customdata[0]} <br>%{x} <br>%{y}<extra></extra>')\n",
      ".update_layout(showlegend=False, font=dict(size=18))\n",
      ")\n",
      "\n",
      "fig2.show()\n",
      "175/71: df2\n",
      "175/72: fig2 = go.Figure()\n",
      "175/73:\n",
      "for stance in df2.stance.unique():\n",
      "    subset = df.query('stance == @stance')\n",
      "175/74: df2\n",
      "175/75:\n",
      "for stance in df2.stance.unique():\n",
      "    print(stance)\n",
      "    subset = df.query('stance == @stance')\n",
      "175/76:\n",
      "for stance in df2.stance.unique():\n",
      "    print(stance)\n",
      "    subset = df2.query('stance == \"@stance\"')\n",
      "175/77: df2\n",
      "175/78:\n",
      "\n",
      "fig = go.Figure()\n",
      "for stance in df2.stance.unique():\n",
      "    print(stance)\n",
      "    subset = df2.query('stance == \"@stance\"')\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys))\n",
      "\n",
      "    ))\n",
      "fig.show()\n",
      "175/79:\n",
      "fig = go.Figure()\n",
      "\n",
      "for stance in df2.stance.unique():\n",
      "    print(stance)\n",
      "    subset = df2.query('stance == \"@stance\"')\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys))\n",
      "\n",
      "fig.show()\n",
      "175/80:\n",
      "fig = go.Figure()\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2.query('stance == \"@stance\"')\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys)row=i, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/81:\n",
      "fig = go.Figure()\n",
      "\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2.query('stance == \"@stance\"')\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys),row=i, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/82:\n",
      "fig = make_subplots(rows=3, cols=1)\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2.query('stance == \"@stance\"')\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys),row=i +, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/83:\n",
      "fig = make_subplots(rows=3, cols=1)\n",
      "\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2.query('stance == \"@stance\"')\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys),row=i +1, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/84:\n",
      "from plotly.subplots import make_subplots\n",
      "fig = make_subplots(rows=3, cols=1)\n",
      "\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2.query('stance == \"@stance\"')\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys),row=i +1, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/85:\n",
      "from plotly.subplots import make_subplots\n",
      "fig = make_subplots(rows=3, cols=1)\n",
      "\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2.query('stance == \"@stance\"')\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys),row=i +1, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/86: subet\n",
      "175/87: subset\n",
      "175/88: df2\n",
      "175/89: stance\n",
      "175/90: df2.query('stance == \"@stance\"')\n",
      "175/91: df2.query('stance == \"@stance\"')\n",
      "175/92: df2.query('stance == \"indecidible\"')\n",
      "175/93: subset = df2[df2.stance == stance]\n",
      "175/94: subset\n",
      "175/95:\n",
      "fig = make_subplots(rows=3, cols=1)\n",
      "\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2[df2.stance == stance]\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys),row=i +1, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/96:\n",
      "fig = make_subplots(rows=3, cols=1)\n",
      "\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2[df2.stance == stance]\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys, marker_color=custom_colors,\n",
      "    ),row=i +1, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/97:\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"T \"  + df2.topic.astype(str)\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "fig2 = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",labels={'x_label':'Topicos', 'estandarizado': 'Nivel de asociación'},\n",
      "             facet_row=\"stance\",width=1200, height= 1000,color_discrete_sequence=custom_colors, hover_data=['stance'])\n",
      "\n",
      "from plotly.subplots import make_subplots\n",
      "fig = make_subplots(rows=3, cols=1)\n",
      "\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2[df2.stance == stance]\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys, marker_color=custom_colors,\n",
      "    ),row=i +1, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/98:\n",
      "df2 = pd.read_csv('data2.csv')\n",
      "df2['x_label'] = \"T\"  + df2.topic.astype(str)\n",
      "custom_colors = [\"#bdbdbd\", \"#636363\", \"#bcbddc\", \"#756bb1\",\"#a1d99b\", '#74c476', \"#fdd0a2\", \"#fd8d3c\", '#c6dbef', \"#6baed6\" ]\n",
      "\n",
      "fig2 = px.bar(df2, x=\"x_label\", y=\"estandarizado\", color=\"x_label\",labels={'x_label':'Topicos', 'estandarizado': 'Nivel de asociación'},\n",
      "             facet_row=\"stance\",width=1200, height= 1000,color_discrete_sequence=custom_colors, hover_data=['stance'])\n",
      "\n",
      "from plotly.subplots import make_subplots\n",
      "fig = make_subplots(rows=3, cols=1)\n",
      "\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2[df2.stance == stance]\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys, marker_color=custom_colors,\n",
      "    ),row=i +1, col=1)\n",
      "\n",
      "fig.show()\n",
      "175/99:\n",
      "fig = make_subplots(rows=3, cols=1)\n",
      "\n",
      "for i, stance in enumerate(df2.stance.unique()):\n",
      "    print(stance)\n",
      "    subset = df2[df2.stance == stance]\n",
      "    xs = subset.x_label\n",
      "    ys = subset.estandarizado\n",
      "    fig.append_trace(go.Bar( x = xs, y = ys, marker_color=custom_colors,\n",
      "    ),row=i +1, col=1)\n",
      "\n",
      "fig.update_layout(template='plotly_white')\n",
      "fig.show()\n",
      "175/100: df2\n",
      "186/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "186/2: Display(tablas['info'])\n",
      "186/3: display(tablas['info'])\n",
      "186/4: printtablas['info'])\n",
      "186/5: print(tablas['info'])\n",
      "186/6: print(*tablas['info'].keys(),)\n",
      "186/7: print(*tablas.keys(),)\n",
      "186/8: tablas['db_dis']\n",
      "186/9: tablas['db']\n",
      "186/10: tablas['db'].query('org == \"Homo sapiens\"')\n",
      "186/11: human_db = tablas['db'].query('org == \"Homo sapiens\"')\n",
      "186/12: human_db\n",
      "186/13: for i in tablas.keys():print(i)\n",
      "186/14: tablas['human_dis']\n",
      "186/15: tablas['human_dz']\n",
      "186/16: dis = tablas['human_dis']\n",
      "186/17: dis\n",
      "186/18: tablas['human_db']\n",
      "186/19: human_db\n",
      "186/20: human_db.drop_duplicates()\n",
      "186/21: mlos = human_db.drop_duplicates(['uniprot','mlo_loc']).set_index('uniprot').to_dict()\n",
      "186/22: nkis\n",
      "186/23: mlos\n",
      "186/24:\n",
      "mlos = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list).to_dict()\n",
      "mlos\n",
      "186/25:\n",
      "mlos = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list).to_dict()[\"mlo\"]\n",
      "mlos\n",
      "186/26: mlos\n",
      "186/27:\n",
      "mlos = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list).to_dict()[\"mlo_loc\"]\n",
      "mlos\n",
      "186/28: mlos\n",
      "186/29: mlos = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list).to_dict()[\"mlo_loc\"]\n",
      "186/30: dbs = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list).to_dict()[\"web\"]\n",
      "186/31: dbs\n",
      "186/32: dis\n",
      "186/33: dis[\"mlo_loc\"].map(tablas['translate'])\n",
      "186/34: human_db.map(tablas['translate'])\n",
      "186/35: human_db.mlo_loc.map(tablas['translate'])\n",
      "186/36: human_db[\"mlo_loc\"] = human_db.mlo_loc.map(tablas['translate'])\n",
      "186/37: dis.web.unique()\n",
      "186/38: human_db[\"web\"] = human_db.web.map({'phasepro':'PhaSePro', 'phasepdb':'PhaSepDB', 'drllps':'DrLLPS'})\n",
      "186/39: mlos = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list).to_dict()[\"mlo_loc\"]\n",
      "186/40: dbs = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list).to_dict()[\"web\"]\n",
      "186/41: dbs\n",
      "186/42: dbs = human_db.drop_duplicates(['uniprot','web']).groupby('uniprot').agg(list).to_dict()[\"web\"]\n",
      "186/43: dbs\n",
      "186/44: dbs = human_db.drop_duplicates(['uniprot','web']).groupby('uniprot').agg(list)['web'].to_frame()\n",
      "186/45: dbs\n",
      "186/46: dbs.merge(mlos)\n",
      "186/47: mlos = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list)[\"mlo_loc\"].to_frame()\n",
      "186/48: dbs = human_db.drop_duplicates(['uniprot','web']).groupby('uniprot').agg(list)['web'].to_frame()\n",
      "186/49: dbs.merge(mlos)\n",
      "186/50: dbs\n",
      "186/51: mlos = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list)[\"mlo_loc\"].to_frame().reset_index()\n",
      "186/52: dbs = human_db.drop_duplicates(['uniprot','web']).groupby('uniprot').agg(list)['web'].to_frame().reset_index()\n",
      "186/53: dbs\n",
      "186/54: dbs.merge(mlos)\n",
      "186/55: tabla = dbs.merge(mlos)\n",
      "186/56: tabla['dc'] = tabla.uniprot.map(tablas['human_dis'][\"uniprot\",'dc'])\n",
      "186/57: tabla.mereg(tablas['human_dis'][\"uniprot\",'dc'])\n",
      "186/58: tabla.merge(tablas['human_dis'][\"uniprot\",'dc'])\n",
      "186/59: tabla.merge(tablas['human_dis'][[\"uniprot\",'dc']])\n",
      "186/60: tabla = tabla.merge(tablas['human_dis'][[\"uniprot\",'dc']])\n",
      "186/61: tabla\n",
      "186/62: tablas['human_dz']\n",
      "186/63:\n",
      "zonas = tablas['human_dz'].copy()\n",
      "zonas['largo'] = zonas.end - zonas.end + 1\n",
      "186/64: zonas\n",
      "186/65: zonas.sort_values(['uniprot','largo'], ascending=[True, False])\n",
      "186/66:\n",
      "zonas = tablas['human_dz'].copy()\n",
      "zonas['largo'] = zonas.end - zonas.start + 1\n",
      "186/67: zonas.sort_values(['uniprot','largo'], ascending=[True, False])\n",
      "186/68: zonas.sort_values('largo')\n",
      "186/69: cantidad_dz = zonas.sort_values(['uniprot','largo'], ascending=[True, False]).uniprot.value_counts().reset_index()\n",
      "186/70: cantidad_dz)\n",
      "186/71: cantidad_dz\n",
      "186/72: tabla\n",
      "186/73: tabla.merge(cantidad_dz)\n",
      "186/74: cantidad_dz\n",
      "186/75: cantidad_dz = zonas.sort_values(['uniprot','largo'], ascending=[True, False]).uniprot.value_counts().reset_index().rename(columns={'index':'uniprot','uniprot':'ndz'})\n",
      "186/76: cantidad_dz\n",
      "186/77: tabla.merge(cantidad_dz, how='left)\n",
      "186/78: tabla.merge(cantidad_dz, how='left')\n",
      "186/79: tabla = tabla.merge(cantidad_dz, how='left')\n",
      "186/80: tabka\n",
      "186/81: tabla\n",
      "186/82: zonas.sort_values(['uniprot','largo'], ascending=[True, False])\n",
      "186/83: zonas.sort_values(['uniprot','largo'], ascending=[True, False]).drop_duplicates(\"uniprot\")\n",
      "186/84: zonas.sort_values(['uniprot','largo'], ascending=[True, False]).drop_duplicates(\"uniprot\").set_index('uniprot').largo.to_dict()\n",
      "186/85: tabla['max_dz'] = tabla.uniprot.map(zonas.sort_values(['uniprot','largo'], ascending=[True, False]).drop_duplicates(\"uniprot\").set_index('uniprot').largo.to_dict())\n",
      "186/86: tabla['max_dz']\n",
      "186/87: tabla\n",
      "186/88: tablas['human_lc']\n",
      "186/89: lc = tablas['human_lc'].query('method == \"SEG_intermediate\"')\n",
      "186/90: lc\n",
      "186/91: lc.sort_values(['uniprot','largo'], ascending=False)\n",
      "186/92: tabla['lc_max'] = lc.uniprot.map(lc.sort_values(['uniprot','largo'], ascending=False).drop_duplicates(\"uniprot\").set_index('uniprot').largo.to_dict())\n",
      "186/93: tabla\n",
      "186/94: tabla.drop_duplicates()\n",
      "186/95: tabla.drop_duplicates(\"uniprot\")\n",
      "186/96: tabla = tabla.drop_duplicates(\"uniprot\")\n",
      "186/97: tabla\n",
      "186/98: tablas\n",
      "186/99: tablas.info()\n",
      "186/100: tablas[\"info\"]\n",
      "186/101: prnt(tablas[\"info\"])\n",
      "186/102: print(tablas[\"info\"])\n",
      "186/103: pfam = tablas['human_dominios']\n",
      "186/104: pfam\n",
      "186/105: pfam[['uniprot','domain']].drop_duplicates()\n",
      "186/106: pfam[['uniprot','domain']].drop_duplicates().gropby('uniprot').agg(list)\n",
      "186/107: pfam[['uniprot','domain']].drop_duplicates().groupby('uniprot').agg(list)\n",
      "186/108: tabla.merge(pfam[['uniprot','domain']].drop_duplicates().groupby('uniprot').agg(list).reset_index(), how='left')\n",
      "186/109: tabla = tabla.merge(pfam[['uniprot','domain']].drop_duplicates().groupby('uniprot').agg(list).reset_index(), how='left')\n",
      "186/110: tabla\n",
      "186/111: print(tablas['info'])\n",
      "186/112: tablas['human_ptms_reg']\n",
      "186/113: tablas.keys()\n",
      "186/114: tablas[\"human_ptms_diseases\"]\n",
      "186/115: len(tablas[\"human_ptms_diseases\"].uniprot.unique())\n",
      "186/116: tablas[\"human_ptms_diseases\"]\n",
      "186/117: tablas[\"human_ptms_diseases\"].groupby('uniprot').agg(list).disease\n",
      "186/118: tablas[\"human_ptms_diseases\"].groupby('uniprot').agg(list).disease.to_dict()\n",
      "186/119: tabla.uniprot.map(tablas[\"human_ptms_diseases\"].groupby('uniprot').agg(list).disease.to_dict())\n",
      "186/120: tablas[\"human_ptms_diseases\"].groupby('uniprot').agg(list).disease.to_dict()\n",
      "186/121: tabla.uniprot.map(tablas[\"human_ptms_diseases\"].groupby('uniprot').agg(list).disease.to_dict())\n",
      "186/122: tabla.uniprot.map(tablas[\"human_ptms_diseases\"].groupby('uniprot').agg(list).disease.to_dict()).isna().value_counts()\n",
      "186/123: tabla['dis_ptm'] = tabla.uniprot.map(tablas[\"human_ptms_diseases\"].groupby('uniprot').agg(list).disease.to_dict())\n",
      "186/124: tabla\n",
      "186/125: tabla.sort_values('dis_ptm')\n",
      "186/126: tabla.fillna(0).sort_values('dis_ptm')\n",
      "186/127: tabla.dropna().sort_values('dis_ptm')\n",
      "186/128: tablasort_values('dis_ptm')\n",
      "186/129: tabla\n",
      "186/130: tabla[tabla.dis_ptm.notna()]\n",
      "186/131:\n",
      "tabla['ndz'] = tabla['ndz'].fillna(0).astype(int)\n",
      "tabla['lc_max'] = tabla['lc_max'].fillna(0).astype(int)\n",
      "tabla['dis_ptm'] = tabla['dis_ptm'].fillna([\"\"])\n",
      "186/132:\n",
      "tabla['ndz'] = tabla['ndz'].fillna(0).astype(int)\n",
      "tabla['lc_max'] = tabla['lc_max'].fillna(0).astype(int)\n",
      "tabla['dis_ptm'] = tabla['dis_ptm'].fillna(\"\")\n",
      "186/133: tabla\n",
      "186/134: tabla.applymap(lambda x: \", \".join(x))\n",
      "186/135: tabla.applymap(lambda x: \", \".join(list(x)))\n",
      "186/136: tabla\n",
      "186/137:\n",
      "for col in tabla.columns:\n",
      "    try:\n",
      "        tabla[col] = tabla[col].map(lambda x: \", \".join(x))\n",
      "    except:\n",
      "        pass\n",
      "186/138: tabla\n",
      "186/139: tabla[\"uniprot\"]\n",
      "186/140: tabla[\"uniprot\"].str.join()\n",
      "186/141: tabla[\"uniprot\"].map(lambda x: \"\".join(x))\n",
      "186/142: tabla[\"uniprot\"].map(lambda x: \"\".join(x.split(\", \")))\n",
      "186/143: tabla = tabla[\"uniprot\"].map(lambda x: \"\".join(x.split(\", \")))\n",
      "186/144: tabla\n",
      "186/145: human_db = tablas['db'].query('org == \"Homo sapiens\"')\n",
      "186/146: dis = tablas['human_dis']\n",
      "186/147: human_db[\"mlo_loc\"] = human_db.mlo_loc.map(tablas['translate'])\n",
      "186/148: human_db[\"web\"] = human_db.web.map({'phasepro':'PhaSePro', 'phasepdb':'PhaSepDB', 'drllps':'DrLLPS'})\n",
      "186/149: mlos = human_db.drop_duplicates(['uniprot','mlo_loc']).groupby('uniprot').agg(list)[\"mlo_loc\"].to_frame().reset_index()\n",
      "186/150: dbs = human_db.drop_duplicates(['uniprot','web']).groupby('uniprot').agg(list)['web'].to_frame().reset_index()\n",
      "186/151: tabla = dbs.merge(mlos)\n",
      "186/152: tabla = tabla.merge(tablas['human_dis'][[\"uniprot\",'dc']])\n",
      "186/153:\n",
      "zonas = tablas['human_dz'].copy()\n",
      "zonas['largo'] = zonas.end - zonas.start + 1\n",
      "186/154: cantidad_dz = zonas.sort_values(['uniprot','largo'], ascending=[True, False]).uniprot.value_counts().reset_index().rename(columns={'index':'uniprot','uniprot':'ndz'})\n",
      "186/155: tabla = tabla.merge(cantidad_dz, how='left')\n",
      "186/156: tabla['max_dz'] = tabla.uniprot.map(zonas.sort_values(['uniprot','largo'], ascending=[True, False]).drop_duplicates(\"uniprot\").set_index('uniprot').largo.to_dict())\n",
      "186/157: lc = tablas['human_lc'].query('method == \"SEG_intermediate\"')\n",
      "186/158: tabla['lc_max'] = lc.uniprot.map(lc.sort_values(['uniprot','largo'], ascending=False).drop_duplicates(\"uniprot\").set_index('uniprot').largo.to_dict())\n",
      "186/159: tabla = tabla.drop_duplicates(\"uniprot\")\n",
      "186/160: pfam = tablas['human_dominios']\n",
      "186/161: tabla = tabla.merge(pfam[['uniprot','domain']].drop_duplicates().groupby('uniprot').agg(list).reset_index(), how='left')\n",
      "186/162: tabla['dis_ptm'] = tabla.uniprot.map(tablas[\"human_ptms_diseases\"].groupby('uniprot').agg(list).disease.to_dict())\n",
      "186/163:\n",
      "tabla['ndz'] = tabla['ndz'].fillna(0).astype(int)\n",
      "tabla['lc_max'] = tabla['lc_max'].fillna(0).astype(int)\n",
      "tabla['dis_ptm'] = tabla['dis_ptm'].fillna(\"\")\n",
      "186/164:\n",
      "for col in tabla.iloc[:,1].columns:\n",
      "    try:\n",
      "        tabla[col] = tabla[col].map(lambda x: \", \".join(x))\n",
      "    except:\n",
      "        pass\n",
      "186/165: tabla\n",
      "186/166:\n",
      "for col in tabla.iloc[:,1:].columns:\n",
      "    try:\n",
      "        tabla[col] = tabla[col].map(lambda x: \", \".join(x))\n",
      "    except:\n",
      "        pass\n",
      "186/167: tabla\n",
      "186/168: tabla.domain\n",
      "186/169: tabla.domain.map(lambda x: \", \".join(x))\n",
      "186/170: tabla.domain.isna.value_counts()\n",
      "186/171: tabla.domain.isna().value_counts()\n",
      "186/172: tabla.domain.fillna(\"\")\n",
      "186/173: tabla.domain = tabla.domain.fillna(\"\")\n",
      "186/174: tabla.domain\n",
      "186/175: tabla.domain.map(lambda x: \", \".join(x))\n",
      "186/176: tabla.domain = tabla.domain.map(lambda x: \", \".join(x))\n",
      "186/177: tabla\n",
      "186/178: tabla[\"human_boxes\"]\n",
      "186/179: tabla[\"boxes_human\"]\n",
      "186/180: tablas[\"boxes_human\"]\n",
      "186/181: tablas[\"human_boxes\"]\n",
      "186/182: \"Box2\".replace('Box','')\n",
      "186/183: tablas[\"human_boxes\"].groupby('uniprot').agg(lambda x: \", \".join(x.replace('box')))\n",
      "186/184: tablas[\"human_boxes\"].groupby('uniprot').agg(lambda x: \", \".join(x.replace('box',''))).reset_index()\n",
      "186/185: tablas[\"human_boxes\"].box = tablas['human_boxes'].box.str.replace('box','')\n",
      "186/186: tablas[\"human_boxes\"]\n",
      "186/187: tabla.merge(tablas[\"human_boxes\"].groupby('uniprot').agg(lambda x: \", \".join(x)).reset_index()[[\"uniprot\",'box']])\n",
      "186/188: tabla.merge(tablas[\"human_boxes\"].drop_duplicates([\"uniprot\",'box']).groupby('uniprot').agg(lambda x: \", \".join(x)).reset_index()[[\"uniprot\",'box']])\n",
      "186/189: tabla = tabla.merge(tablas[\"human_boxes\"].drop_duplicates([\"uniprot\",'box']).groupby('uniprot').agg(lambda x: \", \".join(x)).reset_index()[[\"uniprot\",'box']])\n",
      "186/190: tabla\n",
      "186/191: tabla.sort_values('box')\n",
      "186/192: tabla = tabla.merge(tablas[\"human_boxes\"].drop_duplicates([\"uniprot\",'box']).sort_values(\"box\").groupby('uniprot').agg(lambda x: \", \".join(x)).reset_index()[[\"uniprot\",'box']])\n",
      "186/193: tabla.sort_values('box')\n",
      "186/194: tabla.sort_values('box')\n",
      "186/195: tabla.sort_values('box').box.value_counts()\n",
      "186/196: tabla\n",
      "186/197: tabla['prot'] = tabla.uniprot.map(tablas['gene_names_dict'])\n",
      "186/198: tabla\n",
      "186/199: tabla.to_csv('tabla_final.csv')\n",
      "186/200: tablas['db']\n",
      "186/201: tablas['db'].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "186/202: tablas['human_db'].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "186/203: tablas['db_human'].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "186/204: tablas['human'].drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "186/205: tablas.keys()\n",
      "186/206: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "186/207: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "186/208: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).merge(tablas['human_boxes'])\n",
      "186/209: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).merge(tablas['human_boxes']).pivot_table(index='mlo_loc', columns='box', aggfunc='size')\n",
      "186/210: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).merge(tablas['human_boxes']).pivot_table(index='mlo_loc', columns='box', aggfunc='size').fillna(0)\n",
      "186/211: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).merge(tablas['human_boxes']).pivot_table(index='mlo_loc', columns='box', aggfunc='size').fillna(0).astype(int)\n",
      "186/212: counts = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).merge(tablas['human_boxes']).pivot_table(index='mlo_loc', columns='box', aggfunc='size').fillna(0).astype(int)\n",
      "186/213: counts\n",
      "186/214: counts.index\n",
      "186/215: counts.index = counts.index.map(tablas['translate'])\n",
      "186/216: counts\n",
      "186/217: counts.sort_values(1,ascending=False)\n",
      "186/218: counts.sort_values(\"1\",ascending=False)\n",
      "186/219: counts.sort_values(\"1\",ascending=False).index.tolist()\n",
      "186/220:\n",
      "mlo_order = ['Nucleolus',\n",
      "\n",
      " 'Nuclear Speckles',\n",
      " 'Stress Granule',\n",
      " 'P-body',\n",
      " 'Paraspeckles',\n",
      " 'PML body',\n",
      " 'Cajal Body',\n",
      " 'NPC',\n",
      " 'PSD',\n",
      " 'Other',]\n",
      "186/221: counts[mlo_order]\n",
      "186/222: counts.loc[mlo_order]\n",
      "186/223: counts = counts.loc[mlo_order]\n",
      "186/224: counts = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc', 'box']).merge(tablas['human_boxes']).pivot_table(index='mlo_loc', columns='box', aggfunc='size').fillna(0).astype(int)\n",
      "186/225: counts = tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes']).drop_duplicates([\"uniprot\", 'mlo_loc', 'box']).pivot_table(index='mlo_loc', columns='box', aggfunc='size').fillna(0).astype(int)\n",
      "186/226: counts.index = counts.index.map(tablas['translate'])\n",
      "186/227: counts.sort_values(\"1\",ascending=False).index.tolist()\n",
      "186/228:\n",
      "mlo_order = ['Nucleolus',\n",
      "\n",
      " 'Nuclear Speckles',\n",
      " 'Stress Granule',\n",
      " 'P-body',\n",
      " 'Paraspeckles',\n",
      " 'PML body',\n",
      " 'Cajal Body',\n",
      " 'NPC',\n",
      " 'PSD',\n",
      " 'Other',]\n",
      "186/229: counts = counts.loc[mlo_order]\n",
      "186/230: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc'])\n",
      "186/231: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).mlo_loc.value_counts()\n",
      "186/232: total = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).mlo_loc.value_counts()\n",
      "186/233: total.index = total.index.map(tablas['translate'])\n",
      "186/234: total\n",
      "186/235: counts['total'] = total\n",
      "186/236: counts\n",
      "186/237: counts = tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes']).drop_duplicates([\"uniprot\", 'mlo_loc', 'box']).pivot_table(index='mlo_loc', columns='box', aggfunc='size').fillna(0).astype(int)\n",
      "186/238: counts.index = counts.index.map(tablas['translate'])\n",
      "186/239: counts.sort_values(\"1\",ascending=False).index.tolist()\n",
      "186/240:\n",
      "mlo_order = ['Nucleolus',\n",
      "\n",
      " 'Nuclear Speckles',\n",
      " 'Stress Granule',\n",
      " 'P-body',\n",
      " 'Paraspeckles',\n",
      " 'PML body',\n",
      " 'Cajal Body',\n",
      " 'NPC',\n",
      " 'PSD',\n",
      " 'Other',]\n",
      "186/241: counts = counts.loc[mlo_order]\n",
      "186/242: total = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).mlo_loc.value_counts()\n",
      "186/243: total.index = total.index.map(tablas['translate'])\n",
      "186/244: counts['total'] = total\n",
      "186/245: counts\n",
      "186/246: counts = tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes']).drop_duplicates([\"uniprot\", 'mlo_loc', 'box']).pivot_table(index='mlo_loc', columns='box', aggfunc='size').fillna(0).astype(int)\n",
      "186/247: counts.index = counts.index.map(tablas['translate'])\n",
      "186/248: counts.sort_values(\"1\",ascending=False).index.tolist()\n",
      "186/249:\n",
      "mlo_order = ['Nucleolus',\n",
      "\n",
      " 'Nuclear Speckles',\n",
      " 'Stress Granule',\n",
      " 'P-body',\n",
      " 'Paraspeckles',\n",
      " 'PML body',\n",
      " 'Cajal Body',\n",
      " 'NPC',\n",
      " 'PSD',\n",
      " 'Other',]\n",
      "186/250: counts = counts.loc[mlo_order]\n",
      "186/251: total = tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).mlo_loc.value_counts()\n",
      "186/252: total.index = total.index.map(tablas['translate'])\n",
      "186/253: counts.sum(1)\n",
      "186/254: total\n",
      "186/255: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc']).mlo_loc.value_counts()\n",
      "186/256: tablas['db'].query('org == \"Homo sapiens\"').drop_duplicates([\"uniprot\", 'mlo_loc','box']).mlo_loc.value_counts()\n",
      "186/257: tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes']).drop_duplicates([\"uniprot\", 'mlo_loc', 'box'])\n",
      "186/258: tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes']).drop_duplicates([\"uniprot\", 'mlo_loc', 'box']).mlo_loc.value_counts()\n",
      "186/259: tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes']).drop_duplicates([\"uniprot\", 'mlo_loc']).mlo_loc.value_counts()\n",
      "186/260: counts = tablas['db'].query('org == \"Homo sapiens\"').merge(tablas['human_boxes'], how='left').drop_duplicates([\"uniprot\", 'mlo_loc', 'box']).pivot_table(index='mlo_loc', columns='box', aggfunc='size').fillna(0).astype(int)\n",
      "186/261: counts.index = counts.index.map(tablas['translate'])\n",
      "186/262: counts.sort_values(\"1\",ascending=False).index.tolist()\n",
      "186/263:\n",
      "mlo_order = ['Nucleolus',\n",
      "\n",
      " 'Nuclear Speckles',\n",
      " 'Stress Granule',\n",
      " 'P-body',\n",
      " 'Paraspeckles',\n",
      " 'PML body',\n",
      " 'Cajal Body',\n",
      " 'NPC',\n",
      " 'PSD',\n",
      " 'Other',]\n",
      "186/264: counts = counts.loc[mlo_order]\n",
      "186/265: counts\n",
      "195/1: import pandas as pd\n",
      "195/2: !ls\n",
      "195/3: pd.read_csv('phasepro.tab')\n",
      "195/4: data = pd.read_csv('phasepro.tab', sep='\\t')\n",
      "195/5: data\n",
      "195/6: data.iloc[0]\n",
      "195/7: data.iloc[1]\n",
      "195/8: data.iloc[2]\n",
      "195/9: data.iloc[3]\n",
      "195/10: data.loc[3]\n",
      "195/11: data.loc[3,'determinants']\n",
      "195/12: data.loc[3,'molecular_interaction_to_LLPS']\n",
      "195/13: data['mol'] = data[\"molecular_interaction_to_LLPS\"].str.split(\"; \")\n",
      "195/14: data.mol\n",
      "195/15: data.explode('mol')\n",
      "195/16: df = data.explode('mol')\n",
      "195/17: df\n",
      "195/18: df.mlo\n",
      "195/19: df.mol\n",
      "195/20: df.mol.str.rsplit(\" \",1).str[0]\n",
      "195/21: df.mol = df.mol.str.rsplit(\" \",1).str[0]\n",
      "195/22: df.mol.value_counts()\n",
      "195/23: df.query('mol != \"Not\"').mol.value_counts()\n",
      "195/24: df.query('mol != \"Not\"').mol.value_counts().plot.bar()\n",
      "195/25: conteo = df.query('mol != \"Not\"').mol.value_counts()\n",
      "195/26: conteo[conteo > 1]\n",
      "195/27: df = data.explode('mol')\n",
      "195/28: df.mol = df.mol.str.rsplit(\" (\",1).str[0]\n",
      "195/29: conteo = df.query('mol != \"Not\"').mol.value_counts()\n",
      "195/30: conteo[conteo > 1]\n",
      "195/31: conteo\n",
      "195/32: conteo\n",
      "195/33: conteo.plot.bar()\n",
      "195/34: conteo.plot.hbar()\n",
      "195/35: conteo.plot.bar(orientation='h')\n",
      "195/36: conteo.plot.barh()\n",
      "195/37: conteo\n",
      "195/38: conteo.sort_values().plot.barh()\n",
      "195/39: data\n",
      "195/40: data.organism.value_counts().plot.barh()\n",
      "195/41: data.organism.value_counts().sort_values().plot.barh()\n",
      "195/42: conteo_organism = data.organism.value_counts()#.sort_values().plot.barh()\n",
      "195/43: conteo_organism\n",
      "195/44: conteo_organism[\"Other\"] = conteo_organism[conteo.organism > 4].sum()\n",
      "195/45: conteo_organism[\"Other\"] = conteo_organism[conteo_organism > 4].sum()\n",
      "195/46: conteo_organism\n",
      "195/47: conteo_organism[conteo_organism > 4]\n",
      "195/48:\n",
      "conteo_organism = conteo_organism[conteo_organism > 4]\n",
      "conteo_organism.plot.barh()\n",
      "195/49:\n",
      "conteo_organism = conteo_organism[conteo_organism > 4]\n",
      "conteo_organism.sort_values().plot.barh()\n",
      "196/1: import seaborn as sns\n",
      "196/2: sns.color_palette(\"tab10\")\n",
      "196/3: sns.color_palette(\"tab10\").as_hex()\n",
      "196/4: sns.color_palette(\"paired\").as_hex()\n",
      "196/5: sns.color_palette(\"Paired\").as_hex()\n",
      "197/1: import pandas as pd\n",
      "197/2: df = pd.read_html('https://www.ebi.ac.uk/Tools/hmmer/results/D7826D00-3EDF-11EB-9540-589153F04F9B/score')\n",
      "197/3: df\n",
      "197/4: df[0]\n",
      "197/5: df[0].head()\n",
      "197/6: df[0].head().columns\n",
      "197/7: df.Identifier\n",
      "197/8: df[0].Identifier\n",
      "197/9: df[0][\"Identifier\"]\n",
      "203/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "203/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import pickle\n",
      "import matplotlib as mpl\n",
      "\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "203/3: tablas\n",
      "203/4: tablas.keys()\n",
      "203/5: *tablas.keys(),\n",
      "203/6: tablas['human_dominios_box1']\n",
      "203/7: tablas['human_dominios_box1'].to_csv('~/mlo_box1_pfam.csv',index=False)\n",
      "203/8: tablas['human_dominios_box1'].queyr('uniprot == \"Q5HYJ3\")\n",
      "203/9: tablas['human_dominios_box1'].queyr('uniprot == \"Q5HYJ3\"')\n",
      "203/10: tablas['human_dominios_box1'].query('uniprot == \"Q5HYJ3\"')\n",
      "203/11: dfs = pd.read_html('https://www.ebi.ac.uk/Tools/hmmer/results/D7826D00-3EDF-11EB-9540-589153F04F9B/score')\n",
      "203/12: df[0]\n",
      "203/13: dfs[0]\n",
      "203/14: dfs[0].loc[10]\n",
      "203/15: dfs[0].loc[0]\n",
      "203/16: dfs[0]#.loc[0]\n",
      "203/17: len(dfs)\n",
      "203/18: dos = pd.read_html('https://www.ebi.ac.uk/Tools/hmmer/results/D7826D00-3EDF-11EB-9540-589153F04F9B.2/score')\n",
      "203/19: dos\n",
      "203/20: pd.read_csv(\"https://www.ebi.ac.uk/Tools/hmmer/download/D7826D00-3EDF-11EB-9540-589153F04F9B.{}/score?format=tsv\")\n",
      "203/21: dfs[0]#.loc[0]\n",
      "203/22: dfs[0].columns = ['a','b','c','d','e','f','g','h']#.loc[0]\n",
      "203/23: dfs[0]\n",
      "203/24: dos = pd.read_html('https://www.ebi.ac.uk/Tools/hmmer/results/D7826D00-3EDF-11EB-9540-589153F04F9B.6/score')\n",
      "203/25: dos\n",
      "203/26: dos[0]\n",
      "203/27:\n",
      "import requests\n",
      "\n",
      "url = 'https://www.ebi.ac.uk/Tools/hmmer/download/D7826D00-3EDF-11EB-9540-589153F04F9B.6/score?format=tsv'\n",
      "response = requests.get(url)\n",
      "203/28: response\n",
      "203/29: pd.read_csv('https://www.ebi.ac.uk/Tools/hmmer/download/D7826D00-3EDF-11EB-9540-589153F04F9B.6/score?format=tsv')\n",
      "203/30: pd.read_csv('https://www.ebi.ac.uk/Tools/hmmer/download/D7826D00-3EDF-11EB-9540-589153F04F9B.6/score?format=tsv', sep='\\t')\n",
      "203/31:\n",
      "lista = []\n",
      "for i in range(1,10):\n",
      "    lista.append(pd.read_csv(f'https://www.ebi.ac.uk/Tools/hmmer/download/D7826D00-3EDF-11EB-9540-589153F04F9B.{i}/score?format=tsv', sep='\\t'))\n",
      "203/32: pd.concat(lista)\n",
      "203/33:\n",
      "lista = []\n",
      "for i in range(1,250):\n",
      "    lista.append(pd.read_csv(f'https://www.ebi.ac.uk/Tools/hmmer/download/D7826D00-3EDF-11EB-9540-589153F04F9B.{i}/score?format=tsv', sep='\\t'))\n",
      "203/34:\n",
      "a = pd.read_csv(\"/home/fernando/Untitled.csv\", sep='    ')\n",
      "a\n",
      "206/1: import pandas as pd\n",
      "207/1: import pandas as pd\n",
      "207/2: a = pd.read_csv(\"phasepro\", sep='\\t')\n",
      "207/3: a\n",
      "207/4: a.columns\n",
      "212/1: import pandas as pd\n",
      "212/2: !pwd\n",
      "212/3: !ls ..\n",
      "212/4: entrada = pd.read_csv(\"../data/phasepro_latest_db.tsv\")\n",
      "212/5: entrada = pd.read_csv(\"../data/phasepro_latest_db.tsv\", sep='\\t')\n",
      "212/6: entrada\n",
      "212/7: entrada = pd.read_csv(\"../data/phasepro_latest_db.tsv\", sep='\\t', header=None)\n",
      "212/8: entrada\n",
      "212/9:\n",
      "import urllib.request, json\n",
      "with urllib.request.urlopen(\"https://phasepro.elte.hu/download_full.json\") as url:\n",
      "    # Variable 'data' will contain the full database as a nested dictionary\n",
      "    data = json.loads(url.read().decode())\n",
      "    # Access the data of protein FUS\n",
      "    print(data['P35637'])\n",
      "212/10: print(data['P35637'])\n",
      "212/11: print(data['P35637'].keys())\n",
      "212/12: print(*data['P35637'].keys())\n",
      "212/13:\n",
      "for i in data['P35637'].keys():\n",
      "    print(i)\n",
      "212/14: db.head()\n",
      "212/15: entrada.head()\n",
      "212/16: pd.DataFrame.from_dict(data)\n",
      "212/17: pd.DataFrame.from_dict(data, orient='index')\n",
      "212/18: entrada = pd.DataFrame.from_dict(data, orient='index').rename_axis(\"uniprot\").reset_index()\n",
      "212/19: entrada.head()\n",
      "212/20: entrada#.head()\n",
      "212/21: entrada.iloc[0]\n",
      "212/22:\n",
      "for i in entrada.loc[0]:\n",
      "    print(i)\n",
      "212/23: entrada.set_index('gene')['FUS']\n",
      "212/24: entrada.set_index('gene')#['FUS']\n",
      "212/25: entrada.set_index('gene')['DDX4']\n",
      "212/26: entrada.set_index('gene').loc['FUS']\n",
      "212/27: entrada.set_index('gene').loc['LAF-1']\n",
      "212/28: entrada.set_index('gene').loc['LAF-1'].items()\n",
      "212/29: for i,e in entrada.set_index('gene').loc['LAF-1'].items():print(i,e)\n",
      "212/30:\n",
      "for gen, row in entrada.set_index('gene').iterrows():\n",
      "    print(gen, row.boundaries, row.segment)\n",
      "213/1: import urllib.request, json\n",
      "213/2:\n",
      "with urllib.request.urlopen(\"https://lbgi.fr/orthoinspectorv3/api/eukaryota/protein/P35637/orthologs\") as url:\n",
      "    # Variable 'data' will contain the full database as a nested dictionary\n",
      "    data = json.loads(url.read().decode())\n",
      "213/3:\n",
      "with urllib.request.urlopen(\"https://lbgi.fr/orthoinspectorv3/api/eukaryota/protein/P35637/orthologs\") as url:\n",
      "    # Variable 'data' will contain the full database as a nested dictionary\n",
      "    #data = json.loads(url.read().decode())\n",
      "    print(url.read())\n",
      "213/4: response = requests.get('https://lbgi.fr/orthoinspectorv3/api/eukaryota/protein/P35637/orthologs')\n",
      "213/5: import requests\n",
      "213/6: response = requests.get('https://lbgi.fr/orthoinspectorv3/api/eukaryota/protein/P35637/orthologs')\n",
      "213/7: response.json()\n",
      "213/8: response#.json()\n",
      "213/9: response.read()#.json()\n",
      "213/10: print(response)\n",
      "213/11: dir(response)\n",
      "213/12:\n",
      "for i in dir(response):\n",
      "    if not i.startswith(\"_\"):\n",
      "        print(i)\n",
      "213/13: response.text\n",
      "213/14:\n",
      "import pandas as pd\n",
      "pd.read_html(response.text)\n",
      "213/15:\n",
      "import pandas as pd\n",
      "dfs[0] = pd.read_html(response.text)\n",
      "213/16:\n",
      "import pandas as pd\n",
      "dfs = pd.read_html(response.text)\n",
      "213/17: dfs\n",
      "213/18: response = requests.get('https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/P35637/orthologs')\n",
      "213/19: print(response)\n",
      "213/20: response.json()\n",
      "213/21: response2 = requests.get('https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/P35637')\n",
      "213/22: response2.json()\n",
      "213/23: import urllib.request, json\n",
      "213/24: import requests\n",
      "213/25:\n",
      "with urllib.request.urlopen(\"https://phasepro.elte.hu/download_full.json\") as url:\n",
      "    # Variable 'data' will contain the full database as a nested dictionary\n",
      "    data = json.loads(url.read().decode())\n",
      "213/26: entrada = pd.DataFrame.from_dict(data, orient='index').rename_axis(\"uniprot\").reset_index()\n",
      "213/27: entrada\n",
      "213/28: entrada.to_csv('phasepro_latest_db.csv', index=False)\n",
      "213/29: entrada\n",
      "213/30:\n",
      "jsons = []\n",
      "for uniprot in entrada.uniprot.unique()[:5]:\n",
      "    json[uniprot] = requests.get('https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/P35637/orthologs').json()\n",
      "    time.sleep(1)\n",
      "213/31:\n",
      "jsons = {}\n",
      "for uniprot in entrada.uniprot.unique()[:5]:\n",
      "    json[uniprot] = requests.get('https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/P35637/orthologs').json()\n",
      "    time.sleep(1)\n",
      "213/32: requests.get('https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/P35637/orthologs').json()\n",
      "213/33: json[\"a\"]requests.get('https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/P35637/orthologs').json()\n",
      "213/34: json[\"a\"] = requests.get('https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/P35637/orthologs').json()\n",
      "213/35:\n",
      "jsons = {}\n",
      "for uniprot in entrada.uniprot.unique()[:5]:\n",
      "    jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "    time.sleep(1)\n",
      "213/36:\n",
      "import time\n",
      "import requests\n",
      "213/37:\n",
      "jsons = {}\n",
      "for uniprot in entrada.uniprot.unique()[:5]:\n",
      "    jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "    time.sleep(1)\n",
      "213/38: jsons\n",
      "213/39:\n",
      "jsons = {}\n",
      "f = []\n",
      "for uniprot in entrada.uniprot.unique()[:5]:\n",
      "    try:\n",
      "        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "        time.sleep(1)\n",
      "    except:\n",
      "        f.append(uniprot)\n",
      "        print(uniprot, len(f))\n",
      "213/40: jsons\n",
      "213/41: len(f)\n",
      "213/42:\n",
      "jsons = {}\n",
      "f = []\n",
      "for uniprot in entrada.uniprot.unique():\n",
      "    try:\n",
      "        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "        time.sleep(1)\n",
      "    except:\n",
      "        f.append(uniprot)\n",
      "        print(uniprot, len(f))\n",
      "213/43: len(f)\n",
      "213/44: len(jsons)\n",
      "213/45: jsons[0]\n",
      "213/46: jsons['P35637']\n",
      "213/47: pd.DataFrame(jsons['P35637'])\n",
      "213/48:\n",
      "dfs = []\n",
      "for k,v in in jsons.items():\n",
      "    df = pd.DataFrame(v)\n",
      "    df['uniprot'] = k\n",
      "    dfs.append(df)\n",
      "pd.concat(dfs).head()\n",
      "213/49:\n",
      "dfs = []\n",
      "for k,v in jsons.items():\n",
      "    df = pd.DataFrame(v)\n",
      "    df['uniprot'] = k\n",
      "    dfs.append(df)\n",
      "pd.concat(dfs).head()\n",
      "213/50:\n",
      "dfs = []\n",
      "for k,v in jsons.items():\n",
      "    print(k)\n",
      "    df = pd.DataFrame(v)\n",
      "    df['uniprot'] = k\n",
      "    dfs.append(df)\n",
      "pd.concat(dfs).head()\n",
      "213/51: jsons[\"P03521\"]\n",
      "213/52: 'P03521'\n",
      "213/53: [i for i,v in jsons.items() if len(v) == 2]\n",
      "213/54: entrada.uniprot = entrada.uniprot.str.split(\"-\").str[0]\n",
      "213/55: entrada\n",
      "213/56:\n",
      "jsons = {}\n",
      "f = []\n",
      "for uniprot in entrada.uniprot.unique():\n",
      "    try:\n",
      "        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "        time.sleep(1)\n",
      "    except:\n",
      "        f.append(uniprot)\n",
      "        print(uniprot, len(f))\n",
      "213/57: [i for i,v in jsons.items() if len(v) == 2]\n",
      "213/58:\n",
      "dfs = []\n",
      "for k,v in jsons.items():\n",
      "    try:\n",
      "        df = pd.DataFrame(v)\n",
      "        df['uniprot'] = k\n",
      "        dfs.append(df)\n",
      "    except ValueError:\n",
      "pd.concat(dfs).head()\n",
      "213/59:\n",
      "dfs = []\n",
      "for k,v in jsons.items():\n",
      "    try:\n",
      "        df = pd.DataFrame(v)\n",
      "        df['uniprot'] = k\n",
      "        dfs.append(df)\n",
      "    except ValueError:\n",
      "        pass\n",
      "pd.concat(dfs).head()\n",
      "213/60: df = pd.concat(dfs)\n",
      "213/61: df\n",
      "213/62: df.iloc[0,3]\n",
      "213/63: df.iloc[0,3][0]\n",
      "213/64: ortologos = df.explode('orthologs')\n",
      "213/65: ortologos\n",
      "213/66: ortologos.uniprot.value_counts()\n",
      "213/67: ortologos.uniprot.value_counts()[:40]\n",
      "213/68: ortologos.query('type == \"One-to-One\"').uniprot.value_counts()[:40]\n",
      "213/69: ortologos\n",
      "213/70: ortologos.to_csv('../data/ortologos.csv', index=False)\n",
      "215/1:\n",
      "from Bio import SeqIO\n",
      "from Bio.Align.Applications import MuscleCommandline\n",
      "216/1: import pandas as pd\n",
      "216/2:\n",
      "jsons = {}\n",
      "f = []\n",
      "for uniprot in entrada.uniprot.unique():\n",
      "    try:\n",
      "        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "        time.sleep(1)\n",
      "    except:\n",
      "        f.append(uniprot)\n",
      "        print(uniprot, len(f))\n",
      "216/3: entrada = pd.to_csv('phasepro_latest_db.csv')\n",
      "216/4: import pandas as pd\n",
      "216/5: entrada = pd.read_csv('phasepro_latest_db.csv')\n",
      "216/6: entrada.uniprot = entrada.uniprot.str.split(\"-\").str[0]\n",
      "216/7:\n",
      "jsons = {}\n",
      "f = []\n",
      "for uniprot in entrada.uniprot.unique():\n",
      "    try:\n",
      "        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "        time.sleep(1)\n",
      "    except:\n",
      "        f.append(uniprot)\n",
      "        print(uniprot, len(f))\n",
      "216/8:\n",
      "jsons = {}\n",
      "f = []\n",
      "for uniprot in entrada.uniprot.unique():\n",
      "    try:\n",
      "        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "        time.sleep(1)\n",
      "    except:\n",
      "        f.append(uniprot)\n",
      "        print(uniprot, len(f))\n",
      "216/9: requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "216/10: import urllib.request, json\n",
      "216/11:\n",
      "import time\n",
      "import requests\n",
      "216/12:\n",
      "jsons = {}\n",
      "f = []\n",
      "for uniprot in entrada.uniprot.unique():\n",
      "    try:\n",
      "        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "        time.sleep(1)\n",
      "    except:\n",
      "        f.append(uniprot)\n",
      "        print(uniprot, len(f))\n",
      "217/1:\n",
      "import urllib.parse\n",
      "\n",
      "import urllib.request\n",
      "import pandas as pd\n",
      "from io import StringIO\n",
      "from Bio import SeqIO\n",
      "217/2:\n",
      "def batch_uniprot(uniprots,  format='tab', columns=[],):\n",
      "\n",
      "    import urllib.parse\n",
      "    import urllib.request\n",
      "    import pandas as pd\n",
      "    from io import StringIO\n",
      "    from Bio import SeqIO\n",
      "\n",
      "\n",
      "    if isinstance(uniprots, str):\n",
      "        uniprots = [uniprots]\n",
      "    url = 'https://www.uniprot.org/uploadlists/'\n",
      "\n",
      "    if len(columns) == 0:\n",
      "        columns = ['id', 'genes', 'genes(PREFERRED)', 'genes(ALTERNATIVE)', 'organism-id', 'protein names', 'length',\n",
      "                   'database(GeneID)']\n",
      "    string_columns = \",\".join(columns)\n",
      "\n",
      "    query_uniprots = \" \".join(uniprots)\n",
      "    params = {\n",
      "        'from': 'ACC+ID',\n",
      "        'to': 'ACC',\n",
      "        'format': format,\n",
      "        'columns': string_columns,\n",
      "        'query': query_uniprots,\n",
      "    }\n",
      "    try:\n",
      "        data = urllib.parse.urlencode(params)\n",
      "        data = data.encode('utf-8')\n",
      "        req = urllib.request.Request(url, data)\n",
      "        with urllib.request.urlopen(req) as f:\n",
      "\n",
      "            response = f.read()\n",
      "        if format == 'tab':\n",
      "            df = pd.read_csv(\n",
      "                StringIO(response.decode('utf-8'))\n",
      "                , sep='\\t').iloc[:, :-1]\n",
      "            df.columns = [s.replace('(', '_').replace(')', '_').lower() for s in columns]\n",
      "            return df\n",
      "        elif format == 'fasta':\n",
      "            secuencias =  list(SeqIO.parse(StringIO(response.decode('utf-8')), format = 'fasta'))\n",
      "            for sec in secuencias:\n",
      "                sec.id = sec.id.split('|')[1]\n",
      "            return secuencias\n",
      "    except pd.errors.EmptyDataError:\n",
      "        print('No se encontraron registros. Revisar codigos uniprot')\n",
      "217/3: p = batch_uniprot(['P03521', 'P35972', 'Q5BI67'])\n",
      "217/4: p\n",
      "217/5: import pandas as pd\n",
      "217/6: _ = pd.read_html('https://www.uniprot.org/help/uniprotkb_column_names')\n",
      "217/7: _\n",
      "217/8: columns = pd.concat(_)\n",
      "217/9: columns\n",
      "217/10: columns[:10]\n",
      "217/11: columns[:20]\n",
      "217/12: uniprot_id = pd.concat(_)\n",
      "217/13: *uniprot_id.iterrows()\n",
      "217/14: *uniprot_id.iterrows(),\n",
      "217/15: uniprot_id.iterrows()\n",
      "217/16: for i, row in uniprot_id.iterrows().iterrows():print(i)\n",
      "217/17: for i, row in uniprot_id.iterrows():print(i)\n",
      "217/18: for i, row in uniprot_id.iterrows():print(row)\n",
      "217/19: for i, row in uniprot_id.iterrows():print(row.ilo[0],row.ilo[1])\n",
      "217/20: for i, row in uniprot_id.iterrows():print(row.iloc[0],row.iloc[1])\n",
      "217/21: len(uniprot_id)\n",
      "217/22: _.columns = ['name','code']\n",
      "217/23: _ = pd.read_html('https://www.uniprot.org/help/uniprotkb_column_names')\n",
      "217/24:\n",
      "uniprot_id = pd.concat(_)\n",
      "uniprot_id.columns = ['name','code']\n",
      "217/25: p = batch_uniprot(['P03521', 'P35972', 'Q5BI67'],uniprot_id.code.to_list())\n",
      "217/26: p = batch_uniprot(['P03521', 'P35972', 'Q5BI67'],uniprot_id.code.to_list())\n",
      "217/27: p = batch_uniprot(['P03521', 'P35972', 'Q5BI67'], columns = uniprot_id.code.to_list())\n",
      "217/28:\n",
      "def batch_uniprot(uniprots,  format='tab', columns=[],):\n",
      "\n",
      "    import urllib.parse\n",
      "    import urllib.request\n",
      "    import pandas as pd\n",
      "    from io import StringIO\n",
      "    from Bio import SeqIO\n",
      "\n",
      "\n",
      "    if isinstance(uniprots, str):\n",
      "        uniprots = [uniprots]\n",
      "    url = 'https://www.uniprot.org/uploadlists/'\n",
      "\n",
      "    if len(columns) == 0:\n",
      "        columns = ['id', 'genes', 'genes(PREFERRED)', 'genes(ALTERNATIVE)', 'organism-id', 'protein names', 'length',\n",
      "                   'database(GeneID)']\n",
      "    string_columns = \",\".join(columns)\n",
      "\n",
      "    query_uniprots = \" \".join(uniprots)\n",
      "    params = {\n",
      "        'from': 'ACC+ID',\n",
      "        'to': 'ACC',\n",
      "        'format': format,\n",
      "        'columns': string_columns,\n",
      "        'query': query_uniprots,\n",
      "    }\n",
      "    try:\n",
      "        data = urllib.parse.urlencode(params)\n",
      "        data = data.encode('utf-8')\n",
      "        req = urllib.request.Request(url, data)\n",
      "        with urllib.request.urlopen(req) as f:\n",
      "\n",
      "            response = f.read()\n",
      "        if format == 'tab':\n",
      "            df = pd.read_csv(\n",
      "                StringIO(response.decode('utf-8'))\n",
      "                , sep='\\t').iloc[:, :-1]\n",
      "            #df.columns = [s.replace('(', '_').replace(')', '_').lower() for s in columns]\n",
      "            return df\n",
      "        elif format == 'fasta':\n",
      "            secuencias =  list(SeqIO.parse(StringIO(response.decode('utf-8')), format = 'fasta'))\n",
      "            for sec in secuencias:\n",
      "                sec.id = sec.id.split('|')[1]\n",
      "            return secuencias\n",
      "    except pd.errors.EmptyDataError:\n",
      "        print('No se encontraron registros. Revisar codigos uniprot')\n",
      "217/29: p = batch_uniprot(['P03521', 'P35972', 'Q5BI67'], columns = uniprot_id.code.to_list())\n",
      "217/30: p\n",
      "217/31: p.loc[0]\n",
      "217/32: p.loc[0][:20]\n",
      "217/33: p.loc[0][:30]\n",
      "217/34: p.loc[0][:50]\n",
      "217/35: p.loc[0][:100]\n",
      "217/36: for i in p.loc[0]: print(i)\n",
      "217/37: import xmltodict\n",
      "217/38: r = requests.get(\"https://www.uniprot.org/uniprot/P03521.xml\")\n",
      "217/39: import requests\n",
      "217/40: r = requests.get(\"https://www.uniprot.org/uniprot/P03521.xml\")\n",
      "217/41: xmltodict.parse(r.content)\n",
      "217/42: p = xmltodict.parse(r.content)\n",
      "217/43: p\n",
      "217/44: pd.DataFrame.from_dict(p)\n",
      "217/45: pd.DataFrame.from_dict(p[\"entry\"])\n",
      "217/46: p.keys()\n",
      "217/47: p[\"uniprot\"].keys()\n",
      "217/48: pd.DataFrame.from_dict(p[\"uniprot\"][\"entry\"])\n",
      "216/13: [i for i,v in jsons.items() if len(v) == 2]\n",
      "216/14:\n",
      "dfs = []\n",
      "for k,v in jsons.items():\n",
      "    try:\n",
      "        df = pd.DataFrame(v)\n",
      "        df['uniprot'] = k\n",
      "        dfs.append(df)\n",
      "    except ValueError:\n",
      "        pass\n",
      "216/15: df = pd.concat(dfs)\n",
      "216/16: ortologos = df.explode('orthologs')\n",
      "216/17: ortologos\n",
      "216/18: ortologos.to_csv('../data/ortologos.csv', index=False)\n",
      "216/19: ortologos\n",
      "216/20: ortologos.uniprot.unique().to_list()\n",
      "216/21: ortologos.uniprot.unique().tolist()\n",
      "216/22: entrada\n",
      "216/23: entrada.uniprot.unique().tolist()\n",
      "216/24: uniprots_phasepro = entrada.uniprot.unique().tolist()\n",
      "216/25: from uniprot import batch_uniprot\n",
      "217/49: uniprot_id.code.to_list()\n",
      "216/26:\n",
      "columns = ['id',\n",
      " 'entry name',\n",
      " 'genes',\n",
      " 'genes(PREFERRED)',\n",
      " 'genes(ALTERNATIVE)',\n",
      " 'genes(OLN)',\n",
      " 'genes(ORF)',\n",
      " 'organism',\n",
      " 'organism-id',\n",
      " 'protein names',\n",
      " 'proteome',\n",
      " 'lineage(ALL)',\n",
      " 'virus hosts',\n",
      " 'fragment',\n",
      " 'encodedon',\n",
      " 'comment(ALTERNATIVE PRODUCTS)',\n",
      " 'comment(ERRONEOUS GENE MODEL PREDICTION)',\n",
      " 'comment(ERRONEOUS INITIATION)',\n",
      " 'comment(ERRONEOUS TERMINATION)',\n",
      " 'comment(ERRONEOUS TRANSLATION)',\n",
      " 'comment(FRAMESHIFT)',\n",
      " 'comment(MASS SPECTROMETRY)',\n",
      " 'comment(POLYMORPHISM)',\n",
      " 'comment(RNA EDITING)',\n",
      " 'comment(SEQUENCE CAUTION)',\n",
      " 'length',\n",
      " 'mass',\n",
      " 'sequence',\n",
      " 'feature(ALTERNATIVE SEQUENCE)',\n",
      " 'feature(NATURAL VARIANT)',\n",
      " 'feature(NON ADJACENT RESIDUES)',\n",
      " 'feature(NON STANDARD RESIDUE)',\n",
      " 'feature(NON TERMINAL RESIDUE)',\n",
      " 'feature(SEQUENCE CONFLICT)',\n",
      " 'feature(SEQUENCE UNCERTAINTY)',\n",
      " 'version(sequence)',\n",
      " 'comment(ABSORPTION)',\n",
      " 'feature(ACTIVE SITE)',\n",
      " 'feature(BINDING SITE)',\n",
      " 'comment(CATALYTIC ACTIVITY)',\n",
      " 'chebi',\n",
      " 'chebi(Catalytic activity)',\n",
      " 'chebi(Cofactor)',\n",
      " 'chebi-id',\n",
      " 'comment(COFACTOR)',\n",
      " 'feature(DNA BINDING)',\n",
      " 'ec',\n",
      " 'comment(ENZYME REGULATION)',\n",
      " 'comment(FUNCTION)',\n",
      " 'comment(KINETICS)',\n",
      " 'feature(METAL BINDING)',\n",
      " 'feature(NP BIND)',\n",
      " 'comment(PATHWAY)',\n",
      " 'comment(PH DEPENDENCE)',\n",
      " 'comment(REDOX POTENTIAL)',\n",
      " 'rhea-id',\n",
      " 'feature(SITE)',\n",
      " 'comment(TEMPERATURE DEPENDENCE)',\n",
      " 'annotation score',\n",
      " 'features',\n",
      " 'comment(CAUTION)',\n",
      " 'comment(MISCELLANEOUS)',\n",
      " 'keywords',\n",
      " 'context',\n",
      " 'existence',\n",
      " 'tools',\n",
      " 'reviewed',\n",
      " 'comment(SUBUNIT)',\n",
      " 'interactor',\n",
      " 'comment(DEVELOPMENTAL STAGE)',\n",
      " 'comment(INDUCTION)',\n",
      " 'comment(TISSUE SPECIFICITY)',\n",
      " 'go',\n",
      " 'go(biological process)',\n",
      " 'go(molecular function)',\n",
      " 'go(cellular component)',\n",
      " 'go-id',\n",
      " 'comment(ALLERGEN)',\n",
      " 'comment(BIOTECHNOLOGY)',\n",
      " 'comment(DISRUPTION PHENOTYPE)',\n",
      " 'comment(DISEASE)',\n",
      " 'comment(PHARMACEUTICAL)',\n",
      " 'comment(TOXIC DOSE)',\n",
      " 'comment(SUBCELLULAR LOCATION)',\n",
      " 'feature(INTRAMEMBRANE)',\n",
      " 'feature(TOPOLOGICAL DOMAIN)',\n",
      " 'feature(TRANSMEMBRANE)',\n",
      " 'comment(PTM)',\n",
      " 'feature(CHAIN)',\n",
      " 'feature(CROSS LINK)',\n",
      " 'feature(DISULFIDE BOND)',\n",
      " 'feature(GLYCOSYLATION)',\n",
      " 'feature(INITIATOR METHIONINE)',\n",
      " 'feature(LIPIDATION)',\n",
      " 'feature(MODIFIED RESIDUE)',\n",
      " 'feature(PEPTIDE)',\n",
      " 'feature(PROPEPTIDE)',\n",
      " 'feature(SIGNAL)',\n",
      " 'feature(TRANSIT)',\n",
      " '3d',\n",
      " 'feature(BETA STRAND)',\n",
      " 'feature(HELIX)',\n",
      " 'feature(TURN)',\n",
      " 'citationmapping',\n",
      " 'citation',\n",
      " 'created',\n",
      " 'last-modified',\n",
      " 'sequence-modified',\n",
      " 'version(entry)',\n",
      " 'comment(DOMAIN)',\n",
      " 'comment(SIMILARITY)',\n",
      " 'families',\n",
      " 'feature(COILED COIL)',\n",
      " 'feature(COMPOSITIONAL BIAS)',\n",
      " 'feature(DOMAIN EXTENT)',\n",
      " 'feature(MOTIF)',\n",
      " 'feature(REGION)',\n",
      " 'feature(REPEAT)',\n",
      " 'feature(ZINC FINGER)',\n",
      " 'lineage(all)',\n",
      " 'lineage(SUPERKINGDOM)',\n",
      " 'lineage(KINGDOM)',\n",
      " 'lineage(SUBKINGDOM)',\n",
      " 'lineage(SUPERPHYLUM)',\n",
      " 'lineage(PHYLUM)',\n",
      " 'lineage(SUBPHYLUM)',\n",
      " 'lineage(SUPERCLASS)',\n",
      " 'lineage(CLASS)',\n",
      " 'lineage(SUBCLASS)',\n",
      " 'lineage(INFRACLASS)',\n",
      " 'lineage(SUPERORDER)',\n",
      " 'lineage(ORDER)',\n",
      " 'lineage(SUBORDER)',\n",
      " 'lineage(INFRAORDER)',\n",
      " 'lineage(PARVORDER)',\n",
      " 'lineage(SUPERFAMILY)',\n",
      " 'lineage(FAMILY)',\n",
      " 'lineage(SUBFAMILY)',\n",
      " 'lineage(TRIBE)',\n",
      " 'lineage(SUBTRIBE)',\n",
      " 'lineage(GENUS)',\n",
      " 'lineage(SUBGENUS)',\n",
      " 'lineage(SPECIES GROUP)',\n",
      " 'lineage(SPECIES SUBGROUP)',\n",
      " 'lineage(SPECIES)',\n",
      " 'lineage(SUBSPECIES)',\n",
      " 'lineage(VARIETAS)',\n",
      " 'lineage(FORMA)',\n",
      " 'lineage-id(all)',\n",
      " 'lineage-id(SUPERKINGDOM)',\n",
      " 'lineage-id(KINGDOM)',\n",
      " 'lineage-id(SUBKINGDOM)',\n",
      " 'lineage-id(SUPERPHYLUM)',\n",
      " 'lineage-id(PHYLUM)',\n",
      " 'lineage-id(SUBPHYLUM)',\n",
      " 'lineage-id(SUPERCLASS)',\n",
      " 'lineage-id(CLASS)',\n",
      " 'lineage-id(SUBCLASS)',\n",
      " 'lineage-id(INFRACLASS)',\n",
      " 'lineage-id(SUPERORDER)',\n",
      " 'lineage-id(ORDER)',\n",
      " 'lineage-id(SUBORDER)',\n",
      " 'lineage-id(INFRAORDER)',\n",
      " 'lineage-id(PARVORDER)',\n",
      " 'lineage-id(SUPERFAMILY)',\n",
      " 'lineage-id(FAMILY)',\n",
      " 'lineage-id(SUBFAMILY)',\n",
      " 'lineage-id(TRIBE)',\n",
      " 'lineage-id(SUBTRIBE)',\n",
      " 'lineage-id(GENUS)',\n",
      " 'lineage-id(SUBGENUS)',\n",
      " 'lineage-id(SPECIES GROUP)',\n",
      " 'lineage-id(SPECIES SUBGROUP)',\n",
      " 'lineage-id(SPECIES)',\n",
      " 'lineage-id(SUBSPECIES)',\n",
      " 'lineage-id(VARIETAS)',\n",
      " 'lineage-id(FORMA)',\n",
      " 'database(db_abbrev)',\n",
      " 'database(EMBL)']\n",
      "216/27: uniprots_phasepro_info = batch_uniprot(uniprots_phaspero, columns=columns)\n",
      "216/28: uniprots_phasepro_info = batch_uniprot(uniprots_phasprot, columns=columns)\n",
      "216/29: uniprots_phasepro_info = batch_uniprot(uniprots_phaseprot, columns=columns)\n",
      "216/30: uniprots_phasepro_info = batch_uniprot(uniprots_phasepro , columns=columns)\n",
      "216/31: uniprots_phasepro_info\n",
      "216/32: uniprots_phasepro_info.columns\n",
      "216/33: uniprots_phasepro_info.Sequence\n",
      "216/34: uniprots_phasepro_fasta =  batch_uniprot(uniprots_phasepro , \"fasta\")\n",
      "216/35: uniprots_phasepro_fasta\n",
      "216/36:\n",
      "from Bio import SeqIO\n",
      "with open('sequencias/phasepro.fasta', 'w') as f:\n",
      "    for seq in uniprots_phasepro_fasta:\n",
      "        SeqIO.write(seq, f, 'fasta')\n",
      "216/37:\n",
      "from Bio import SeqIO\n",
      "with open('secuencias/phasepro.fasta', 'w') as f:\n",
      "    for seq in uniprots_phasepro_fasta:\n",
      "        SeqIO.write(seq, f, 'fasta')\n",
      "216/38: !pwd\n",
      "216/39:\n",
      "from Bio import SeqIO\n",
      "with open('../secuencias/phasepro.fasta', 'w') as f:\n",
      "    for seq in uniprots_phasepro_fasta:\n",
      "        SeqIO.write(seq, f, 'fasta')\n",
      "216/40: ortologos\n",
      "216/41:\n",
      "_ = batch_uniprot(ortologos, 'fasta')\n",
      "with open('../tmp/_.fasta', 'w') as f:\n",
      "    for seq in _:\n",
      "        SeqIO.write(seq, f, 'fasta')\n",
      "        \n",
      "muscle_cline = MuscleCommandline(input=\"../tmp/_.fasta\", \n",
      "                                 out=\"../tmp/__aligned.fasta\", \n",
      "                                 diags = True)\n",
      "216/42: from Bio.Align.Applications import MuscleCommandline\n",
      "216/43:\n",
      "    \n",
      "muscle_cline = MuscleCommandline(input=\"../tmp/_.fasta\", \n",
      "                                 out=\"../tmp/__aligned.fasta\", \n",
      "                                 diags = True)\n",
      "216/44: muscle_cline\n",
      "216/45:\n",
      "    \n",
      "muscle_cline = MuscleCommandline(input=\"../tmp/_.fasta\", \n",
      "                                 out=\"../tmp/__aligned.fasta\",\n",
      "                                 maxiters = 1, \n",
      "                                 log=\"../tmp/_log.fasta\"\n",
      "                                 diags = True)\n",
      "216/46:\n",
      "    \n",
      "muscle_cline = MuscleCommandline(input=\"../tmp/_.fasta\", \n",
      "                                 out=\"../tmp/__aligned.fasta\",\n",
      "                                 maxiters = 1, \n",
      "                                 log=\"../tmp/_log.fasta\",\n",
      "                                 diags = True)\n",
      "216/47: muscle_cline\n",
      "216/48:\n",
      "_ = batch_uniprot(ortologos, 'fasta')\n",
      "with open('../tmp/_.fasta', 'w') as f:\n",
      "    for seq in _:\n",
      "        SeqIO.write(seq, f, 'fasta')\n",
      "216/49: _\n",
      "216/50:\n",
      "_ = batch_uniprot(fus_ort, 'fasta')\n",
      "with open('../tmp/_.fasta', 'w') as f:\n",
      "    for seq in _:\n",
      "        SeqIO.write(seq, f, 'fasta')\n",
      "216/51: fus_ort = ortologos.query('uniprot == \"P35637\"').orthologs.to_list()\n",
      "216/52:\n",
      "_ = batch_uniprot(fus_ort, 'fasta')\n",
      "with open('../tmp/_.fasta', 'w') as f:\n",
      "    for seq in _:\n",
      "        SeqIO.write(seq, f, 'fasta')\n",
      "216/53:\n",
      "    \n",
      "muscle_cline = MuscleCommandline(input=\"../tmp/_.fasta\", \n",
      "                                 out=\"../tmp/__aligned.fasta\",\n",
      "                                 maxiters = 1, \n",
      "                                 log=\"../tmp/_log.fasta\",\n",
      "                                 diags = True)\n",
      "216/54: muscle_cline\n",
      "216/55:\n",
      "    \n",
      "muscle_cline = MuscleCommandline(input=\"../tmp/_.fasta\", \n",
      "                                 out=\"../tmp/__aligned.fasta\",\n",
      "                                 maxiters = 1, \n",
      "                                 log=\"../tmp/_log.fasta\",\n",
      "                                 diags = True)\n",
      "216/56: muscle_cline\n",
      "216/57: muscle_cline\n",
      "216/58: import subprocess\n",
      "216/59: subprocess.run(\"ls\")\n",
      "216/60: subprocess.run(\"muscle -in _.fasta -out _aligned.fasta\")\n",
      "216/61:\n",
      "list_files = subprocess.run([\"ls\", \"-l\"])\n",
      "print(\"The exit code was: %d\" % list_files.returncode)\n",
      "216/62: list_files\n",
      "216/63: subprocess.run([\"muscle\", \"-in\",'_.fasta','-out','_aligned.fasta'])\n",
      "216/64: subprocess.run([\"muscle\", \"-in\",'../tmp/_.fasta','-out','../tmp/_aligned.fasta'])\n",
      "216/65: subprocess.run([\"muscle\", \"-in\",'../tmp/_.fasta','-out','../tmp/_aligned.fasta'])\n",
      "216/66: subprocess.run([\"muscle\", \"-in\",'../tmp/_.fasta','-out','../tmp/_aligned.fasta', '-log','../tmp/_log.fasta'])\n",
      "216/67: disorder = request.url('https://mobidb.bio.unipd.it/api/search?acc=P35637,'+\",\".join(fus_ort))\n",
      "216/68: disorder = requests.url('https://mobidb.bio.unipd.it/api/search?acc=P35637,'+\",\".join(fus_ort))\n",
      "216/69: disorder = requests.get('https://mobidb.bio.unipd.it/api/search?acc=P35637,'+\",\".join(fus_ort))\n",
      "216/70: len(disorder)\n",
      "216/71:\n",
      "with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/search?acc=P35637,'+\",\".join(fus_ort)) as url:\n",
      "    # Variable 'data' will contain the full database as a nested dictionary\n",
      "    mobidb = json.loads(url.read().decode())\n",
      "216/72: mobidb\n",
      "216/73: mobidb\n",
      "216/74:\n",
      "with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc=P35637,'+\",\".join(fus_ort)) as url:\n",
      "    # Variable 'data' will contain the full database as a nested dictionary\n",
      "    mobidb = json.loads(url.read().decode())\n",
      "216/75: len(mobidb)\n",
      "216/76: mobidb[0]\n",
      "216/77: [i for i in mobidb.keys() if 'mobidb' in i]\n",
      "216/78: [i for i in mobidb[0].keys() if 'mobidb' in i]\n",
      "216/79:\n",
      "for prot in mobidb:\n",
      "    print(prot['accession'], prot[\"prediction-disorder-mobidb_lite\"])\n",
      "216/80: scores = {prot['accession']:prot[\"prediction-disorder-mobidb_lite\"][\"scores\"] for prot in mobidb}\n",
      "221/1: from Bio import AlignIO\n",
      "221/2: ali = AlignIO.parse('../secuencias/_.fasta', 'fasta')\n",
      "221/3: ali\n",
      "221/4: ali = listo(AlignIO.parse('../secuencias/_.fasta', 'fasta'))\n",
      "221/5: ali = list(AlignIO.parse('../secuencias/_.fasta', 'fasta'))\n",
      "221/6: ali = list(AlignIO.parse('../tmp/_.fasta', 'fasta'))\n",
      "221/7: ali = list(AlignIO.parse('../tmp/_aligned.fasta', 'fasta'))\n",
      "221/8: ali\n",
      "221/9: AlignIO.parse('../tmp/_aligned.fasta', 'fasta')\n",
      "221/10: ali\n",
      "221/11: ali[0]\n",
      "221/12: ali = AlignIO.parse('../tmp/_aligned.fasta', 'fasta')\n",
      "221/13: ali[0]\n",
      "221/14: ali = AlignIO.read('../tmp/_aligned.fasta', 'fasta')\n",
      "221/15: ali[0]\n",
      "221/16: seq = \"--AAA--A-AAA-\"\n",
      "221/17: ali = \"--AAA--A-AAA-\"\n",
      "221/18: seq = \"aaaaaaa\"\n",
      "221/19: gaps = [i for i,char in enumerate(ali) if char == \"-\"]\n",
      "221/20: gaps\n",
      "221/21:\n",
      "gaps = [i for i,char in enumerate(ali) if char == \"-\"]\n",
      "nogaps = [i for i,char in enumerate(ali) if char != \"-\"]\n",
      "221/22: nogaps\n",
      "221/23:\n",
      "molde = '-' * len(ali)\n",
      "molde\n",
      "221/24:\n",
      "molde = ['-'] * len(ali)\n",
      "molde\n",
      "221/25: list(seq)\n",
      "221/26:\n",
      "for i in nogaps:\n",
      "    print(i)\n",
      "221/27:\n",
      "seq_list = list(seq)\n",
      "for i in nogaps:\n",
      "    seq_list[i] = seq_list.pop(0)\n",
      "221/28:\n",
      "seq_list = list(seq)\n",
      "for i in nogaps:\n",
      "    seq_list[i] = seq_list.pop(0)\n",
      "    print(seq_list)\n",
      "221/29:\n",
      "seq_list = list(seq)\n",
      "for i in nogaps:\n",
      "    molde[i] = seq_list.pop(0)\n",
      "    print(seq_list)\n",
      "221/30: molde\n",
      "221/31: molde\n",
      "221/32:\n",
      "new_pos = {}\n",
      "for i(s, a) in enumerate(seq, ali):\n",
      "    print(i,s,a)\n",
      "221/33:\n",
      "new_pos = {}\n",
      "for i(s, a) in enumerate(zip(seq, ali)):\n",
      "    print(i,s,a)\n",
      "221/34:\n",
      "new_pos = {}\n",
      "for i,(s, a) in enumerate(zip(seq, ali)):\n",
      "    print(i,s,a)\n",
      "221/35:\n",
      "ali_pos = {}\n",
      "seq_pos = {}\n",
      "c = 0\n",
      "for i,a in enumerate(ali):\n",
      "    #if a != \n",
      "    print(i,s,a)\n",
      "221/36:\n",
      "ali_pos = {}\n",
      "seq_pos = {}\n",
      "c = 0\n",
      "for i,a in enumerate(ali):\n",
      "    #if a != \n",
      "    print(i,a)\n",
      "221/37:\n",
      "ali_pos = {}\n",
      "seq_pos = {}\n",
      "c = 0\n",
      "for i,a in enumerate(ali):\n",
      "    if a != \"-\":\n",
      "        c += 1\n",
      "    ali_pos[i] = c\n",
      "221/38: ali_pos\n",
      "221/39: for i,a in enumrate(ali):print(i,a)\n",
      "221/40: for i,a in enumerate(ali):print(i,a)\n",
      "221/41:\n",
      "ali_pos = {}\n",
      "seq_pos = {}\n",
      "c = 0\n",
      "for i,a in enumerate(ali):\n",
      "    if a != \"-\":\n",
      "        c += 1\n",
      "    ali_pos[i] = c\n",
      "    seq_pos[c] = i\n",
      "221/42: seq_pos\n",
      "221/43:\n",
      "ali_pos = {}\n",
      "seq_pos = {}\n",
      "c = 0\n",
      "for i,a in enumerate(ali):\n",
      "    if a != \"-\":\n",
      "        c += 1\n",
      "    ali_pos[i] = c\n",
      "    seq_pos[c + 1] = i\n",
      "221/44: seq_pos\n",
      "221/45:\n",
      "ali_pos = {}\n",
      "seq_pos = {}\n",
      "c = 0\n",
      "for i,a in enumerate(ali):\n",
      "    if a != \"-\":\n",
      "        c += 1\n",
      "    ali_pos[i] = c\n",
      "    seq_pos[c] = i\n",
      "221/46: seq_pos\n",
      "221/47: {k:v for v,k in ali_seq.items()}\n",
      "221/48: {k:v for v,k in ali_pos.items()}\n",
      "221/49: reversed(ali_pos.items())\n",
      "221/50: {k:v for v,k in reversed(ali_pos.items())}\n",
      "221/51: for i,a in enumerate(ali_pos):print(i,a)\n",
      "221/52: for i,a in enumerate(ali):print(i,a)\n",
      "221/53: reversed({k:v for v,k in reversed(ali_pos.items())})\n",
      "221/54: dict(reversed({k:v for v,k in reversed(ali_pos.items())}))\n",
      "221/55: {k:v for v,k in reversed(ali_pos.items())}\n",
      "221/56: seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "221/57: seq_pos[3]\n",
      "221/58: for i,a in enumerate(ali):print(i,a)\n",
      "221/59: seq_pos[4]\n",
      "221/60: seq_pos[5]\n",
      "221/61: seq_pos[12]\n",
      "221/62: import numpy as np\n",
      "221/63: list([2,3])\n",
      "221/64: list([\"2\",\"3\"])\n",
      "221/65: list(23)\n",
      "221/66: list(\"23\")\n",
      "221/67: np.array(\"-\")\n",
      "221/68: np.array(\"-\") * 10\n",
      "221/69: np.chararray(\"-\") * 10\n",
      "221/70: np.chararray(\"-\")\n",
      "221/71: np.chararray(1,len(ali_seq))\n",
      "221/72: np.chararray(1,len(ali))\n",
      "221/73:\n",
      "a = np.chararray(1,len(ali))\n",
      "a[:] = '-'\n",
      "221/74: a\n",
      "221/75: len(ali)\n",
      "221/76:\n",
      "a = np.chararray((1,len(ali)))\n",
      "a[:] = '-'\n",
      "221/77: a\n",
      "221/78:\n",
      "def map_feature(raw_seq, in_ali_seq):\n",
      "    rseq = list(raw_seq)\n",
      "    molde = np.chararray((1,len(ali)))\n",
      "    molde[:] = '-'\n",
      "    nogaps = [i for i,char in enumerate(ali) if char != \"-\"]\n",
      "    for i in nogaps:\n",
      "        molde[i] = rseq.pop(0)\n",
      "221/79:\n",
      "def map_feature(raw_seq, in_ali_seq):\n",
      "    rseq = list(raw_seq)\n",
      "    molde = np.chararray((1,len(ali)))\n",
      "    molde[:] = '-'\n",
      "    nogaps = [i for i,char in enumerate(ali) if char != \"-\"]\n",
      "    for i in nogaps:\n",
      "        molde[i] = rseq.pop(0)\n",
      "    return moled\n",
      "221/80:\n",
      "def map_feature(raw_seq, in_ali_seq):\n",
      "    rseq = list(raw_seq)\n",
      "    molde = np.chararray((1,len(ali)))\n",
      "    molde[:] = '-'\n",
      "    nogaps = [i for i,char in enumerate(ali) if char != \"-\"]\n",
      "    for i in nogaps:\n",
      "        molde[i] = rseq.pop(0)\n",
      "    return molde\n",
      "221/81: map_feature(seq,ali)\n",
      "221/82:\n",
      "def map_feature(raw_seq, in_ali_seq):\n",
      "    rseq = list(raw_seq)\n",
      "    molde = ['-'] * len(ali)\n",
      "    nogaps = [i for i,char in enumerate(ali) if char != \"-\"]\n",
      "    for i in nogaps:\n",
      "        molde[i] = rseq.pop(0)\n",
      "    return molde\n",
      "221/83: map_feature(seq,ali)\n",
      "221/84:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "    ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(ali)\n",
      "        nogaps = [i for i,char in enumerate(ali) if char != \"-\"]\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida{'new_seq':molde}\n",
      "            \n",
      "        \n",
      "    return salida\n",
      "221/85:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "    ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(ali)\n",
      "        nogaps = [i for i,char in enumerate(ali) if char != \"-\"]\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde\n",
      "            \n",
      "        \n",
      "    return salida\n",
      "221/86:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "    ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(ali)\n",
      "        nogaps = [i for i,char in enumerate(ali) if char != \"-\"]\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "221/87: map_feature(\"-A-A-AAA----A-A-A\", \"aaaaaaaa\")\n",
      "221/88:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(ali)\n",
      "        nogaps = [i for i,char in enumerate(ali) if char != \"-\"]\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "221/89: map_feature(\"-A-A-AAA----A-A-A\", \"aaaaaaaa\")\n",
      "221/90:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(in_ali_seq)\n",
      "        nogaps = [i for i,char in enumerate(in_ali_seq) if char != \"-\"]\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "221/91: map_feature(\"-A-A-AAA----A-A-A\", \"aaaaaaaa\")\n",
      "221/92:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(in_ali_seq)\n",
      "        nogaps = [i for i,char in enumerate(in_ali_seq) if char != \"-\"]\n",
      "        print(nogaps)\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "221/93: map_feature(\"-A-A-AAA----A-A-A\", \"aaaaaaaa\")\n",
      "221/94: map_feature(\"-A-A-AAA----A-A-A\", \"aaaaaaaa\")\n",
      "221/95:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(in_ali_seq)\n",
      "        nogaps = [i for i,char in enumerate(in_ali_seq) if char != \"-\"]\n",
      "        print(molde)\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "221/96: map_feature(\"-A-A-AAA----A-A-A\", \"aaaaaaaa\")\n",
      "221/97: map_feature(\"aaaaaaaa\", \"-A-A-AAA----A-A-A\")\n",
      "221/98: a = \"-------------------------------------------------------MFMSGHKLLQLATKSKFKNTHSIFKRCNRLQKWARENQGRAVCTSLIHHSYSGRYCISVTIFNIGILPRTLGINNLSTFYFDVSLFAAYGT-------PYQQQPSSAPVSSADPNSATYQQYDPAAIAQ-----FQQAWQQYYAVMGQNTGVGHSGTSVAQQPAGATSNANADPNA----TQYSGYFAQSSG-----PGNG-----------------------------------------------------------------------------------------PSGNNGG-------PSSGYG-------------------NNSNSNYVGPQGPGRSSGGDHSGDMKGGGYDGGYQGDRGGRGGGYPGDLTDVSGDR------------------YNRDDRSKDRGDRDRDRDRDRDRDYDRDYGRGDRDRDYG-SRDRG--------SRDDRGRDRPSRWSD-DRERGPPNDGFRGGP-----GGGG----------------------PGPR-GGFQDRG-PPHGYGAS--------RGEQIEVRETV------------------FVQGIPTNVNEQFIHDVFCTQGDIARNER--T-GEP-----RIKIYTDRDTNQPKGECT-ITFVDARTAEDVIRTYNGAQFPGSDQKMTLSFAKYKT-D---S------------------------G--Q------RGSGR------GG----------------------------------RGGFDRGGYGGR---DRDFSSERHGGDRYGYGNGGNDKFNDRGGGGRNYGGGRS---------------------SFEGDRGGYNGGRGGFGGGRGGFGGSDRGGRGGK---------------------------YGGPRDDFGHGDGGSRG----G-----FRGGRGGP---------GGPG-GFERGRGGRGG--------------------------GFDRG-------GR---------GGRGRGGGVNMEARPNDWVCEPCGNTNFGFRRECNRCHAPRGASGGSGPMRGDGGRGRGRM-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------DRPAPY--------------------------------------------\"\n",
      "221/99: A =\"\".join([i for i in a if i != \"-\"]\n",
      "221/100: A =\"\".join([i for i in a if i != \"-\"])\n",
      "221/101: A\n",
      "221/102: salida = map_feature(A,a)\n",
      "221/103:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(in_ali_seq)\n",
      "        nogaps = [i for i,char in enumerate(in_ali_seq) if char != \"-\"]\n",
      "\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "221/104: salida = map_feature(A,a)\n",
      "221/105: salida\n",
      "221/106:\n",
      "salida[\"in_ali_pos\"]\n",
      "salida['in_seq_pos']\n",
      "221/107:\n",
      "print(salida[\"in_ali_pos\"][1])\n",
      "print(salida['in_seq_pos'][1])\n",
      "221/108:\n",
      "print(salida[\"in_ali_pos\"][1])\n",
      "print(salida['in_seq_pos'][98])\n",
      "221/109:\n",
      "print(salida[\"in_ali_pos\"][98])\n",
      "print(salida['in_seq_pos'][1])\n",
      "221/110:\n",
      "print(salida[\"in_ali_pos\"][390])\n",
      "print(salida['in_seq_pos'][667])\n",
      "221/111:\n",
      "print(salida[\"in_ali_pos\"][667])\n",
      "print(salida['in_seq_pos'][390])\n",
      "221/112:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 1\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(in_ali_seq)\n",
      "        nogaps = [i for i,char in enumerate(in_ali_seq) if char != \"-\"]\n",
      "\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "221/113: salida = map_feature(A,a)\n",
      "221/114:\n",
      "print(salida[\"in_ali_pos\"][667])\n",
      "print(salida['in_seq_pos'][390])\n",
      "221/115:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(in_ali_seq)\n",
      "        nogaps = [i for i,char in enumerate(in_ali_seq) if char != \"-\"]\n",
      "\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "221/116: salida = map_feature(A,a)\n",
      "221/117:\n",
      "print(salida[\"in_ali_pos\"][667])\n",
      "print(salida['in_seq_pos'][390])\n",
      "221/118:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v+1 for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(in_ali_seq)\n",
      "        nogaps = [i for i,char in enumerate(in_ali_seq) if char != \"-\"]\n",
      "\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "221/119: salida = map_feature(A,a)\n",
      "221/120:\n",
      "print(salida[\"in_ali_pos\"][667])\n",
      "print(salida['in_seq_pos'][390])\n",
      "221/121:\n",
      "print(salida[\"in_ali_pos\"][1121])\n",
      "print(salida['in_seq_pos'][650])\n",
      "222/1:\n",
      "import pandas as pd\n",
      "import requests\n",
      "222/2: token = \"4b4385b258c39a48449863dc6731c8feefb87515\"\n",
      "222/3:\n",
      "import requests\n",
      "respons = requests.get('http://platoloco.aei.polsl.pl/restapi/proteins/4b4385b258c39a48449863dc6731c8feefb87515/1')\n",
      "json_response = response.json()\n",
      "222/4:\n",
      "response = requests.get('http://platoloco.aei.polsl.pl/restapi/proteins/4b4385b258c39a48449863dc6731c8feefb87515/1')\n",
      "json_response = response.json()\n",
      "222/5: json_response\n",
      "222/6: ali = AlignIO.read('../tmp/_aligned.fasta', 'fasta')\n",
      "222/7:\n",
      "import pandas as pd\n",
      "import requests\n",
      "from Bio import AlignIO\n",
      "222/8: ali = AlignIO.read('../tmp/_aligned.fasta', 'fasta')\n",
      "222/9: [i.id for i in ali]\n",
      "222/10:\n",
      "entropias = {}\n",
      "for i,uniprot in enumerate([i.id for i in ali]):\n",
      "    response = requests.get('http://platoloco.aei.polsl.pl/restapi/proteins/4b4385b258c39a48449863dc6731c8feefb87515/' + str(i + 1))\n",
      "    entropias[uniprot] = response.json()[\"data\"][\"entropy\"]\n",
      "222/11: entropias\n",
      "222/12: list([2,3])\n",
      "222/13: ali\n",
      "222/14: print(ali)\n",
      "222/15: response.json()[\"data\"]\n",
      "222/16: response.json()[\"entry\"]\n",
      "222/17: response.json()#[\"entry\"]\n",
      "222/18: entropias[\"A0A183J953\"]\n",
      "222/19: from Bio import SeqIO\n",
      "222/20: seqs = list(SeqIO.parse(\"../tmp/_.fasta\",'fasta'))\n",
      "222/21: seqs\n",
      "222/22: seqs = list(SeqIO.parse(\"../tmp/_.fasta\",'fasta'))\n",
      "222/23:\n",
      "for seq in seqs:\n",
      "    seq.name = seq.id\n",
      "222/24: seqs\n",
      "222/25:\n",
      "for seq in seqs:\n",
      "    seq.description = \"\"\n",
      "    seq.name = \"\"\n",
      "222/26: seqs\n",
      "222/27:\n",
      "with open('../secuencias/fus_ort.fasta', 'w') as f:\n",
      "    for seq in seqs:\n",
      "        SeqIO.write(seq, f)\n",
      "222/28:\n",
      "with open('../secuencias/fus_ort.fasta', 'w') as f:\n",
      "    for seq in seqs:\n",
      "        SeqIO.write(seq, f, 'fasta')\n",
      "222/29: token = \"0c37f8e321642f0fbe13199b0511574465da43ff\"\n",
      "222/30: ali = AlignIO.read('../secuencias/fus_ort_aligned.fasta', 'fasta')\n",
      "222/31: [i.id for i in ali]\n",
      "222/32:\n",
      "response = requests.get('http://platoloco.aei.polsl.pl/restapi/proteins/4b4385b258c39a48449863dc6731c8feefb87515/1')\n",
      "data = response.json()\n",
      "222/33: data\n",
      "222/34:\n",
      "response = requests.get('http://platoloco.aei.polsl.pl/restapi/proteins/0c37f8e321642f0fbe13199b0511574465da43ff/1')\n",
      "data = response.json()\n",
      "222/35: data\n",
      "222/36: data[\"header\"]\n",
      "222/37:\n",
      "response = requests.get('http://platoloco.aei.polsl.pl/restapi/proteins/0c37f8e321642f0fbe13199b0511574465da43ff/2')\n",
      "data = response.json()\n",
      "222/38: data[\"header\"]\n",
      "222/39: data[\"header\"].replace(\">\",'')\n",
      "222/40:\n",
      "entropias = {}\n",
      "for i,uniprot in enumerate([i.id for i in ali]):\n",
      "    response = requests.get('http://platoloco.aei.polsl.pl/restapi/proteins/4b4385b258c39a48449863dc6731c8feefb87515/' + str(i + 1))\n",
      "    data = response.json()\n",
      "    entropias[data[\"header\"].replace(\">\",'')] = response.json()[\"data\"][\"entropy\"]\n",
      "222/41: entropias\n",
      "222/42:\n",
      "entropias = {}\n",
      "for i,uniprot in enumerate([i.id for i in ali]):\n",
      "    response = requests.get('http://platoloco.aei.polsl.pl/restapi/proteins/0c37f8e321642f0fbe13199b0511574465da43ff/' + str(i + 1))\n",
      "    data = response.json()\n",
      "    entropias[data[\"header\"].replace(\">\",'')] = response.json()[\"data\"][\"entropy\"]\n",
      "222/43: entropias\n",
      "222/44:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v+1 for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(in_ali_seq)\n",
      "        nogaps = [i for i,char in enumerate(in_ali_seq) if char != \"-\"]\n",
      "\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "222/45: ali\n",
      "222/46: alineamiento = {seq.id:seq.seq for seq in ali}\n",
      "222/47: alineamiento\n",
      "222/48: alineamiento = {seq.id:str(seq.seq) for seq in ali}\n",
      "222/49: alineamiento\n",
      "222/50:\n",
      "matriz = []\n",
      "for uniprot,scores in entropias.items():\n",
      "    matriz.append(map_features(scores, alineamiento[uniprot])['new_seq'])\n",
      "222/51:\n",
      "def map_feature(raw_seq, in_ali_seq, new_seq = True): \n",
      "    ali_pos = {}\n",
      "    seq_pos = {}\n",
      "    c = 0\n",
      "    for i,a in enumerate(in_ali_seq):\n",
      "        if a != \"-\":\n",
      "            c += 1\n",
      "        ali_pos[i] = c\n",
      "    seq_pos = {k:v+1 for v,k in reversed(ali_pos.items())}\n",
      "    salida = {'in_ali_pos':ali_pos, 'in_seq_pos':seq_pos,'new_seq':None}\n",
      "    if new_seq:\n",
      "        rseq = list(raw_seq)\n",
      "        molde = ['-'] * len(in_ali_seq)\n",
      "        nogaps = [i for i,char in enumerate(in_ali_seq) if char != \"-\"]\n",
      "\n",
      "        for i in nogaps:\n",
      "            molde[i] = rseq.pop(0)\n",
      "        salida['new_seq'] = molde          \n",
      "    return salida\n",
      "222/52:\n",
      "matriz = []\n",
      "for uniprot,scores in entropias.items():\n",
      "    matriz.append(map_feature(scores, alineamiento[uniprot])['new_seq'])\n",
      "222/53: matriz\n",
      "222/54: np.NA\n",
      "222/55: import numpy\n",
      "222/56: import numpy as no\n",
      "222/57: import numpy as np\n",
      "222/58:\n",
      "\n",
      "np\n",
      "222/59: np.nan()\n",
      "222/60: np.nan\n",
      "222/61:\n",
      "matriz = []\n",
      "for uniprot,scores in entropias.items():\n",
      "    new_seq = map_feature(scores, alineamiento[uniprot])['new_seq']\n",
      "    seq_num = [i for i in new_seq if it != \"-\" else np.nan]\n",
      "    matriz.append(map_feature(scores, alineamiento[uniprot])['new_seq'])\n",
      "222/62:\n",
      "matriz = []\n",
      "for uniprot,scores in entropias.items():\n",
      "    new_seq = map_feature(scores, alineamiento[uniprot])['new_seq']\n",
      "    seq_num = [i for i in new_seq if i != \"-\" else np.nan]\n",
      "    matriz.append(map_feature(scores, alineamiento[uniprot])['new_seq'])\n",
      "222/63: [i for i in \"AAA-A\" if i != \"-\"]\n",
      "222/64: [i for i in \"AAA-A\" if i != \"-\" else 'S']\n",
      "222/65:\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "222/66: float('nan')\n",
      "222/67: float('nans')\n",
      "222/68:\n",
      "matriz = []\n",
      "for uniprot,scores in entropias.items():\n",
      "    new_seq = map_feature(scores, alineamiento[uniprot])['new_seq']\n",
      "    seq_num = []\n",
      "    for i in new_seq:\n",
      "        if i != '-':\n",
      "            seq_num.append(i)\n",
      "        else:\n",
      "            seq_num.append(float(nan))\n",
      "    matriz.append(pd.Series(seq_num))\n",
      "222/69:\n",
      "matriz = []\n",
      "for uniprot,scores in entropias.items():\n",
      "    new_seq = map_feature(scores, alineamiento[uniprot])['new_seq']\n",
      "    seq_num = []\n",
      "    for i in new_seq:\n",
      "        if i != '-':\n",
      "            seq_num.append(i)\n",
      "        else:\n",
      "            seq_num.append(float(\"nan\"))\n",
      "    matriz.append(pd.Series(seq_num))\n",
      "222/70: matriz\n",
      "222/71: matriz[0]\n",
      "222/72: matriz[0][:30]\n",
      "222/73: matriz[0][:50]\n",
      "222/74: matriz[0][:100]\n",
      "222/75: matriz[0].sum()\n",
      "222/76: import seaborn as sns\n",
      "222/77: sns.heatmap(matriz)\n",
      "222/78:\n",
      "import seaborn as sns\n",
      "import matplotlib.pyplot as plt\n",
      "222/79:\n",
      "_ = plt.subplots(figsize=(10,10))\n",
      "sns.heatmap(matriz)\n",
      "225/1: import urllib.request, json\n",
      "225/2:\n",
      "import time\n",
      "import requests\n",
      "225/3: import urllib.request, json\n",
      "225/4:\n",
      "import time\n",
      "import requests\n",
      "225/5:\n",
      "#with urllib.request.urlopen(\"https://phasepro.elte.hu/download_full.json\") as url:\n",
      "#    # Variable 'data' will contain the full database as a nested dictionary\n",
      "#    data = json.loads(url.read().decode())\n",
      "#entrada = pd.DataFrame.from_dict(data, orient='index').rename_axis(\"uniprot\").reset_index()\n",
      "#entrada.to_csv('phasepro_latest_db.csv', index=False)\n",
      "225/6: import pandas as pd\n",
      "225/7:\n",
      "#ortologos.to_csv('../data/ortologos.csv', index=False)\n",
      "ortologos = pd.read_csv('../data/ortologos.csv')\n",
      "225/8: uniprots_phasepro = entrada.uniprot.unique().tolist()\n",
      "225/9:\n",
      "#jsons = {}\n",
      "#f = []\n",
      "#for uniprot in entrada.uniprot.unique():\n",
      "#    try:\n",
      "#        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "#        time.sleep(1)\n",
      "#    except:\n",
      "#        f.append(uniprot)\n",
      "#        print(uniprot, len(f))\n",
      "225/10:\n",
      "import time\n",
      "import requests\n",
      "import pandas as pd\n",
      "import json\n",
      "225/11: entrada = pd.read_csv('phasepro_latest_db.csv')\n",
      "225/12: entrada.uniprot = entrada.uniprot.str.split(\"-\").str[0]\n",
      "225/13: [i for i,v in jsons.items() if len(v) == 2]\n",
      "225/14:\n",
      "#jsons = {}\n",
      "#f = []\n",
      "#for uniprot in entrada.uniprot.unique():\n",
      "#    try:\n",
      "#        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "#        time.sleep(1)\n",
      "#    except:\n",
      "#        f.append(uniprot)\n",
      "#        print(uniprot, len(f))\n",
      "#dfs = []\n",
      "#for k,v in jsons.items():\n",
      "#    try:\n",
      "#        df = pd.DataFrame(v)\n",
      "#        df['uniprot'] = k\n",
      "#        dfs.append(df)\n",
      "#    except ValueError:\n",
      "#        pass\n",
      "#df = pd.concat(dfs)\n",
      "#ortologos = df.explode('orthologs')\n",
      "225/15:\n",
      "#jsons = {}\n",
      "#f = []\n",
      "#for uniprot in entrada.uniprot.unique():\n",
      "#    try:\n",
      "#        jsons[uniprot] = requests.get(f'https://lbgi.fr/orthoinspectorv3/api/Eukaryota/protein/{uniprot}/orthologs').json()\n",
      "#        time.sleep(1)\n",
      "#    except:\n",
      "#        f.append(uniprot)\n",
      "#        print(uniprot, len(f))\n",
      "#dfs = []\n",
      "#for k,v in jsons.items():\n",
      "#    try:\n",
      "#        df = pd.DataFrame(v)\n",
      "#        df['uniprot'] = k\n",
      "#        dfs.append(df)\n",
      "#    except ValueError:\n",
      "#        pass\n",
      "#print([i for i,v in jsons.items() if len(v) == 2])\n",
      "#df = pd.concat(dfs)\n",
      "#ortologos = df.explode('orthologs')\n",
      "225/16:\n",
      "\n",
      "ortologos = pd.read_csv('../data/ortologos.csv')\n",
      "225/17: uniprots_phasepro = entrada.uniprot.unique().tolist()\n",
      "225/18: from uniprot import batch_uniprot\n",
      "225/19: uniprots_phasepro = entrada.uniprot.unique().tolist()\n",
      "225/20: uniprots_phasepro_info = batch_uniprot(uniprots_phasepro , columns=columns)\n",
      "225/21: from uniprot import batch_uniprot, columns\n",
      "225/22: uniprots_phasepro_info = batch_uniprot(uniprots_phasepro , columns=columns)\n",
      "225/23: uniprots_phasepro_info[\"org\"] = uniprots_phasepro_info[\"Organism\"].str.split(\"\\s+\\(\").str[0]\n",
      "225/24: uniprots_phasepro_info.columns = [i.replace(' ','_',).replace('(','_').replace(')','_').replace('__','_').lower()  for i in uniprots_phasepro_info.columns]\n",
      "225/25: uniprots_phasepro_info\n",
      "225/26: ortologos\n",
      "225/27: uniprots = entrada.uniprot.unique().tolist()\n",
      "225/28: ortologos.expand(ortologos.orthologs.unique().tolist())\n",
      "225/29: uniprots.expand(ortologos.orthologs.unique().tolist())\n",
      "225/30: uniprots.extend(ortologos.orthologs.unique().tolist())\n",
      "225/31: uniprots_phasepro_info = batch_uniprot(uniprots, columns=columns)\n",
      "225/32:\n",
      "import time\n",
      "import requests\n",
      "import pandas as pd\n",
      "import json\n",
      "225/33: from uniprot import batch_uniprot, columns\n",
      "225/34: entrada = pd.read_csv('phasepro_latest_db.csv')\n",
      "225/35: entrada.uniprot = entrada.uniprot.str.split(\"-\").str[0]\n",
      "225/36:\n",
      "\n",
      "ortologos = pd.read_csv('../data/ortologos.csv')\n",
      "225/37: uniprots = entrada.uniprot.unique().tolist()\n",
      "225/38: uniprots.extend(ortologos.orthologs.unique().tolist())\n",
      "225/39: len(uniprots)\n",
      "225/40: uniprots_phasepro_info = batch_uniprot(uniprots, columns=columns)\n",
      "225/41: uniprots_phasepro_info = batch_uniprot(uniprots, columns=columns)\n",
      "225/42: uniprots_phasepro_info\n",
      "225/43: uniprots_phasepro_info[\"org\"] = uniprots_phasepro_info[\"Organism\"].str.split(\"\\s+\\(\").str[0]\n",
      "225/44: uniprots_phasepro_info.columns = [i.replace(' ','_',).replace('(','_').replace(')','_').replace('__','_').lower()  for i in uniprots_phasepro_info.columns]\n",
      "225/45: uniprots_phasepro_info\n",
      "225/46: uniprots_phasepro_info.to_csv('../data/uniprot_info.csv',index=False)\n",
      "225/47: uniprots_phasepro_fasta =  batch_uniprot(uniprots, \"fasta\")\n",
      "225/48: uniprots_phasepro_fasta =  batch_uniprot(uniprots, \"fasta\")\n",
      "225/49: uniprots_phasepro_fasta =  batch_uniprot(uniprots, format=\"fasta\")\n",
      "225/50: uniprots_phasepro_fasta =  batch_uniprot(uniprots, format=\"fasta\")\n",
      "225/51: uniprots_phasepro_info\n",
      "225/52: uniprots_phasepro_info.sequence\n",
      "225/53: uniprots_phasepro_info[['entry','sequence']]\n",
      "225/54: uniprots_phasepro_info[['entry','sequence']].rename(columns={'entry':'uniprot',{'sequence':'seq'}).to_csv('../data/secuencias.csv', index=False)\n",
      "225/55: uniprots_phasepro_info[['entry','sequence']].rename(columns={'entry':'uniprot','sequence':'seq'}).to_csv('../data/secuencias.csv', index=False)\n",
      "223/1:\n",
      "from Bio import SeqIO\n",
      "from Bio import Phylo\n",
      "from Bio.Phylo.TreeConstruction import DistanceCalculator\n",
      "from Bio.Phylo.TreeConstruction import DistanceTreeConstructor\n",
      "from Bio import AlignIO\n",
      "from uniprot import batch_uniprot\n",
      "223/2: ali = AlignIO.read('../secuencias/fus_ort_aligned.fasta', 'fasta')\n",
      "223/3: uniprots = [proteina.id for proteina in ali]\n",
      "223/4: orgs_table = batch_uniprot(uniprots, 'tab', columns=[\"id\",'entry name','organism','lineage(ALL)','lineage(FAMILY)','lineage(ORDER)'])\n",
      "223/5: orgs_table['org'] = orgs_table.Organism.str.split(\"\\s+\\(\").str[0]\n",
      "223/6:\n",
      "#orgs_table['fam'] = orgs_table[\"Taxonomic lineage (FAMILY)\"].str.split(\"\\s+\\(\").str[0]\n",
      "orgs_table['new_id'] = orgs_table['Entry'] + \"_\" + orgs_table.org + orgs_table[\"Taxonomic lineage (FAMILY)\"] + orgs_table[\"Taxonomic lineage (ORDER)\"]\n",
      "223/7: #AlignIO.write(ali, '../secuencias/fus_ort_aligned_new_id.fasta','fasta')\n",
      "223/8: ali = AlignIO.read('../secuencias/fus_ort_aligned_new_id.fasta','fasta')\n",
      "223/9:\n",
      "calculator = DistanceCalculator('identity')\n",
      "dm = calculator.get_distance(ali)\n",
      "223/10:\n",
      "constructor = DistanceTreeConstructor()\n",
      "tree = constructor.upgma(dm)\n",
      "223/11: from ete3 import PhyloTree, TreeStyle\n",
      "223/12: !pip install ete3\n",
      "223/13: from ete3 import PhyloTree, TreeStyle\n",
      "223/14: import Bio\n",
      "223/15:\n",
      "#with open('../secuencias/tree.nw','w') as f:\n",
      "#    Bio.Phylo.NewickIO.write([tree],f,plain=True )\n",
      "223/16:\n",
      "a = []\n",
      "for seq in ali[:,638:720]:\n",
      "    a.append(\">\"+seq.id)\n",
      "    a.append(str(seq.seq))\n",
      "new_ali = \"\\n\".join(a)\n",
      "223/17: tr = PhyloTree(\"../secuencias/tmp.phy\", alignment=new_ali)\n",
      "223/18: tr.show()\n",
      "223/19:\n",
      "a = []\n",
      "for seq in ali[:,638:740] + ali[:,761:780]:\n",
      "    a.append(\">\"+seq.id)\n",
      "    a.append(str(seq.seq))\n",
      "new_ali = \"\\n\".join(a)\n",
      "223/20: tr = PhyloTree(\"../secuencias/tmp.phy\", alignment=new_ali)\n",
      "223/21: tr.show()\n",
      "223/22: tr.show()\n",
      "225/56: entrada\n",
      "225/57: entrada.columns\n",
      "225/58: entrada.boundaries\n",
      "225/59: entrada\n",
      "225/60: entrada.columns\n",
      "225/61: entrada.loc[0]\n",
      "225/62: entrada.segment\n",
      "225/63: entrada.segment.str.split(\"; \")\n",
      "225/64: entrada.segment.str.split(\"; \").pipe(lambda x: x.explode('segment'))\n",
      "225/65: entrada.segment.str.split(\"; \").pipe(lambda x: x)\n",
      "225/66: entrada.assign(llps_driver= lambda x: x.segment.str.split(\"; \"))\n",
      "225/67: entrada.assign(llps_driver= lambda x: x.segment.str.split(\"; \")).pipe(lambda x: x.explode('llps_driver'))\n",
      "225/68: entrada\n",
      "225/69: entrada.assign(llps_driver= lambda x: x.segment.str.split(\"; \")).pipe(lambda x: x.explode('llps_driver')).llps_driver.value_counts()\n",
      "225/70: entrada.assign(llps_driver= lambda x: x.segment.str.split(\"; \")).pipe(lambda x: x.explode('llps_driver')).llps_driver.value_counts()[:30]\n",
      "225/71: entrada.assign(llps_driver= lambda x: x.segment.str.split(\"; \")).pipe(lambda x: x.explode('llps_driver')).query('llps_driver == \"Low complexity region with Pro-Xn-Gly motifs\"')\n",
      "225/72: ortologos\n",
      "225/73: ortologos.query('uniprot == \"Q7Z739\"')\n",
      "225/74: ortologos.query('uniprot == \"P35637\"')\n",
      "225/75: from Seq import AlignIO\n",
      "225/76: from Bio import AlignIO\n",
      "225/77: ali = AlignIO.read('../secuencias/fus_ort_aligned_new_id.fasta')\n",
      "225/78: ali = AlignIO.read('../secuencias/fus_ort_aligned_new_id.fasta', 'fasta')\n",
      "225/79: ortologos.query('uniprot == \"P35637\"').query('type == \"One-to-One\"')\n",
      "225/80: ortologos.query('uniprot == \"P35637\"').query('type == \"One-to-One\"').uniprot.tolist()\n",
      "225/81: ortologos.query('uniprot == \"P35637\"').query('type == \"One-to-One\"').orthologs.tolist()\n",
      "225/82: ali = AlignIO.read('../secuencias/fus_ort.fasta', 'fasta')\n",
      "225/83: ali = AlignIO.read('../secuencias/fus_ort_aligned.fasta', 'fasta')\n",
      "225/84: ali\n",
      "225/85: print(ali)\n",
      "225/86: ortologos.query('uniprot == \"P35637\"').query('type != \"One-to-One\"').orthologs.tolist()\n",
      "225/87: ortologos.query('uniprot == \"Q7Z739\"').orthologs.to_list()\n",
      "225/88: [\"Q7Z739\"] + ortologos.query('uniprot == \"Q7Z739\"').orthologs.to_list()\n",
      "225/89: secuencias = pd.read_csv('../data/secuencias.csv')\n",
      "225/90: secuencias[secuencias.uniprot.isin(([\"Q7Z739\"] + ortologos.query('uniprot == \"Q7Z739\"').orthologs.to_list()))]\n",
      "225/91:\n",
      "for i,row in secuencias[secuencias.uniprot.isin(([\"Q7Z739\"] + ortologos.query('uniprot == \"Q7Z739\"').orthologs.to_list()))].iterrows():\n",
      "    print('>' + row.uniprot)\n",
      "    print(row.seq)\n",
      "225/92:\n",
      "for i,row in secuencias[secuencias.uniprot.isin(([\"Q7Z739\"] + ortologos.query('uniprot == \"Q7Z739\"').orthologs.to_list()))].dropna().iterrows():\n",
      "    print('>' + row.uniprot)\n",
      "    print(row.seq)\n",
      "223/23: ali = AlignIO.read('../secuencias/fus_ort_aligned.fasta','fasta')\n",
      "223/24: ali\n",
      "223/25:\n",
      "for i in ali:\n",
      "    print(i.id)\n",
      "223/26:\n",
      "print('(')\n",
      "for i in ali:\n",
      "    print(\"'\"+i.id+\"',\")\n",
      "223/27:\n",
      "print('(')\n",
      "for i in ali:\n",
      "    print(\"'\"+i.seq+\"',\")\n",
      "    \n",
      "print(')')\n",
      "223/28:\n",
      "print('(')\n",
      "for i in ali:\n",
      "    e [f in i if f != '-']\n",
      "    print(\"'\"+e+\"',\")\n",
      "    \n",
      "print(')')\n",
      "223/29:\n",
      "print('(')\n",
      "for i in ali:\n",
      "    e =  [f in i if f != '-']\n",
      "    print(\"'\"+e+\"',\")\n",
      "    \n",
      "print(')')\n",
      "223/30:\n",
      "print('(')\n",
      "for i in ali:\n",
      "    e =  [f for f in i if f != '-']\n",
      "    print(\"'\"+e+\"',\")\n",
      "    \n",
      "print(')')\n",
      "223/31:\n",
      "print('(')\n",
      "for i in ali:\n",
      "    e =  [f for f in i if f != '-']\n",
      "    \n",
      "    #print(\"'\"+e+\"',\")\n",
      "    \n",
      "print(')')\n",
      "223/32:\n",
      "print('(')\n",
      "for i in ali:\n",
      "    e =  [f for f in i if f != '-']\n",
      "    print(\"'\"+e+\"',\")\n",
      "    \n",
      "print(')')\n",
      "223/33:\n",
      "print('(')\n",
      "for i in ali:\n",
      "    e =  [f for f in i if f != '-']\n",
      "    print(\"'\"+\"\".join(e)+\"',\")\n",
      "    \n",
      "print(')')\n",
      "225/93: %history\n",
      "225/94: %history -g\n",
      "228/1: import pandas as pd\n",
      "228/2: import requests\n",
      "225/95: fus_ort\n",
      "225/96:\n",
      "import time\n",
      "import requests\n",
      "import pandas as pd\n",
      "import json\n",
      "225/97: from uniprot import batch_uniprot, columns\n",
      "225/98:\n",
      "\n",
      "ortologos = pd.read_csv('../data/ortologos.csv')\n",
      "225/99: ortologos = pd.read_csv('../data/ortologos.csv')\n",
      "225/100: ortologos\n",
      "225/101: uniprots = entrada.uniprot.unique().tolist()\n",
      "225/102: uniprots.extend(ortologos.orthologs.unique().tolist())\n",
      "225/103: uniprots\n",
      "225/104: disorder = request.url('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots))\n",
      "225/105:\n",
      "import pandas as pd\n",
      "import requests\n",
      "225/106: disorder = request.url('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots))\n",
      "225/107: disorder = requests.url('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots))\n",
      "225/108:\n",
      "import pandas as pd\n",
      "import request\n",
      "225/109:\n",
      "import pandas as pd\n",
      "import urllib.request\n",
      "225/110: disorder = request.url('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots))\n",
      "225/111:\n",
      "import pandas as pd\n",
      "import request\n",
      "225/112:\n",
      "import pandas as pd\n",
      "import requests\n",
      "225/113: disorder = requests.url('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots))\n",
      "225/114: disorder = requests.get('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots))\n",
      "225/115: len(disorder)\n",
      "225/116: a = pd.read_json(disorder)\n",
      "225/117: a = pd.read_json('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots))\n",
      "225/118:\n",
      "with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots)) as url:\n",
      "    # Variable 'data' will contain the full database as a nested dictionary\n",
      "    mobidb = json.loads(url.read().decode())\n",
      "225/119:\n",
      "for i in range(0,len(uniprots),1000):\n",
      "    print(i)\n",
      "225/120:\n",
      "for i in range(0,len(uniprots),1000):\n",
      "lista_jsons =  []\n",
      "with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots[i: i + 1000])) as url:\n",
      "    # Variable 'data' will contain the full database as a nested dictionary\n",
      "    mobidb = json.loads(url.read().decode())\n",
      "lista_jsons.extend(mobidb)\n",
      "225/121:\n",
      "lista_jsons =  []\n",
      "\n",
      "for i in range(0,len(uniprots),1000):\n",
      "\n",
      "    with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots[i: i + 1000])) as url:\n",
      "        # Variable 'data' will contain the full database as a nested dictionary\n",
      "        mobidb = json.loads(url.read().decode())\n",
      "    lista_jsons.extend(mobidb)\n",
      "225/122:\n",
      "lista_jsons =  []\n",
      "\n",
      "for i in range(0,len(uniprots),500):\n",
      "\n",
      "    with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/search?acc='+\",\".join(uniprots[i: i + 500])) as url:\n",
      "        # Variable 'data' will contain the full database as a nested dictionary\n",
      "        mobidb = json.loads(url.read().decode())\n",
      "    lista_jsons.extend(mobidb)\n",
      "225/123: len(lisdta_json)\n",
      "225/124: len(lista_json)\n",
      "225/125: len(lista_jsons)\n",
      "225/126: lista_jsons\n",
      "225/127:\n",
      "lista_jsons =  []\n",
      "\n",
      "for i in range(0,len(uniprots),500):\n",
      "\n",
      "    with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500])) as url:\n",
      "        # Variable 'data' will contain the full database as a nested dictionary\n",
      "        mobidb = json.loads(url.read().decode())\n",
      "    lista_jsons.extend(mobidb)\n",
      "228/3: import subprocess\n",
      "228/4: seqs = pd.read_csv('../data/secuencias.csv')\n",
      "228/5: from Bio import SeqIO\n",
      "228/6:\n",
      "for i,row in seqs.iterrows()[:3]:\n",
      "    print(\">\" + row.uniprot, file=open('tmp.fasta','w'))\n",
      "    print(row.seq, file=open('tmp.fasta','a'))\n",
      "228/7:\n",
      "for i,row in seqs[:3].iterrows():\n",
      "    print(\">\" + row.uniprot, file=open('tmp.fasta','w'))\n",
      "    print(row.seq, file=open('tmp.fasta','a'))\n",
      "228/8: !ls\n",
      "228/9: subproces.run([\"ls\"])\n",
      "228/10: subprocess.run([\"ls\"])\n",
      "228/11: subprocess.run([\"ls\"], shell=True)\n",
      "228/12: a = subprocess.run([\"ls\"])\n",
      "228/13: a\n",
      "228/14: a = subprocess.run([\"ls\"], stderr=subprocess.PIPE)\n",
      "228/15: a\n",
      "228/16: a.stderr\n",
      "228/17:\n",
      "for i,row in seqs.iterrows():\n",
      "    print(\">\" + row.uniprot, file=open('tmp.fasta','w'))\n",
      "    print(row.seq, file=open('tmp.fasta','a'))\n",
      "228/18: subprocess.run([\"iupred2a\",'P53_HUMAN.seq','long'])\n",
      "228/19: subprocess.run([\"iupred2a.py\",'P53_HUMAN.seq','long'])\n",
      "228/20: subprocess.run(['python3', \"iupred2a.py\",'P53_HUMAN.seq','long'])\n",
      "228/21: a = subprocess.run(['python3', \"iupred2a.py\",'P53_HUMAN.seq','long'])\n",
      "228/22: a.stdout\n",
      "228/23: a.stderr\n",
      "228/24: a = subprocess.run(['python3', \"iupred2a.py\",'P53_HUMAN.seq','long'], stdout=subprocess.PIPE, shell=True)\n",
      "228/25: a.stdout\n",
      "228/26: a = subprocess.run(['python3', \"iupred2a.py\",'P53_HUMAN.seq','long'], stdout=subprocess.PIPE)\n",
      "228/27: a\n",
      "228/28:\n",
      "a = subprocess.run(['python3', \"iupred2a.py\",'P53_HUMAN.seq','long'],stdout=subprocess.PIPE,\n",
      "                       stderr=subprocess.STDOUT)\n",
      "228/29: a\n",
      "228/30:\n",
      "a = subprocess.run(['python3', \"/home/fernando/iupred2a/iupred2a.py\",'P53_HUMAN.seq','long'],stdout=subprocess.PIPE,\n",
      "                       stderr=subprocess.STDOUT)\n",
      "228/31: a\n",
      "228/32: a\n",
      "228/33: a = subprocess.run(['python3', \"/home/fernando/iupred2a/iupred2a.py\",'P53_HUMAN.seq','long'],stdout=subprocess.PIPE)\n",
      "228/34: a\n",
      "228/35: ! /home/fernando/iupred2a/iupred2a.py P53_HUMAN.seq long\n",
      "228/36: !/home/fernando/iupred2a/iupred2a.py P53_HUMAN.seq long\n",
      "228/37: !/home/fernando/iupred2a/iupred2a.py /home/fernando/iupred2a/P53_HUMAN.seq long\n",
      "228/38:\n",
      "a = subprocess.run(['python3', \"/home/fernando/iupred2a/iupred2a.py\",'/home/fernando/iupred2a/P53_HUMAN.seq','long'],stdout=subprocess.PIPE,\n",
      "                       stderr=subprocess.STDOUT)\n",
      "228/39: a\n",
      "228/40: a.split(\"Prediction output\\n# \")[-1]\n",
      "228/41: a.stdout.split(\"Prediction output\\n# \")[-1]\n",
      "228/42: a\n",
      "228/43: a.\n",
      "228/44: a.stdout\n",
      "228/45: print(a.stdout)\n",
      "228/46: str(a.stdout)\n",
      "228/47: a.stdout\n",
      "228/48: a.stdout.split(\"\")\n",
      "228/49: !/home/fernando/iupred2a/iupred2a.py /home/fernando/iupred2a/P53_HUMAN.seq long\n",
      "228/50: a = !/home/fernando/iupred2a/iupred2a.py /home/fernando/iupred2a/P53_HUMAN.seq long\n",
      "228/51: a\n",
      "228/52: [i.split(\"\\t\") for i in a]\n",
      "228/53: pd.DataFrame([i.split(\"\\t\") for i in a], columns=['pos','res','score'])\n",
      "225/128: prin('LISTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTTOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO')\n",
      "228/54: pd.DataFrame([i.split(\"\\t\") for i in a], columns=['pos','res','score'], comment='#',)\n",
      "228/55: pd.DataFrame([i.split(\"\\t\") for i in a], columns=['pos','res','score'], comment='#')\n",
      "228/56: import StringIO\n",
      "228/57: pd.read_csv(StringIO(a), sep=',', comment='#')\n",
      "228/58: import StringIO\n",
      "228/59: import io.StringIO\n",
      "228/60: from io import StringIO\n",
      "228/61: pd.read_csv(StringIO(a), sep=',', comment='#')\n",
      "228/62: a\n",
      "228/63: pd.read_csv(StringIO([i.split(\"\\t\") for i in a]), sep=',', comment='#')\n",
      "228/64: pd.read_csv(StringIO(a), sep=',', comment='#')\n",
      "228/65: a\n",
      "228/66: type(a)\n",
      "228/67: list((a))\n",
      "228/68: type(list((a)))\n",
      "228/69: pd.DataFrame([i.split(\"\\t\") for i in a if i[0]!= \"#\"], columns=['pos','res','score'])\n",
      "228/70: \"!/home/fernando/iupred2a/iupred2a.py /home/fernando/iupred2a/P53_HUMAN.seq long\"\n",
      "228/71:\n",
      "\"\"\"for i,row in seqs.iterrows():\n",
      "    print(\">\" + row.uniprot, file=open('tmp.fasta','w'))\n",
      "    print(row.seq, file=open('tmp.fasta','a'))\n",
      "    \n",
      "    _ = !/home/fernando/iupred2a/iupred2a.py /home/fernando/iupred2a/tmp.fasta long\"\"\"\n",
      "228/72:\n",
      "dfs = {}\n",
      "for i,row in seqs[:3].iterrows():\n",
      "    print(\">\" + row.uniprot, file=open('tmp.fasta','w'))\n",
      "    print(row.seq, file=open('tmp.fasta','a'))\n",
      "    _ = !/home/fernando/iupred2a/iupred2a.py /home/fernando/iupred2a/tmp.fasta long\n",
      "    dfs[row.uniprot] = pd.DataFrame([i.split(\"\\t\") for i in _ if i[0]!= \"#\"], columns=['pos','res','score'])\n",
      "228/73:\n",
      "dfs = {}\n",
      "for i,row in seqs[:3].iterrows():\n",
      "    print(\">\" + row.uniprot, file=open('tmp.fasta','w'))\n",
      "    print(row.seq, file=open('tmp.fasta','a'))\n",
      "    _ = !/home/fernando/iupred2a/iupred2a.py /home/fernando/iupred2a/tmp.fasta long\n",
      "    print(_)\n",
      "    #dfs[row.uniprot] = pd.DataFrame([i.split(\"\\t\") for i in _ if i[0]!= \"#\"], columns=['pos','res','score'])\n",
      "228/74:\n",
      "dfs = {}\n",
      "for i,row in seqs[:3].iterrows():\n",
      "    print(\">\" + row.uniprot, file=open('tmp.fasta','w'))\n",
      "    print(row.seq, file=open('tmp.fasta','a'))\n",
      "    _ = !/home/fernando/iupred2a/iupred2a.py tmp.fasta long\n",
      "    print(_)\n",
      "    #dfs[row.uniprot] = pd.DataFrame([i.split(\"\\t\") for i in _ if i[0]!= \"#\"], columns=['pos','res','score'])\n",
      "228/75:\n",
      "dfs = {}\n",
      "for i,row in seqs[:3].iterrows():\n",
      "    print(\">\" + row.uniprot, file=open('tmp.fasta','w'))\n",
      "    print(row.seq, file=open('tmp.fasta','a'))\n",
      "    _ = !/home/fernando/iupred2a/iupred2a.py tmp.fasta long\n",
      "    \n",
      "    dfs[row.uniprot] = pd.DataFrame([i.split(\"\\t\") for i in _ if i[0]!= \"#\"], columns=['pos','res','score'])\n",
      "228/76: dfs\n",
      "228/77:\n",
      "dfs = {}\n",
      "for i,row in seqs.iterrows():\n",
      "    print(\">\" + row.uniprot, file=open('tmp.fasta','w'))\n",
      "    print(row.seq, file=open('tmp.fasta','a'))\n",
      "    _ = !/home/fernando/iupred2a/iupred2a.py tmp.fasta long\n",
      "    \n",
      "    dfs[row.uniprot] = pd.DataFrame([i.split(\"\\t\") for i in _ if i[0]!= \"#\"], columns=['pos','res','score'])\n",
      "225/129: len(lista_jsons)\n",
      "225/130: lista_jsons[0]\n",
      "225/131: pd.read_json(lista_jsons[0])\n",
      "225/132: pd.read_dict(lista_jsons[0])\n",
      "225/133: pd.DataFrame(lista_jsons[:10])\n",
      "225/134: pd.DataFrame(lista_jsons[:10]).columns\n",
      "225/135: lista_jsons[0]\n",
      "225/136: lista_jsons[0].keys()\n",
      "225/137: pd.Series(lista_jsons[0])\n",
      "225/138: pd.Series(lista_jsons[0]).index.tolisdt()\n",
      "225/139: pd.Series(lista_jsons[0]).index.tolist()\n",
      "225/140:\n",
      "a = pd.DataFrame(([[i['acc'], i['prediction-disorder-mobidb_lite']] for i in lista_jsons]))\n",
      "a\n",
      "225/141: [i for i in lista_jsons if i.haskey(\"prediction-disorder-mobidb_lite\")]\n",
      "225/142:\n",
      "a = pd.DataFrame(([[i['acc'], i.get('prediction-disorder-mobidb_lite')] for i in lista_jsons]))\n",
      "a\n",
      "225/143: a = lista_jsons[0]\n",
      "225/144: a.keys()\n",
      "225/145: a.get(\"prediction-disorder-mobidb_lite\")\n",
      "225/146:\n",
      "lista = []\n",
      "for proteina in lista_jsons:\n",
      "    if \"prediction-disorder-mobidb_lite\" in lista_jsons.keys():\n",
      "        lista.append([proteina['acc'], proteina[\"prediction-disorder-mobidb_lite\"][\"content_fraction\"]])\n",
      "225/147:\n",
      "lista = []\n",
      "for proteina in lista_jsons:\n",
      "    if \"prediction-disorder-mobidb_lite\" in proteina.keys():\n",
      "        lista.append([proteina['acc'], proteina[\"prediction-disorder-mobidb_lite\"][\"content_fraction\"]])\n",
      "225/148: proteina.keys()\n",
      "225/149: proteina[\"prediction-disorder-mobidb_lite\"].keys()\n",
      "225/150: proteina[\"prediction-disorder-mobidb_lite\"][\"scores\"].keys()\n",
      "225/151: proteina[\"prediction-disorder-mobidb_lite\"][\"scores\"]\n",
      "225/152: entrada\n",
      "225/153: entrada.columns\n",
      "225/154: entrada.loc[0]\n",
      "225/155: disorder = pd.read_csv('disorder_phasepro.csv')\n",
      "225/156: disorder = pd.read_csv('../data/disorder_phasepro.csv')\n",
      "225/157: !ls\n",
      "225/158: disorder = pd.read_csv('../data/disorder_phasepro.tsv', sep='\\t')\n",
      "225/159: disorder\n",
      "225/160: disorder.content_fraction.describe()\n",
      "230/1:\n",
      "#with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"projection=prediction-disorder-mobidb_lite&format=tsv\" as url:\n",
      "with urllib.request.urlopen(\"https://mobidb.bio.unipd.it/api/download?acc=P16554&projection=prediction-disorder-mobidb_lite&format=tsv\") as url:\n",
      "    pd.read_csv(url.read().decode())\n",
      "230/2: entrada.assign(llps_driver= lambda x: x.segment.str.split(\"; \")).pipe(lambda x: x.explode('llps_driver')).query('llps_driver == \"Low complexity region with Pro-Xn-Gly motifs\"')\n",
      "230/3:\n",
      "#with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"projection=prediction-disorder-mobidb_lite&format=tsv\" as url:\n",
      "with urllib.request.urlopen(\"https://mobidb.bio.unipd.it/api/download?acc=P16554&projection=prediction-disorder-mobidb_lite&format=tsv\") as url:\n",
      "    pd.read_csv(url.read().decode())\n",
      "230/4:\n",
      "import urllib\n",
      "#with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"projection=prediction-disorder-mobidb_lite&format=tsv\" as url:\n",
      "with urllib.request.urlopen(\"https://mobidb.bio.unipd.it/api/download?acc=P16554&projection=prediction-disorder-mobidb_lite&format=tsv\") as url:\n",
      "    pd.read_csv(url.read().decode())\n",
      "230/5:\n",
      "import urllib\n",
      "import pandas as pd\n",
      "#with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"projection=prediction-disorder-mobidb_lite&format=tsv\" as url:\n",
      "with urllib.request.urlopen(\"https://mobidb.bio.unipd.it/api/download?acc=P16554&projection=prediction-disorder-mobidb_lite&format=tsv\") as url:\n",
      "    pd.read_csv(url.read().decode())\n",
      "230/6:\n",
      "import urllib\n",
      "import pandas as pd\n",
      "from io import StringIO\n",
      "#with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"projection=prediction-disorder-mobidb_lite&format=tsv\" as url:\n",
      "with urllib.request.urlopen(\"https://mobidb.bio.unipd.it/api/download?acc=P16554&projection=prediction-disorder-mobidb_lite&format=tsv\") as url:\n",
      "    pd.read_csv(StringIO(url.read().decode()))\n",
      "230/7:\n",
      "import urllib\n",
      "import pandas as pd\n",
      "from io import StringIO\n",
      "#with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"projection=prediction-disorder-mobidb_lite&format=tsv\" as url:\n",
      "with urllib.request.urlopen(\"https://mobidb.bio.unipd.it/api/download?acc=P16554&projection=prediction-disorder-mobidb_lite&format=tsv\") as url:\n",
      "    a = pd.read_csv(StringIO(url.read().decode()))\n",
      "230/8: a\n",
      "230/9:\n",
      "import urllib\n",
      "import pandas as pd\n",
      "from io import StringIO\n",
      "#with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"projection=prediction-disorder-mobidb_lite&format=tsv\" as url:\n",
      "with urllib.request.urlopen(\"https://mobidb.bio.unipd.it/api/download?acc=P16554&projection=prediction-disorder-mobidb_lite&format=tsv\") as url:\n",
      "    a = pd.read_csv(StringIO(url.read().decode()), sep='\\t')\n",
      "230/10: a\n",
      "230/11:\n",
      "import urllib\n",
      "import pandas as pd\n",
      "from io import StringIO\n",
      "#with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"projection=prediction-disorder-mobidb_lite&format=tsv\" as url:\n",
      "with urllib.request.urlopen(\"https://mobidb.bio.unipd.it/api/download?acc=P16554&format=tsv\") as url:\n",
      "    a = pd.read_csv(StringIO(url.read().decode()), sep='\\t')\n",
      "230/12: a\n",
      "230/13:\n",
      "\n",
      "ortologos = pd.read_csv('../data/ortologos.csv')\n",
      "230/14: uniprots = entrada.uniprot.unique().tolist()\n",
      "230/15: uniprots.extend(ortologos.orthologs.unique().tolist())\n",
      "230/16: entrada = pd.read_csv('phasepro_latest_db.csv')\n",
      "230/17: entrada.uniprot = entrada.uniprot.str.split(\"-\").str[0]\n",
      "230/18:\n",
      "\n",
      "ortologos = pd.read_csv('../data/ortologos.csv')\n",
      "230/19: uniprots = entrada.uniprot.unique().tolist()\n",
      "230/20: uniprots.extend(ortologos.orthologs.unique().tolist())\n",
      "230/21:\n",
      "for i in range(0,500,500):\n",
      "    print(i)\n",
      "230/22:\n",
      "for i in range(0,len(uniprots),500):\n",
      "    with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"projection=prediction-disorder-mobidb_lite&format=tsv\" as url:\n",
      "        # Variable 'data' will contain the full database as a nested dictionary\n",
      "        mobidb = json.loads(url.read().decode())\n",
      "    lista_jsons.extend(mobidb)\n",
      "230/23:\n",
      "for i in range(0,1000,500):\n",
      "    print(i)\n",
      "230/24:\n",
      "tablas =  []\n",
      "\n",
      "for i in range(0,1000,500):\n",
      "'https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"&projection=prediction-disorder-mobidb_lite&format=tsv\"\n",
      "    with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500])) as url:\n",
      "        # Variable 'data' will contain the full database as a nested dictionary\n",
      "        tablas.append(pd.read_csv(StringIO(url.read().decode()), sep='\\t'))\n",
      "230/25:\n",
      "tablas =  []\n",
      "\n",
      "for i in range(0,1000,500):\n",
      "\n",
      "    with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500]))+\"&format=tsv\") as url:\n",
      "        # Variable 'data' will contain the full database as a nested dictionary\n",
      "        tablas.append(pd.read_csv(StringIO(url.read().decode()), sep='\\t'))\n",
      "230/26:\n",
      "tablas =  []\n",
      "\n",
      "for i in range(0,1000,500):\n",
      "\n",
      "    with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500])+\"&format=tsv\") as url:\n",
      "        # Variable 'data' will contain the full database as a nested dictionary\n",
      "        tablas.append(pd.read_csv(StringIO(url.read().decode()), sep='\\t'))\n",
      "230/27: tablas[0]\n",
      "230/28:\n",
      "tablas =  []\n",
      "\n",
      "for i in range(0,len(uniprots),500):\n",
      "    print(i)\n",
      "    with urllib.request.urlopen('https://mobidb.bio.unipd.it/api/download?acc='+\",\".join(uniprots[i: i + 500])+\"&format=tsv\") as url:\n",
      "        # Variable 'data' will contain the full database as a nested dictionary\n",
      "        tablas.append(pd.read_csv(StringIO(url.read().decode()), sep='\\t'))\n",
      "230/29:\n",
      "disorder = pd.concat(tablas)\n",
      "print(len(disorder))\n",
      "print(len(disorder.acc.unique()))\n",
      "230/30: disorder\n",
      "230/31: disorder.feature.value_counts()\n",
      "230/32: disorder.feature.value_counts()[:30]\n",
      "230/33: disorder.query('feaure == \"prediction-disorder-mobidb_lite\"')\n",
      "230/34: disorder.query('feature == \"prediction-disorder-mobidb_lite\"')\n",
      "230/35: dis = disorder.query('feature == \"prediction-disorder-mobidb_lite\"')\n",
      "230/36: dis\n",
      "230/37: dis[['acc','start..end']]\n",
      "230/38: dis[['acc','start..end']].explode('start..end')\n",
      "230/39: zonas = dis[['acc','start..end']].assign(pos = lambda x: x['start..end'].split(\",\"))\n",
      "230/40: zonas = dis[['acc','start..end']].assign(pos = lambda x: x['start..end'].str.split(\",\"))\n",
      "230/41: zonas\n",
      "230/42: zonas.explode('pos')\n",
      "230/43: zonas.explode('pos')[['acc','pos']]\n",
      "230/44: zonas.explode('pos')[['acc','pos']].str.split('..', expand=True)\n",
      "230/45: zonas.explode('pos')[['acc','pos']].pos.str.split('..', expand=True)\n",
      "230/46: zonas.explode('pos')[['acc','pos']]\n",
      "230/47: zonas.explode('pos')[['acc','pos']].pos.str.split(\"..\")\n",
      "230/48: zonas.explode('pos')[['acc','pos']].pos.str.replace('..','_')\n",
      "230/49: zonas.explode('pos')[['acc','pos']]#.pos.str.replace('..','_')\n",
      "230/50: zonas.explode('pos')[['acc','pos']].pos.map(lambda x: x.replace(r'..','-')\n",
      "230/51: zonas.explode('pos')[['acc','pos']].pos.map(lambda x: x.replace(r'..','-'))\n",
      "230/52: zonas.explode('pos')[['acc','pos']].pos.str.replace(r'..','_')#.map(lambda x: x.replace(r'..','-'))\n",
      "230/53: zonas.explode('pos')[['acc','pos']].pos.map(lambda x: x.replace(r'..','-'))\n",
      "230/54:\n",
      "zonas = zonas.explode('pos')[['acc','pos']]\n",
      "zonas.pos = zonas.pos.map(lambda x: x.replace(r'..','-'))\n",
      "230/55: zonas\n",
      "230/56: zonas[[\"start\",'end']] = zonas.pos.str.split(\"-\",expand=True)\n",
      "230/57: zonas\n",
      "230/58: zonas\n",
      "230/59: zonas\n",
      "230/60: zonas[['acc','start','end']].rename(columns = {'acc':'uniprot'})\n",
      "230/61: zonas = zonas[['acc','start','end']].rename(columns = {'acc':'uniprot'})\n",
      "230/62: zonas.to_csv('../data/zonas_desordenadas.csv', index= False)\n",
      "230/63: from Bio import AlignIO\n",
      "230/64: ali = AlignIO.read('../secuencias/fus_ort_aligned.fasta')\n",
      "230/65: ali = AlignIO.read('../secuencias/fus_ort_aligned.fasta', 'fasta')\n",
      "230/66: ali\n",
      "230/67: print(ali)\n",
      "230/68: ali = AlignIO.read('alineamiento_desorden_fus.fasta', 'fasta')\n",
      "230/69: ali = AlignIO.read('../secuencias/alineamiento_desorden_fus.fasta', 'fasta')\n",
      "230/70: print(ali)\n",
      "230/71: fus = [i.seq for i in ali if i.id == \"P35637\"][0]\n",
      "230/72: fus\n",
      "230/73: str(fus)\n",
      "230/74: fus_seq = str(fus)\n",
      "230/75: map_features(ali,fus_seq)\n",
      "230/76:\n",
      "from funciones import map_feature\n",
      "map_feature(ali,fus_seq)\n",
      "230/77:\n",
      "fus_inali_seq = str(fus)\n",
      "fus_raw_seq = \"\".join([i for i in fus_inali_seq if i != \"-\"])\n",
      "230/78:\n",
      "from funciones import map_feature\n",
      "map_feature(fus_raw_seq,fus_in_ali)\n",
      "230/79:\n",
      "from funciones import map_feature\n",
      "map_feature(fus_raw_seq,fus_inali_seq)\n",
      "230/80:\n",
      "from funciones import map_feature\n",
      "fus_map = map_feature(fus_raw_seq,fus_inali_seq)\n",
      "230/81: fus_map\n",
      "230/82: fus_map\n",
      "230/83: fus_map.keys()\n",
      "230/84: fus_map[\"in_seq_pos\"]\n",
      "230/85: fus_map[\"in_seq_pos\"][1]\n",
      "230/86: fus_map[\"in_seq_pos\"][348]\n",
      "230/87: fus_map[\"in_seq_pos\"][526]\n",
      "230/88: disorder.query('uniprot == \"P35637\"')\n",
      "230/89: zonas.query('uniprot == \"P35637\"')\n",
      "230/90:\n",
      "for i,row in zonas.query('uniprot == \"P35637\"').iterrows():\n",
      "    new_start = fus_map[\"in_seq_pos\"][row.start]\n",
      "    new_end = fus_map[\"in_seq_pos\"][row.end]\n",
      "    print(row.start,row.end, new_start, new_end)\n",
      "230/91: fus_map[\"in_seq_pos\"][1]\n",
      "230/92:\n",
      "for i,row in zonas.query('uniprot == \"P35637\"').iterrows():\n",
      "    new_start = fus_map[\"in_seq_pos\"][int(row.start)]\n",
      "    new_end = fus_map[\"in_seq_pos\"][int(row.end)]\n",
      "    print(row.start,row.end, new_start, new_end)\n",
      "230/93:\n",
      "ali_des = AlignIO.read('../secuencias/alineamiento_desorden_fus.fasta', 'fasta')\n",
      "ali_ord = AlignIO.read('../secuencias/fus_ort_aligned.fasta.fasta', 'fasta')\n",
      "230/94:\n",
      "ali_des = AlignIO.read('../secuencias/alineamiento_desorden_fus.fasta', 'fasta')\n",
      "ali_ord = AlignIO.read('../secuencias/fus_ort_aligned.fasta', 'fasta')\n",
      "230/95: ali_ord\n",
      "230/96: ali_des\n",
      "230/97: zonas_query('uniprot == \"P35637\"')\n",
      "230/98: zonas:query('uniprot == \"P35637\"')\n",
      "230/99: zonas.query('uniprot == \"P35637\"')\n",
      "230/100: for i in len(zonas):print(i)\n",
      "230/101: for i in range(len(zonas.query('uniprot == \"P35637\"'))):print(i)\n",
      "230/102: fus_zonas = zonas.query('uniprot == \"P35637\"')\n",
      "230/103:\n",
      "for i in range(len(fus_zonas)):\n",
      "    print()\n",
      "230/104: fus_zonas\n",
      "230/105:\n",
      "zonas_desordenadas\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_desordenadas.append([\"P35637\", fus_zonas.iloc[i,3]+ 1, fus_zonas[i + 1, 2] -1])\n",
      "230/106:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,3]+ 1, fus_zonas[i + 1, 2] -1])\n",
      "230/107:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2+ 1, fus_zonas[i + 1, 1] -1])\n",
      "230/108:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2+ 1, fus_zonas.iloc[i + 1, 1] -1])\n",
      "230/109:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1])\n",
      "230/110:\n",
      "fus_zonas = zonas.query('uniprot == \"P35637\"')\n",
      "fus_zonas.start = fus_zonas.start.astype(int)\n",
      "fus_zonas.end = fus_zonas.end.astype(int)\n",
      "230/111:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1])\n",
      "230/112: fus_zonas.append(zonas_ordenaddas)\n",
      "230/113: fus_zonas.append(zonas_ordenadas)\n",
      "230/114: fus_zonas.append(zonas_ordenadas,axis=1)\n",
      "230/115: fus_zonas.concat(zonas_ordenadas)\n",
      "230/116: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns='uniprot','start','end'))\n",
      "230/117: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end']))\n",
      "230/118: disorder\n",
      "230/119: disorder.set_index('acc')['lenght']\n",
      "230/120: thht']\n",
      "230/121: disorder.set_index('acc')['length']\n",
      "230/122: disorder.set_index('acc')['length']['acc']\n",
      "230/123: disorder.set_index('acc')['length'].to_dict()\n",
      "230/124: largos = disorder.set_index('acc')['length'].to_dict()\n",
      "230/125:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1])\n",
      "    if fus_zonas[-1,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[-i,2]+ 1, largos['P35637'])\n",
      "230/126:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1])\n",
      "    if fus_zonas[-1,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[-i,2]+ 1, largos['P35637']])\n",
      "230/127:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1])\n",
      "    if fus_zonas.iloc[-1,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[-i,2]+ 1, largos['P35637']])\n",
      "230/128: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end']))\n",
      "230/129: largos['P35637']\n",
      "230/130: largos['P35637'] = 600\n",
      "230/131:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1])\n",
      "    if fus_zonas.iloc[-1,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[-i,2]+ 1, largos['P35637']])\n",
      "230/132: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end']))\n",
      "230/133: largos = disorder.set_index('acc')['length'].to_dict()\n",
      "230/134: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end']))\n",
      "230/135:\n",
      "fus_zonas = zonas.query('uniprot == \"P35637\"')\n",
      "fus_zonas.start = fus_zonas.start.astype(int)\n",
      "fus_zonas.end = fus_zonas.end.astype(int)\n",
      "230/136: fus_zonas['tipo'] = 'd'\n",
      "230/137:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1,'s'])\n",
      "    if fus_zonas.iloc[-1,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[-i,2]+ 1, largos['P35637'],'s'])\n",
      "230/138: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end']))\n",
      "230/139: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo']))\n",
      "230/140:\n",
      "fus_zonas = zonas.query('uniprot == \"P35637\"')\n",
      "fus_zonas.start = fus_zonas.start.astype(int)\n",
      "fus_zonas.end = fus_zonas.end.astype(int)\n",
      "230/141: fus_zonas['tipo'] = 'd'\n",
      "230/142: largos = disorder.set_index('acc')['length'].to_dict()\n",
      "230/143:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1,'s'])\n",
      "    if fus_zonas.iloc[-1,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[-i,2]+ 1, largos['P35637'],'s'])\n",
      "230/144: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo']))\n",
      "230/145: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo'])).sort_values('start')\n",
      "230/146:\n",
      "fus_zonas = zonas.query('uniprot == \"P35637\"')\n",
      "fus_zonas.start = fus_zonas.start.astype(int)\n",
      "fus_zonas.end = fus_zonas.end.astype(int)\n",
      "230/147: fus_zonas['tipo'] = 'd'\n",
      "230/148: largos = disorder.set_index('acc')['length'].to_dict()\n",
      "230/149:\n",
      "zonas_ordenadas = []\n",
      "for i in range(len(fus_zonas) - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1,'s'])\n",
      "    if fus_zonas.iloc[-1,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[-1,2]+ 1, largos['P35637'],'s'])\n",
      "230/150: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo'])).sort_values('start')\n",
      "230/151:\n",
      "zonas_ordenadas = []\n",
      "n = len(fus_zonas)\n",
      "for i in range(n - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1,'s'])\n",
      "    if fus_zonas.iloc[n,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[n,2]+ 1, largos['P35637'],'s'])\n",
      "230/152: n\n",
      "230/153: fus_zonas\n",
      "230/154:\n",
      "zonas_ordenadas = []\n",
      "n = len(fus_zonas)\n",
      "for i in range(n - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1,'s'])\n",
      "    if fus_zonas.iloc[n - 1 ,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[n - 1 ,2]+ 1, largos['P35637'],'s'])\n",
      "230/155: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo'])).sort_values('start')\n",
      "230/156:\n",
      "fus_zonas = zonas.query('uniprot == \"P35637\"')\n",
      "fus_zonas.start = fus_zonas.start.astype(int)\n",
      "fus_zonas.end = fus_zonas.end.astype(int)\n",
      "230/157: fus_zonas['tipo'] = 'd'\n",
      "230/158: largos = disorder.set_index('acc')['length'].to_dict()\n",
      "230/159:\n",
      "zonas_ordenadas = []\n",
      "n = len(fus_zonas)\n",
      "for i in range(n - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1,'s'])\n",
      "    if fus_zonas.iloc[n - 1 ,-1] != largos['P35637']:\n",
      "        zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[n - 1 ,2]+ 1, largos['P35637'],'s'])\n",
      "230/160: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo'])).sort_values('start')\n",
      "230/161:\n",
      "zonas_ordenadas = []\n",
      "n = len(fus_zonas)\n",
      "for i in range(n - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1,'s'])\n",
      "    #if fus_zonas.iloc[n - 1 ,-1] != largos['P35637']:\n",
      "    #    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[n - 1 ,2]+ 1, largos['P35637'],'s'])\n",
      "230/162: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo'])).sort_values('start')\n",
      "230/163: fus_zonas.reset_index(drop=True)\n",
      "230/164: fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo'])).sort_values('start').reset_index(drop=True)\n",
      "230/165: fus_zonas = fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo'])).sort_values('start').reset_index(drop=True)\n",
      "230/166: fus_zonas\n",
      "230/167: fus_zonas.iloc[-1,-1]\n",
      "230/168: if fus_zonas.iloc[-1,-2]\n",
      "230/169: fus_zonas.iloc[-1,-2]\n",
      "230/170:\n",
      "if fus_zonas.iloc[-1,-2] != largos['P35637']:\n",
      "    fus_zonas[len(fus_zonas)] = ['P35637', fus_zonas.iloc[-1,-2] + 1, largos['P35637'], 's']\n",
      "230/171: fus_zonas\n",
      "230/172:\n",
      "fir i,row in fus_zonas.iterrows():\n",
      "    print(i)\n",
      "230/173:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i)\n",
      "230/174: ali_dis\n",
      "230/175: ali_des\n",
      "230/176: fus_alis = {'d':ali_dis, 's':ali_ord}\n",
      "230/177: fus_alis = {'d':ali_des, 's':ali_ord}\n",
      "230/178:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/179: ali\n",
      "230/180: ali_des\n",
      "230/181: ali_ord\n",
      "230/182: ali\n",
      "230/183:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, :row.end + 1 ]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/184: ali\n",
      "230/185: print(ali)\n",
      "230/186: print(ali_des)\n",
      "230/187: print(ali_ord)\n",
      "230/188: ali_ord.sort()\n",
      "230/189: ali_ord\n",
      "230/190: print(ali_ord)\n",
      "230/191: ali_des.sort()\n",
      "230/192:\n",
      "for i,e in zip(ali_ord, ali_des):\n",
      "    print(i.id, e.id)\n",
      "230/193:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, :row.end + 1 ]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/194: ali\n",
      "230/195: print(ali_des)\n",
      "230/196: print(ali)\n",
      "230/197:\n",
      "for seq in ali:\n",
      "    print(\">\" + seq.id)\n",
      "    print(str(seq.seq))\n",
      "230/198: zonas_fus\n",
      "230/199: fus_zonas\n",
      "230/200:\n",
      "fus_zonas['start_fus'] = fus_zonas.start\n",
      "fus_zonas['start_end'] = fus_zonas.end\n",
      "230/201:\n",
      "fus_zonas['end']= fus_zonas.end.map(fus_map[\"in_seq_pos\"])\n",
      "fus_zonas['start']= fus_zonas.start.map(fus_map[\"in_seq_pos\"])\n",
      "230/202:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, :row.end + 1 ]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/203: print(ali)\n",
      "230/204:\n",
      "for i in ali:\n",
      "    print(\">\" + i.id)\n",
      "    print(i.seq)\n",
      "230/205: fus_zonas\n",
      "230/206:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, :row.end + 1 ]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/207:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/208:\n",
      "for i in ali:\n",
      "    print(\">\" + i.id)\n",
      "    print(i.seq)\n",
      "230/209: ali_Des = AlignIO.read('../secuencias/alineamiento_desorden_fus.fasta', 'fasta')\n",
      "230/210: ali_des = AlignIO.read('../secuencias/alineamiento_desorden_fus.fasta', 'fasta')\n",
      "230/211:\n",
      "ali_des = AlignIO.read('../secuencias/alineamiento_desorden_fus.fasta', 'fasta')\n",
      "ali_des = AlignIO.read('../secuencias/fus_ort_aligned.fasta', 'fasta')\n",
      "230/212: fus = [i.seq for i in ali if i.id == \"P35637\"][0]\n",
      "230/213:\n",
      "ali_des = AlignIO.read('../secuencias/alineamiento_desorden_fus.fasta', 'fasta')\n",
      "ali_ord = AlignIO.read('../secuencias/fus_ort_aligned.fasta', 'fasta')\n",
      "230/214: fus = [i.seq for i in ali if i.id == \"P35637\"][0]\n",
      "230/215:\n",
      "fus_des = [i.seq for i in ali_des if i.id == \"P35637\"][0]\n",
      "fus_ord = [i.seq for i in ali_ord if i.id == \"P35637\"][0]\n",
      "230/216:\n",
      "fus_inali_seq_des = str(fus_des)\n",
      "fus_inali_seq_ord = str(fus_ord)\n",
      "fus_raw_seq = \"\".join([i for i in fus_inali_seq if i != \"-\"])\n",
      "230/217:\n",
      "from funciones import map_feature\n",
      "fus_map_des = map_feature(fus_raw_seq,fus_inali_seq_des)\n",
      "fus_map_ord = map_feature(fus_raw_seq,fus_inali_seq_ord)\n",
      "230/218: mapeo = {'s':fus_map_ord,'d':fus_map_des}\n",
      "230/219:\n",
      "fus_zonas = zonas.query('uniprot == \"P35637\"')\n",
      "fus_zonas.start = fus_zonas.start.astype(int)\n",
      "fus_zonas.end = fus_zonas.end.astype(int)\n",
      "230/220: fus_zonas['tipo'] = 'd'\n",
      "230/221: largos = disorder.set_index('acc')['length'].to_dict()\n",
      "230/222:\n",
      "zonas_ordenadas = []\n",
      "n = len(fus_zonas)\n",
      "for i in range(n - 1):\n",
      "    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[i,2]+ 1, fus_zonas.iloc[i + 1, 1] -1,'s'])\n",
      "    #if fus_zonas.iloc[n - 1 ,-1] != largos['P35637']:\n",
      "    #    zonas_ordenadas.append([\"P35637\", fus_zonas.iloc[n - 1 ,2]+ 1, largos['P35637'],'s'])\n",
      "230/223: fus_zonas = fus_zonas.append(pd.DataFrame(zonas_ordenadas, columns=['uniprot','start','end','tipo'])).sort_values('start').reset_index(drop=True)\n",
      "230/224:\n",
      "if fus_zonas.iloc[-1,-2] != largos['P35637']:\n",
      "    fus_zonas[len(fus_zonas)] = ['P35637', fus_zonas.iloc[-1,-2] + 1, largos['P35637'], 's']\n",
      "230/225:\n",
      "fus_zonas['start_fus'] = fus_zonas.start\n",
      "fus_zonas['end_fus'] = fus_zonas.end\n",
      "230/226:\n",
      "fus_zonas['end']= fus_zonas.apply(lambda x: mapeo[x.tipo][\"in_seq_pos\"][x.start_fus])\n",
      "fus_zonas['start']= fus_zonas.apply(lambda x: mapeo[x.tipo][\"in_seq_pos\"][x.end_fus])\n",
      "230/227: fus_zonas\n",
      "230/228:\n",
      "fus_zonas['end']= fus_zonas.apply(lambda x: mapeo[x.tipo][\"in_seq_pos\"][x.start_fus],1)\n",
      "fus_zonas['start']= fus_zonas.apply(lambda x: mapeo[x.tipo][\"in_seq_pos\"][x.end_fus],1)\n",
      "230/229: fus_alis = {'d':ali_des, 's':ali_ord}\n",
      "230/230:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/231: fus_zonas\n",
      "230/232:\n",
      "fus_zonas['start']= fus_zonas.apply(lambda x: mapeo[x.tipo][\"in_seq_pos\"][x.start_fus],1)\n",
      "fus_zonas['end']= fus_zonas.apply(lambda x: mapeo[x.tipo][\"in_seq_pos\"][x.end_fus],1)\n",
      "230/233: fus_alis = {'d':ali_des, 's':ali_ord}\n",
      "230/234:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/235: fus_zonas\n",
      "230/236:\n",
      "for i in ali:\n",
      "    print(\">\" + i.id)\n",
      "    print(i.seq)\n",
      "230/237: ali_des\n",
      "230/238: dir(ali_des)\n",
      "230/239:\n",
      "ali_des = AlignIO.read('../secuencias/alineamiento_desorden_fus.fasta', 'fasta')\n",
      "ali_ord = AlignIO.read('../secuencias/fus_ort_aligned.fasta', 'fasta')\n",
      "230/240:\n",
      "ali_des = AlignIO.read('../secuencias/alineamiento_desorden_fus.fasta', 'fasta')\n",
      "ali_ord = AlignIO.read('../secuencias/fus_ort_aligned.fasta', 'fasta')\n",
      "230/241:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/242:\n",
      "for i in ali:\n",
      "    print(\">\" + i.id)\n",
      "    print(i.seq)\n",
      "230/243: fus_alis = {'d':ali_des, 's':ali_ord}\n",
      "230/244:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "    ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/245: fus_zonas\n",
      "230/246:\n",
      "for i in ali:\n",
      "    print(\">\" + i.id)\n",
      "    print(i.seq)\n",
      "230/247:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/248:\n",
      "for i in ali:\n",
      "    print(\">\" + i.id)\n",
      "    print(i.seq)\n",
      "230/249: print(ali)\n",
      "230/250: print(ali_ord)\n",
      "230/251: sorted(print(ali_ord))\n",
      "230/252: print(sorted(ali_ord))\n",
      "230/253:\n",
      "ali_ord.sort()\n",
      "ali_des.sort()\n",
      "230/254:\n",
      "for i in ali:\n",
      "    print(\">\" + i.id)\n",
      "    print(i.seq)\n",
      "230/255:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/256:\n",
      "for i in ali:\n",
      "    print(\">\" + i.id)\n",
      "    print(i.seq)\n",
      "230/257: [i for i in ali if i.id ==\"P35637\"]\n",
      "230/258: [i for i in ali if i.id ==\"P35637\"][0]\n",
      "230/259: str([i for i in ali if i.id ==\"P35637\"][0])\n",
      "230/260: str([i.id for i in ali if i.id ==\"P35637\"][0])\n",
      "230/261: str([i.seq for i in ali if i.id ==\"P35637\"][0])\n",
      "230/262: fus_raw_seq\n",
      "230/263: fus_raw_seq[1:]\n",
      "230/264: \"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"])\n",
      "230/265: \"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]) == fus_raw_seq[1:]\n",
      "230/266: \"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"])# == fus_raw_seq[1:]\n",
      "230/267:\n",
      "for i, e in zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq[1:]):\n",
      "    print(i,e, i == e)\n",
      "230/268:\n",
      "for c(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq[1:])):\n",
      "    print(c,i,e, i == e)\n",
      "230/269:\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq[1:])):\n",
      "    print(c,i,e, i == e)\n",
      "230/270:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "230/271:\n",
      "for i in ali:\n",
      "    print(\">\" + i.id)\n",
      "    print(i.seq)\n",
      "230/272:\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq[1:])):\n",
      "    print(c,i,e, i == e)\n",
      "230/273:\n",
      "for i in ali:\n",
      "    if i.id == \"P35637\":\n",
      "        print(\">\" + i.id)\n",
      "        print(i.seq)\n",
      "230/274:\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "    print(c,i,e, i == e)\n",
      "230/275:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/276:\n",
      "for i in ali:\n",
      "    if i.id == \"P35637\":\n",
      "        print(\">\" + i.id)\n",
      "        print(i.seq)\n",
      "230/277:\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "    print(c,i,e, i == e)\n",
      "230/278:\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "    if i != e:\n",
      "        print(c,i,e, i == e)\n",
      "230/279:\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "    print(\"pos_fus\",'new_ali','raw_seq')\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/280:\n",
      "    print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/281:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/282: fus_zonas\n",
      "230/283: ali_des\n",
      "230/284: ali_des[0,578]\n",
      "230/285: [i.seq[578] for i in ali if i.id == \"P35637\"]\n",
      "230/286: [i.seq[570:580] for i in ali if i.id == \"P35637\"]\n",
      "230/287: [i.seq[570:580] for i in ali_des if i.id == \"P35637\"]\n",
      "230/288: [i.seq[578] for i in ali_des if i.id == \"P35637\"]\n",
      "230/289: [i.seq[577] for i in ali_des if i.id == \"P35637\"]\n",
      "230/290:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "230/291:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/292:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start-2:row.end + 1 ]\n",
      "230/293:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/294:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start:row.end + 1 ]\n",
      "230/295: cod = {'s':0, 'd':1}\n",
      "230/296:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - cod[row.tipo]:row.end + 1 ]\n",
      "230/297:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/298: cod = {'s':1, 'd':1}\n",
      "230/299:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/300:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - cod[row.tipo]:row.end + 1 ]\n",
      "230/301:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/302:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - cod[row.tipo]:row.end + 1 ]\n",
      "230/303:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/304: cod = {'s':0, 'd':1}\n",
      "230/305:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/306:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - cod[row.tipo]:row.end + 1 ]\n",
      "230/307:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/308:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - cod[row.tipo]:row.end + 1 ]\n",
      "230/309:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/310:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - cod[row.tipo]:row.end + 1 ]\n",
      "230/311:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/312: cod = {'s':2, 'd':1}\n",
      "230/313:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/314:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - cod[row.tipo]:row.end + 1 ]\n",
      "230/315:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/316:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/317: ali_des[0,578]\n",
      "230/318: cod = {'s':0, 'd':1}\n",
      "230/319:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - cod[row.tipo]:row.end + 1 ]\n",
      "230/320:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/321: fus_zonas\n",
      "230/322:\n",
      "for i in ali_ord:\n",
      "    if i.id == \"P35637\":\n",
      "        print(i.seq[902])\n",
      "230/323:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 2 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - 1:row.end + 2 ]\n",
      "230/324:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/325:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - 1:row.end + 1 ]\n",
      "230/326:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/327:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end + 1 ]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - 1:row.end + 1 ]\n",
      "230/328:\n",
      "for i in ali_ord:\n",
      "    if i.id == \"P35637\":\n",
      "        print(i.seq[902])\n",
      "230/329:\n",
      "for i in ali_ord:\n",
      "    if i.id == \"P35637\":\n",
      "        print(i.seq[902 + 1])\n",
      "230/330:\n",
      "for i in ali_ord:\n",
      "    if i.id == \"P35637\":\n",
      "        print(i.seq[902 -1])\n",
      "230/331:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "230/332:\n",
      "for i,row in fus_zonas.iterrows():\n",
      "    print(i, row.start, row.end,)\n",
      "    if i == 0:\n",
      "        ali = fus_alis[row.tipo][:, row.start-1:row.end]\n",
      "    else:\n",
      "        ali = ali + fus_alis[row.tipo][:, row.start - 1:row.end  ]\n",
      "230/333:\n",
      "   print(\"pos_fus\",'new_ali','raw_seq')\n",
      "for c,(i, e) in enumerate(zip(\"\".join([a for a in str([i.seq for i in ali if i.id ==\"P35637\"][0]) if a != \"-\"]),fus_raw_seq)):\n",
      "\n",
      "    if i != e:\n",
      "        \n",
      "        print(c,i,e, i == e)\n",
      "235/1:\n",
      "import time\n",
      "import requests\n",
      "import pandas as pd\n",
      "import json\n",
      "235/2: from uniprot import batch_uniprot, columns\n",
      "235/3:\n",
      "#with urllib.request.urlopen(\"https://phasepro.elte.hu/download_full.json\") as url:\n",
      "#    # Variable 'data' will contain the full database as a nested dictionary\n",
      "#    data = json.loads(url.read().decode())\n",
      "#entrada = pd.DataFrame.from_dict(data, orient='index').rename_axis(\"uniprot\").reset_index()\n",
      "#entrada.to_csv('phasepro_latest_db.csv', index=False)\n",
      "235/4: entrada = pd.read_csv('phasepro_latest_db.csv')\n",
      "235/5: entrada.uniprot = entrada.uniprot.str.split(\"-\").str[0]\n",
      "235/6:\n",
      "\n",
      "ortologos = pd.read_csv('../data/ortologos.csv')\n",
      "235/7: uniprots = entrada.uniprot.unique().tolist()\n",
      "235/8: uniprots.extend(ortologos.orthologs.unique().tolist())\n",
      "235/9: from Bio import SeqIO\n",
      "235/10: ortologos\n",
      "235/11: secuencias = pd.read_csv('../data/secuencias.csv')\n",
      "235/12: ortologos.type.value_counts()\n",
      "235/13: ortologos.query('type == \"One-to-One\"')\n",
      "235/14:\n",
      "\n",
      "ortologos = pd.read_csv('../data/ortologos.csv').query('type == \"One-to-One\"')\n",
      "235/15: uniprots = entrada.uniprot.unique().tolist()\n",
      "235/16: uniprots.extend(ortologos.orthologs.unique().tolist())\n",
      "235/17: from Bio import SeqIO\n",
      "235/18: secuencias\n",
      "235/19:\n",
      "from Bio import SeqIO\n",
      "from Bio import Seq, SeqRecord\n",
      "235/20: SeqRecord(seq=Seq('KKKKK'),id='as')\n",
      "235/21: SeqRecord.SeqRecord(seq=Seq('KKKKK'),id='as')\n",
      "235/22: Seq(seq=Seq('KKKKK'),id='as')\n",
      "235/23: Seq.Seq(seq=Seq('KKKKK'),id='as')\n",
      "235/24:\n",
      "from Bio import SeqIO\n",
      "from Bio.Seq import Seq,SeqRecord\n",
      "235/25:\n",
      "from Bio import SeqIO\n",
      "from Bio.Seq import Seq\n",
      "235/26: Seq(seq=Seq('KKKKK'),id='as')\n",
      "235/27: Seq('KKKKK',id='as')\n",
      "235/28: Seq('KKKKK')\n",
      "235/29:\n",
      "from Bio import SeqIO\n",
      "from Bio.Seq import Seq\n",
      "from Bio import SeqRecord\n",
      "235/30: SeqRecord(Seq('KKKKK'))\n",
      "235/31: SeqRecord.SeqRecord(Seq('KKKKK'))\n",
      "235/32: SeqRecord.SeqRecord(Seq('KKKKK'),id='a')\n",
      "235/33:\n",
      "from Bio import SeqIO\n",
      "from Bio.Seq import Seq\n",
      "from Bio.SeqRecord import SeqRecord\n",
      "235/34:\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)]\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "        seq = SeqRecord(Seq(row.seq), id=seq.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/35:\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)]\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/36:\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)]\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "        seq = SeqRecord(row.Seq, id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/37:\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)]\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "        seq = SeqRecord(row.seq, id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/38:\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)]\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/39:\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)]\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "        print(row.seq)\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/40:\n",
      "for scaffold, grupo in ortologos.dropna().groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)]\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "        print(row.seq)\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/41:\n",
      "for scaffold, grupo in ortologos.dropna().groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)]\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "        print(uniprot,row.seq)\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/42:\n",
      "for scaffold, grupo in ortologos.dropna().groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)]\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "        print(i,row.seq)\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/43: secuencias\n",
      "235/44: secuencias.dropna()\n",
      "235/45:\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)].dropn()\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/46:\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)].dropna()\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "    fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/47: len(fastas)\n",
      "235/48:\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)].dropna()\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "        fastas.append(seq)\n",
      "    break\n",
      "print(fastas)\n",
      "235/49: len(fastas)\n",
      "235/50:\n",
      "ortologos_fasta = {}\n",
      "for scaffold, grupo in ortologos.groupby('uniprot'):\n",
      "    uniprots = [scaffold] + grupo.orthologs.unique().tolist()\n",
      "    df = secuencias[secuencias.uniprot.isin(uniprots)].dropna()\n",
      "    fastas = []\n",
      "    for i,row in df.iterrows():\n",
      "\n",
      "        seq = SeqRecord(Seq(row.seq), id=row.uniprot)\n",
      "        fastas.append(seq)\n",
      "    ortologos_fasta[scaffold] = fastas\n",
      "235/51:\n",
      "for uniprot, prots in ortologos_fasta.items():\n",
      "    with open(f'../secuencias/ortologos/{uniprot}.fasta','w') as f:\n",
      "        SeqIO.write(prots,f,'fasta')\n",
      "235/52: secuencias\n",
      "235/53: secuencias.dropna()\n",
      "235/54: secuencias.dropna().to_csv('../data/secuencias.csv')\n",
      "235/55: secuencias.dropna().to_csv('../data/secuencias.csv',index=False)\n",
      "237/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "237/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_latest.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "237/3: sp = pd.read_csv('../../tablas/swiss_prot.tab', sep='\\t')\n",
      "237/4:\n",
      "uniprots = sp.Entry.tolist()\n",
      "uniprots_mlos = tablas['tidy'].uniprot.tolist()\n",
      "uniprots.extend(tablas['tidy'].uniprot.tolist())\n",
      "237/5: sp\n",
      "237/6:\n",
      "uniprots = sp.Entry.tolist()\n",
      "uniprots_mlos = tablas['tidy'].uniprot.tolist()\n",
      "uniprots.extend(tablas['tidy'].uniprot.tolist())\n",
      "237/7: uniprots\n",
      "237/8: !head /home/fernando/Descargas/Pfam-A.regions.tsv.gz\n",
      "237/9: seq = \"MPARALLPRRMGHRTLASTPALWASIPCPRSELRLDLVLPSGQSFRWREQSPAHWSGVLADQVWTLTQTEEQLHCTVYRGDKSQASRPTPDELEAVRKYFQLDVTLAQLYHHWGSVDSHFQEVAQKQGVRLLRQDPIECLFSFICSSNNNIARITGMVERLCQAFGPRLIQLDDVTYHGFPSLQALAGPEVEAHLRKLGLGYRARYVSASARAILEEQGGLAWLQQLRESSYEEAHKALCILPGVGTKVADCILMALDKPQAVPVDVHMWHIAQRDYSWHPTTSQAKGPSPQTNKELGNFFRSLWGPYAGWAQAVLFSADLRQSRHAQEPPAKRRKGSKGPEG\"\n",
      "237/10: seq[0]\n",
      "237/11: seq[142:150]\n",
      "237/12: tablas['pfam']\n",
      "237/13: tablas.keys()\n",
      "237/14: tablas[\"human_dominios_box1\"]\n",
      "237/15: tablas[\"human_dominios_box1\"].query('uniprot == \"O15527\"')\n",
      "237/16: a = 'NM_002542.5(OGG1):c.137G>A (p.Arg46Gln)'\n",
      "237/17: a.split(\"(p.\")\n",
      "237/18: a.split(\"(p.\")[-1]\n",
      "237/19: a.split(\"(p.\")[-1][:-1]\n",
      "237/20: a.rsplit(\"(p.\")[-1][:-1]\n",
      "237/21: a[-1]\n",
      "237/22: df = pd.read_csv('/home/fernando/variant_summary.txt', sep='\\t')\n",
      "237/23: df\n",
      "237/24: vs = df[df.Name.map(lambda x: True if x[-1] == \"(\" else False)]\n",
      "237/25: vs\n",
      "237/26: df\n",
      "237/27: df.Name\n",
      "237/28: df.head().Name.map(lambda x: True if x[-1] == \"(\" else False)\n",
      "237/29: vs = df[df.Name.map(lambda x: True if x[-1] == \")\" else False)]\n",
      "237/30: vs\n",
      "237/31: vs['cambio'] = vs.name.str.rsplit(\"(\").str[-1].str[:-1]\n",
      "237/32: vs['cambio'] = vs.Name.str.rsplit(\"(\").str[-1].str[:-1]\n",
      "237/33: vs\n",
      "237/34: vs[['Name','cambio']]\n",
      "237/35: vs[['Name','cambio']].loc[0]\n",
      "237/36: vs['cambio'] = vs.Name.str.rsplit(\"(p.\").str[-1].str[:-1]\n",
      "237/37: vs[['Name','cambio']].loc[0]\n",
      "237/38: vs[['Name','cambio']].loc[1]\n",
      "237/39: vs[['Name','cambio']].loc[3]\n",
      "237/40: vs.cambio\n",
      "237/41: vs.cambio.str.len().value_counts()\n",
      "237/42: vs.cambio.str.len().value_counts()[:30]\n",
      "237/43: vs\n",
      "237/44: vs\n",
      "237/45: vs[\"Type == 'single nucleotide variant'\"]\n",
      "237/46: vs.query(\"Type != 'single nucleotide variant'\")\n",
      "237/47: snsp = vs.query(\"Type != 'single nucleotide variant'\")\n",
      "237/48: snps.cambio\n",
      "237/49: snp = vs.query(\"Type != 'single nucleotide variant'\")\n",
      "237/50: snp\n",
      "237/51: snp.cambio\n",
      "237/52: snp = vs.query(\"Type == 'single nucleotide variant'\")\n",
      "237/53: snp.cambio\n",
      "237/54: df\n",
      "237/55: pfam\n",
      "237/56:\n",
      "pfam = pd.concat(dfs)\n",
      "\n",
      "pfam = pfam[['pfamseq_acc','pfamA_acc','seq_start','seq_end']]\n",
      "\n",
      "pfam.columns = ['uniprot',\"pfam_acc\",\"start\",'end']\n",
      "237/57: *tabas.keys(),\n",
      "237/58: *tablas.keys(),\n",
      "237/59: tablas['human_dominios']\n",
      "238/1: import pandas as pd\n",
      "238/2:\n",
      "import pandas as pd\n",
      "import pickle\n",
      "238/3:\n",
      "with open(\"/home/fernando/git/mlo/todo/tablas/tablas_pickle_latest.pkl\", 'rb') as f:\n",
      "    tablas = pickle.load(f)\n",
      "238/4: tablas['human_box1']\n",
      "238/5: tablas['human_box1'].uniprot.unique()\n",
      "238/6: uniprots = tablas['human_box1'].uniprot.unique()\n",
      "238/7: len(uniprots)\n",
      "238/8:\n",
      "for df in pd.read_csv('home/fernando/Downloads/Pfam-A.regions.tsv.gz', compression='gzip', chunksize=1_000_000, sep='\\t'):\n",
      "    print(df.head)\n",
      "    breal\n",
      "238/9:\n",
      "for df in pd.read_csv('home/fernando/Downloads/Pfam-A.regions.tsv.gz', compression='gzip', chunksize=1_000_000, sep='\\t'):\n",
      "    print(df.head)\n",
      "    break\n",
      "238/10:\n",
      "for df in pd.read_csv('/home/fernando/Downloads/Pfam-A.regions.tsv.gz', compression='gzip', chunksize=1_000_000, sep='\\t'):\n",
      "    print(df.head)\n",
      "    break\n",
      "238/11:\n",
      "for df in pd.read_csv('/home/fernando/Downloads/Pfam-A.regions.tsv.gz', compression='gzip', chunksize=1_000_000, sep='\\t'):\n",
      "    print(df.head())\n",
      "    break\n",
      "238/12:\n",
      "for df in pd.read_csv('/home/fernando/Downloads/Pfam-A.regions.tsv.gz', compression='gzip', chunksize=1_000_000, sep='\\t'):\n",
      "    display(df.head())\n",
      "    break\n",
      "238/13:\n",
      "dfs = []\n",
      "for df in pd.read_csv('/home/fernando/Downloads/Pfam-A.regions.tsv.gz', compression='gzip', chunksize=1_000_000, sep='\\t'):\n",
      "    df = df[df.pfamseq_acc.isin(uniprots)]\n",
      "    dfs.append(df)\n",
      "tabla = pd.concat(dfs)\n",
      "238/14: tabla\n",
      "238/15: tabla.query('Q5HY92')\n",
      "238/16: tabla.query('uniprot == \"Q5HY92\"')\n",
      "238/17: tabla.query('pfamseq_acc == \"Q5HY92\"')\n",
      "241/1: !history\n",
      "241/2: !history -g\n",
      "241/3: %history -g\n",
      "238/18: tabla.query('pfamseq_acc == \"O15527\"')\n",
      "238/19: clanes = pd.read_csv('/home/fernando/Downloads/Pfam-A.clans.tsv', sep='\\t')\n",
      "238/20: clanes\n",
      "238/21: clanes = pd.read_csv('/home/fernando/Downloads/Pfam-A.clans.tsv', sep='\\t', header=None)\n",
      "238/22: clanes\n",
      "238/23: clanes.columns = ['pfamA_acc', 'clan_id', 'clan_name','domain','domain_name']\n",
      "238/24: claes\n",
      "238/25: clanes\n",
      "238/26: tabla.merge(clanes[['pfamA_acc','domain']].drop_duplicatess(), how='left')\n",
      "238/27: tabla.merge(clanes[['pfamA_acc','domain']].drop_duplicates(), how='left')\n",
      "238/28: tabla\n",
      "238/29: tabla = tabla.merge(clanes[['pfamA_acc','domain']].drop_duplicates(), how='left')[['pfamseq_acc', 'pfamA_acc','domain', 'seq_start','seq_end']]\n",
      "238/30: tabla\n",
      "238/31: tabla.columns = ['uniprot','pfam_acc','domain','start','end']\n",
      "238/32: tabla\n",
      "238/33: tabla.query('uniprot == \"O15527\"')\n",
      "238/34: tabla.to_csv('~/home/fernando/pfam_box1.csv',index=False)\n",
      "238/35: tabla.to_csv('~/pfam_box1.csv',index=False)\n",
      "242/1: tabla\n",
      "242/2:\n",
      "import pandas as pd\n",
      "tabla = pd.read_csv('~/pfam_box1.csv')\n",
      "242/3: tabla.sample().head()\n",
      "242/4: tabla.sample(30).head()\n",
      "245/1: tablas\n",
      "246/1:\n",
      "import pandas as pd\n",
      "import pickle\n",
      "246/2:\n",
      "with open(\"/home/fernando/git/mlo/todo/tablas/tablas_pickle_latest.pkl\", 'rb') as f:\n",
      "    tablas = pickle.load(f)\n",
      "246/3: uniprots = tablas['human_box1'].uniprot.unique()\n",
      "245/2: pfam\n",
      "245/3: tablas\n",
      "241/4: pfam\n",
      "241/5: tablas['human_dominios_box1']\n",
      "241/6:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "241/7:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_latest.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "241/8: tablas['human_dominios_box1']\n",
      "241/9: tablas['human_dominios_box1'].query(\"domain == 'P55854'\")\n",
      "241/10: tablas['human_dominios_box1'].query(\"uniprot == 'P55854'\")\n",
      "241/11: tablas['human_dominios_box1'].query(\"uniprot == 'O15527'\")\n",
      "242/5: tabla.query('uniprot == \"O15527\"')\n",
      "247/1:\n",
      "import pandas as pd\n",
      "import pickle\n",
      "247/2: variantes = pd.read_csv('/home/fernando/Downloads/variant_summary.txt')\n",
      "247/3: variantes = pd.read_csv('/home/fernando/Downloads/variant_summary.txt', sep='\\t')\n",
      "247/4: variantes\n",
      "247/5:\n",
      "import pandas as pd\n",
      "import pickle\n",
      "247/6:\n",
      "with open(\"/home/fernando/git/mlo/todo/tablas/tablas_pickle_latest.pkl\", 'rb') as f:\n",
      "    tablas = pickle.load(f)\n",
      "247/7: uniprots = tablas['human_box1'].uniprot.unique()\n",
      "247/8: variantes.columns\n",
      "247/9: variantes.iloc[0]\n",
      "247/10:\n",
      "import re\n",
      "\n",
      "string = 'hello 12 hi 89. Howdy 34'\n",
      "pattern = '\\d+'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/11:\n",
      "import re\n",
      "\n",
      "string = 'hello 12 hi 89. Howdy 34'\n",
      "pattern = '[0-9]'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/12:\n",
      "import re\n",
      "\n",
      "string = 'hello 12 hi 89. Howdy 34'\n",
      "pattern = '[0-9]{1,5}'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/13:\n",
      "import re\n",
      "\n",
      "string = 'hello 1222222 hi 89. Howdy 34'\n",
      "pattern = '[0-9]{1,5}'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/14:\n",
      "import re\n",
      "\n",
      "string = 'hello 1222222222 hi 89. Howdy 34'\n",
      "pattern = '[0-9]{1,5}'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/15:\n",
      "import re\n",
      "\n",
      "string = 'hello 122 hi 89. Howdy 34'\n",
      "pattern = '[0-9]{3}'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/16:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224 hi 89. Howdy 34'\n",
      "pattern = '[0-9]{3}'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/17:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224 hi 89. Howdy 34'\n",
      "pattern = '[0-9]{3,3}'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/18:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224 hi 89. Howdy 34'\n",
      "pattern = '[0-9]{2}'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/19:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224 hi 89. Howdy 34'\n",
      "pattern = '[0-9][A-Z]'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/20:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224A hi 89. Howdy 34'\n",
      "pattern = '[0-9][A-Z]'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/21:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224A hi 89. Howdy 34'\n",
      "pattern = '[0-9]*[A-Z]'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/22:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224A hi 89. Howdy 34'\n",
      "pattern = '[0-9]*[A-Z][a-z]{2}[0-9]*'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/23:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224A hi 89. Howdy 34'\n",
      "pattern = '[0-9]*[A-Z][a-z]{2}[0-9]'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/24:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224Try235 hi 89. Howdy 34'\n",
      "pattern = '[0-9]*[A-Z][a-z]{2}[0-9]'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/25:\n",
      "import re\n",
      "\n",
      "string = 'hello 1224Try235 hi 89. Howdy 34'\n",
      "pattern = '\\([0-9]*[A-Z][a-z]{2}[0-9]'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/26:\n",
      "import re\n",
      "\n",
      "string = 'hello (1224Try235 hi 89. Howdy 34'\n",
      "pattern = '\\([0-9]*[A-Z][a-z]{2}[0-9]'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/27:\n",
      "import re\n",
      "\n",
      "string = 'hello (1224Try235 hi 89. Howdy 34'\n",
      "pattern = '\\([0-9]*[A-Z][a-z]{2}[0-9]*\\)'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/28:\n",
      "import re\n",
      "\n",
      "string = 'hello (1224Try235) hi 89. Howdy 34'\n",
      "pattern = '\\([0-9]*[A-Z][a-z]{2}[0-9]*\\)'\n",
      "\n",
      "result = re.findall(pattern, string) \n",
      "print(result)\n",
      "247/29: variantes.head()\n",
      "247/30: variantes.head().Name\n",
      "247/31: variantes.head().Name.map(lambda x: re.findall(pattern, x))\n",
      "247/32: variantes.head().Name[0]\n",
      "247/33: variantes.head().Name[1]\n",
      "247/34: variantes.head().Name[2]\n",
      "247/35: variantes.head().Name[3]\n",
      "247/36: variantes.head().Name[4]\n",
      "247/37: variantes.Type.value_counts()\n",
      "247/38: snps = variantes.query('Type == \"single nucleotide variant\"')\n",
      "247/39: snps.head().Name[4]\n",
      "247/40: snps.head().Name.map(lambda x: re.findall(pattern, x))\n",
      "247/41: snps.head().Name\n",
      "247/42: pattern = '\\(p.[A-Z][a-z]{2}[0-9]*[A-Z][a-z]{2}\\)'\n",
      "247/43: sns.Name.map(lambda x: re.findall(pattern, x))\n",
      "247/44: snps.Name.map(lambda x: re.findall(pattern, x))\n",
      "247/45: snps.Name.map(lambda x: re.findall(pattern, x)[2:-1])\n",
      "247/46: snps.Name.map(lambda x: re.findall(pattern, x))\n",
      "247/47: snps.Name.head().map(lambda x: re.findall(pattern, x))\n",
      "247/48: snps.Name.head().map(lambda x: re.findall(pattern, x)[0])\n",
      "247/49: snps.Name.head().map(lambda x: re.findall(pattern, x)[0][2:,-1])\n",
      "247/50: snps.Name.head().map(lambda x: re.findall(pattern, x)[0][2:-1])\n",
      "247/51: snps.Name.head().map(lambda x: re.findall(pattern, x)[0][3:-1])\n",
      "247/52: snsp['cambio'] = snps.Name.map(lambda x: re.findall(pattern, x)[0][3:-1])\n",
      "247/53: snsp['cambio'] = snps.Name.map(lambda x: re.findall(pattern, x)[0])\n",
      "247/54: snsp['cambio'] = snps.Name.map(lambda x: re.findall(pattern, x))\n",
      "247/55: snps['cambio'] = snps.Name.map(lambda x: re.findall(pattern, x))\n",
      "247/56: snps\n",
      "247/57: snps.cambio\n",
      "247/58: snps.cambio.str[0]\n",
      "247/59: snps[\"cambio\"] = snps.cambio.str[0]\n",
      "247/60: snps\n",
      "247/61: snps.str[2:-1]\n",
      "247/62: snsp.head().cambio\n",
      "247/63: snps.head().cambio\n",
      "247/64: snps.str.replace('(p.','').str.replace(')','')\n",
      "247/65: snps.cambio.str.replace('(p.','').str.replace(')','')\n",
      "247/66:\n",
      "pattern = '\\(p.[A-Z][a-z]{2}[0-9]*[A-Z][a-z]{2}\\)'\n",
      "pattern = '[A-Z][a-z]{2}[0-9]*[A-Z][a-z]{2})'\n",
      "247/67: snps.Name.head().map(lambda x: re.findall(pattern, x)[0][3:-1])\n",
      "247/68: snps.Name.head().map(lambda x: re.findall(pattern, x)[0])\n",
      "247/69:\n",
      "pattern = '\\(p.[A-Z][a-z]{2}[0-9]*[A-Z][a-z]{2}\\)'\n",
      "pattern = '[A-Z][a-z]{2}[0-9]*[A-Z][a-z]{2}'\n",
      "247/70: snps.Name.head().map(lambda x: re.findall(pattern, x)[0])\n",
      "247/71: snps['cambio'] = snps.Name.map(lambda x: re.findall(pattern, x))\n",
      "247/72: snps.head()\n",
      "247/73: snps[\"cambio\"] = snps.cambio.str[0]\n",
      "247/74: snps.cambio.head()\n",
      "247/75: snps.cambio.head().str.split(\"[0-9]*\")\n",
      "247/76: snps.cambio.head().str.split(\"[0-9]\")\n",
      "247/77: snps.cambio.head().map(lambda x: x.split(\"[0-9]*\"))\n",
      "247/78: snps.cambio.head().map(lambda x: x.split(r\"[0-9]*\"))\n",
      "247/79: snps.cambio.head().map(lambda x: x.split(\"[0-9]\"))\n",
      "247/80: snps.cambio.head().map(lambda x: x.split(\"y\"))\n",
      "247/81: snps.cambio.head().map(lambda x: x.split(\"(y)\"))\n",
      "247/82: snps.cambio.head().map(lambda x: x.split(\"0\"))\n",
      "247/83: snps.cambio.head().map(lambda x: x.split(\"[0-9]\"))\n",
      "247/84: snps.cambio.head().map(lambda x: x.split(\"0-9\"))\n",
      "247/85: snps.cambio.head().map(lambda x: x.split(\"[0-9]\"))\n",
      "247/86: snps.cambio\n",
      "247/87:\n",
      "snps['aa1'] = snp.cambio.str[:3]\n",
      "snps['pos'] = snp.cambio.str[3:-3]\n",
      "snps['aa2'] = snp.cambio.str[-3:]\n",
      "247/88:\n",
      "snps['aa1'] = snps.cambio.str[:3]\n",
      "snps['pos'] = snps.cambio.str[3:-3]\n",
      "snps['aa2'] = snps.cambio.str[-3:]\n",
      "247/89: snsp[['cambio','aa1','pos','aa2']]\n",
      "247/90: snps[['cambio','aa1','pos','aa2']]\n",
      "247/91:\n",
      "snps['aa1'] = snps.cambio.str[:3]\n",
      "snps['pos'] = snps.cambio.str[3:-3].astype(int)\n",
      "snps['aa2'] = snps.cambio.str[-3:]\n",
      "247/92: snps[['cambio','aa1','pos','aa2']]\n",
      "247/93:\n",
      "snps['aa1'] = snps.cambio.str[:3]\n",
      "snps['pos'] = snps.cambio.str[3:-3]\n",
      "snps['aa2'] = snps.cambio.str[-3:]\n",
      "247/94: snps[['cambio','aa1','pos','aa2']]\n",
      "251/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "251/2:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "import pickle\n",
      "\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "251/3: human_db = tablas['db'].query('org == \"Homo sapiens\"')\n",
      "251/4: human_db\n",
      "251/5: human_db.drop_duplicates([\"uniprot\",'mlo','db'])\n",
      "251/6: human_db.drop_duplicates([\"uniprot\",'mlo_loc','db']).pivot_table(index='mlo_loc', columns='db',aggfunc='size')\n",
      "251/7: human_db\n",
      "251/8: human_db.query('uniprot == \"P52948\"')\n",
      "251/9: human_db.loc[human_db.mlo == \"nuclear pore central transport channel\", 'mlo_loc'] = \"nuclear_pore_complex\"\n",
      "251/10: human_db.drop_duplicates([\"uniprot\",'mlo_loc','db']).pivot_table(index='mlo_loc', columns='db',aggfunc='size')\n",
      "251/11: tablas['human_lc_box1']\n",
      "251/12: lc = tablas['human_lc_box1']\n",
      "251/13: lc = tablas['human_lc_box1'].query('method == \"SEG_intermediate\"')\n",
      "251/14: lc\n",
      "251/15: lc.descrption\n",
      "251/16: lc.description\n",
      "251/17: lc.description[9]\n",
      "251/18: lc.description[8]\n",
      "251/19: lc.description.map(eval)\n",
      "251/20: lc.description = lc.description.map(eval)\n",
      "251/21: lc.explode('description')\n",
      "251/22: lcexp = lc.explode('description')\n",
      "251/23: lcexp[lcexp.description.str.contains(\"P\")]\n",
      "251/24: prich = lcexp[lcexp.description.str.contains(\"P\")]\n",
      "251/25: prich\n",
      "251/26: prich_uni = prich.uniprot.unique().tolist()\n",
      "251/27: prich_uni\n",
      "251/28: tablas['human_box1']\n",
      "251/29: tablas.keys()\n",
      "251/30: tablas['human_lc_boxes']\n",
      "251/31: lc = tablas['human_lc_boxes'].query('SEG_intermediate')\n",
      "251/32: lc = tablas['human_lc_boxes'].query('method == \"SEG_intermediate\"')\n",
      "251/33: lc\n",
      "251/34: lc.description = lc.description.map(eval)\n",
      "251/35: lcexp = lc.explode('description')\n",
      "251/36: prich = lcexp[lcexp.description.str.contains(\"P\")]\n",
      "251/37: prich\n",
      "251/38: prich.drop_duplicates([\"uniprot\",'box'])\n",
      "251/39: prich.drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "251/40: tablas['human_boxes']\n",
      "251/41: boxes = tablas['human_boxes']\n",
      "251/42: boxes['prich'] = boxes.uniprot.isin(prich.uniprot)\n",
      "251/43: boxes\n",
      "251/44: boxes.prich.value_counts()\n",
      "251/45: p = boxes[boxes.prich]\n",
      "251/46: p\n",
      "251/47: p.box.value_counts()\n",
      "251/48: prich\n",
      "251/49: prich[prich.uniprot.isin(boxes.query('box == \"box1\"'))]\n",
      "251/50: prich[prich.uniprot.isin(boxes.query('box == \"box1\"').uniprot)]\n",
      "251/51: prich[prich.uniprot.isin(boxes.query('box == \"box1\"').uniprot)].drop_duplicates(\"uniprot\")\n",
      "251/52: p.drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "251/53: prich[prich.uniprot.isin(boxes.query('box == \"box1\"').uniprot)].drop_duplicates(\"uniprot\").sort_values(\"largo\",ascending=False)\n",
      "251/54: P =prich[prich.uniprot.isin(boxes.query('box == \"box1\"').uniprot)]\n",
      "251/55: P\n",
      "251/56: P['freq'] = p.seq.map(lambda x: x.count('P')/len(x))\n",
      "251/57: P['freq'] = P.seq.map(lambda x: x.count('P')/len(x))\n",
      "251/58: P\n",
      "251/59: P.sort_values('freq', ascending=False)\n",
      "251/60: P.query('freq > 0.6').sort_values('largo', ascending=False)\n",
      "252/1: a = [\"2020-03-29\",\"2020-03-30\",\"2020-03-31\",\"2020-04-01\",\"2020-04-02\",\"2020-04-03\",\"2020-04-04\",\"2020-04-05\",\"2020-04-06\",\"2020-04-07\",\"2020-04-08\",\"2020-04-09\",\"2020-04-10\",\"2020-04-11\",\"2020-04-12\",\"2020-04-13\",\"2020-04-14\",\"2020-04-15\",\"2020-04-16\",\"2020-04-17\",\"2020-04-18\",\"2020-04-19\",\"2020-04-20\",\"2020-04-21\",\"2020-04-22\",\"2020-04-23\",\"2020-04-24\",\"2020-04-25\",\"2020-04-26\",\"2020-04-27\",\"2020-04-28\",\"2020-04-29\",\"2020-04-30\",\"2020-05-01\",\"2020-05-02\",\"2020-05-03\",\"2020-05-04\",\"2020-05-05\",\"2020-05-06\",\"2020-05-07\",\"2020-05-08\",\"2020-05-09\",\"2020-05-10\",\"2020-05-11\",\"2020-05-12\",\"2020-05-13\",\"2020-05-14\",\"2020-05-15\",\"2020-05-16\",\"2020-05-17\",\"2020-05-18\",\"2020-05-19\",\"2020-05-20\",\"2020-05-21\",\"2020-05-22\",\"2020-05-23\",\"2020-05-24\",\"2020-05-25\",\"2020-05-26\",\"2020-05-27\",\"2020-05-28\",\"2020-05-29\",\"2020-05-30\",\"2020-05-31\",\"2020-06-01\",\"2020-06-02\",\"2020-06-03\",\"2020-06-04\",\"2020-06-05\",\"2020-06-06\",\"2020-06-07\",\"2020-06-08\",\"2020-06-09\",\"2020-06-10\",\"2020-06-11\",\"2020-06-12\",\"2020-06-13\",\"2020-06-14\",\"2020-06-15\",\"2020-06-16\",\"2020-06-17\",\"2020-06-18\",\"2020-06-19\",\"2020-06-20\",\"2020-06-21\",\"2020-06-22\",\"2020-06-23\",\"2020-06-24\",\"2020-06-25\",\"2020-06-26\",\"2020-06-27\",\"2020-06-28\",\"2020-06-29\",\"2020-06-30\",\"2020-07-01\",\"2020-07-02\",\"2020-07-03\",\"2020-07-04\",\"2020-07-05\",\"2020-07-06\",\"2020-07-07\",\"2020-07-08\",\"2020-07-09\",\"2020-07-10\",\"2020-07-11\",\"2020-07-12\",\"2020-07-13\",\"2020-07-14\",\"2020-07-15\",\"2020-07-16\",\"2020-07-17\",\"2020-07-18\",\"2020-07-19\",\"2020-07-20\",\"2020-07-21\",\"2020-07-22\",\"2020-07-23\",\"2020-07-24\",\"2020-07-25\",\"2020-07-26\",\"2020-07-27\",\"2020-07-28\",\"2020-07-29\",\"2020-07-30\",\"2020-07-31\",\"2020-08-01\",\"2020-08-02\",\"2020-08-03\",\"2020-08-04\",\"2020-08-05\",\"2020-08-06\",\"2020-08-07\",\"2020-08-08\",\"2020-08-09\",\"2020-08-10\",\"2020-08-11\",\"2020-08-12\",\"2020-08-13\",\"2020-08-14\",\"2020-08-15\",\"2020-08-16\",\"2020-08-17\",\"2020-08-18\",\"2020-08-19\",\"2020-08-20\",\"2020-08-21\",\"2020-08-22\",\"2020-08-23\",\"2020-08-24\",\"2020-08-25\",\"2020-08-26\",\"2020-08-27\",\"2020-08-28\",\"2020-08-29\",\"2020-08-30\",\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\"2020-09-04\",\"2020-09-05\",\"2020-09-06\",\"2020-09-07\",\"2020-09-08\",\"2020-09-09\",\"2020-09-10\",\"2020-09-11\",\"2020-09-12\",\"2020-09-13\",\"2020-09-14\",\"2020-09-15\",\"2020-09-16\",\"2020-09-17\",\"2020-09-18\",\"2020-09-19\",\"2020-09-20\",\"2020-09-21\",\"2020-09-22\",\"2020-09-23\",\"2020-09-24\",\"2020-09-25\",\"2020-09-26\",\"2020-09-27\",\"2020-09-28\",\"2020-09-29\",\"2020-09-30\",\"2020-10-01\",\"2020-10-02\",\"2020-10-03\",\"2020-10-04\",\"2020-10-05\",\"2020-10-06\",\"2020-10-07\",\"2020-10-08\",\"2020-10-09\",\"2020-10-10\",\"2020-10-11\",\"2020-10-12\",\"2020-10-13\",\"2020-10-14\",\"2020-10-15\",\"2020-10-16\",\"2020-10-17\",\"2020-10-18\",\"2020-10-19\",\"2020-10-20\",\"2020-10-21\",\"2020-10-22\",\"2020-10-23\",\"2020-10-24\",\"2020-10-25\",\"2020-10-26\",\"2020-10-27\",\"2020-10-28\",\"2020-10-29\",\"2020-10-30\",\"2020-10-31\",\"2020-11-01\",\"2020-11-02\",\"2020-11-03\",\"2020-11-04\",\"2020-11-05\",\"2020-11-06\",\"2020-11-07\",\"2020-11-08\",\"2020-11-09\",\"2020-11-10\",\"2020-11-11\",\"2020-11-12\",\"2020-11-13\",\"2020-11-14\",\"2020-11-15\",\"2020-11-16\",\"2020-11-17\",\"2020-11-18\",\"2020-11-19\",\"2020-11-20\",\"2020-11-21\",\"2020-11-22\",\"2020-11-23\",\"2020-11-24\",\"2020-11-25\",\"2020-11-26\",\"2020-11-27\",\"2020-11-28\",\"2020-11-29\",\"2020-11-30\",\"2020-12-01\",\"2020-12-02\",\"2020-12-03\",\"2020-12-04\",\"2020-12-05\",\"2020-12-06\",\"2020-12-07\",\"2020-12-08\",\"2020-12-09\",\"2020-12-10\",\"2020-12-11\",\"2020-12-12\",\"2020-12-13\",\"2020-12-14\",\"2020-12-15\",\"2020-12-16\",\"2020-12-17\",\"2020-12-18\",\"2020-12-19\",\"2020-12-20\",\"2020-03-29\",\"2020-03-30\",\"2020-03-31\",\"2020-04-01\",\"2020-04-02\",\"2020-04-03\",\"2020-04-04\",\"2020-04-05\",\"2020-04-06\",\"2020-04-07\",\"2020-04-08\",\"2020-04-09\",\"2020-04-10\",\"2020-04-11\",\"2020-04-12\",\"2020-04-13\",\"2020-04-14\",\"2020-04-15\",\"2020-04-16\",\"2020-04-17\",\"2020-04-18\",\"2020-04-19\",\"2020-04-20\",\"2020-04-21\",\"2020-04-22\",\"2020-04-23\",\"2020-04-24\",\"2020-04-25\",\"2020-04-26\",\"2020-04-27\",\"2020-04-28\",\"2020-04-29\",\"2020-04-30\",\"2020-05-01\",\"2020-05-02\",\"2020-05-03\",\"2020-05-04\",\"2020-05-05\",\"2020-05-06\",\"2020-05-07\",\"2020-05-08\",\"2020-05-09\",\"2020-05-10\",\"2020-05-11\",\"2020-05-12\",\"2020-05-13\",\"2020-05-14\",\"2020-05-15\",\"2020-05-16\",\"2020-05-17\",\"2020-05-18\",\"2020-05-19\",\"2020-05-20\",\"2020-05-21\",\"2020-05-22\",\"2020-05-23\",\"2020-05-24\",\"2020-05-25\",\"2020-05-26\",\"2020-05-27\",\"2020-05-28\",\"2020-05-29\",\"2020-05-30\",\"2020-05-31\",\"2020-06-01\",\"2020-06-02\",\"2020-06-03\",\"2020-06-04\",\"2020-06-05\",\"2020-06-06\",\"2020-06-07\",\"2020-06-08\",\"2020-06-09\",\"2020-06-10\",\"2020-06-11\",\"2020-06-12\",\"2020-06-13\",\"2020-06-14\",\"2020-06-15\",\"2020-06-16\",\"2020-06-17\",\"2020-06-18\",\"2020-06-19\",\"2020-06-20\",\"2020-06-21\",\"2020-06-22\",\"2020-06-23\",\"2020-06-24\",\"2020-06-25\",\"2020-06-26\",\"2020-06-27\",\"2020-06-28\",\"2020-06-29\",\"2020-06-30\",\"2020-07-01\",\"2020-07-02\",\"2020-07-03\",\"2020-07-04\",\"2020-07-05\",\"2020-07-06\",\"2020-07-07\",\"2020-07-08\",\"2020-07-09\",\"2020-07-10\",\"2020-07-11\",\"2020-07-12\",\"2020-07-13\",\"2020-07-14\",\"2020-07-15\",\"2020-07-16\",\"2020-07-17\",\"2020-07-18\",\"2020-07-19\",\"2020-07-20\",\"2020-07-21\",\"2020-07-22\",\"2020-07-23\",\"2020-07-24\",\"2020-07-25\",\"2020-07-26\",\"2020-07-27\",\"2020-07-28\",\"2020-07-29\",\"2020-07-30\",\"2020-07-31\",\"2020-08-01\",\"2020-08-02\",\"2020-08-03\",\"2020-08-04\",\"2020-08-05\",\"2020-08-06\",\"2020-08-07\",\"2020-08-08\",\"2020-08-09\",\"2020-08-10\",\"2020-08-11\",\"2020-08-12\",\"2020-08-13\",\"2020-08-14\",\"2020-08-15\",\"2020-08-16\",\"2020-08-17\",\"2020-08-18\",\"2020-08-19\",\"2020-08-20\",\"2020-08-21\",\"2020-08-22\",\"2020-08-23\",\"2020-08-24\",\"2020-08-25\",\"2020-08-26\",\"2020-08-27\",\"2020-08-28\",\"2020-08-29\",\"2020-08-30\",\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\"2020-09-04\",\"2020-09-05\",\"2020-09-06\",\"2020-09-07\",\"2020-09-08\",\"2020-09-09\",\"2020-09-10\",\"2020-09-11\",\"2020-09-12\",\"2020-09-13\",\"2020-09-14\",\"2020-09-15\",\"2020-09-16\",\"2020-09-17\",\"2020-09-18\",\"2020-09-19\",\"2020-09-20\",\"2020-09-21\",\"2020-09-22\",\"2020-09-23\",\"2020-09-24\",\"2020-09-25\",\"2020-09-26\",\"2020-09-27\",\"2020-09-28\",\"2020-09-29\",\"2020-09-30\",\"2020-10-01\",\"2020-10-02\",\"2020-10-03\",\"2020-10-04\",\"2020-10-05\",\"2020-10-06\",\"2020-10-07\",\"2020-10-08\",\"2020-10-09\",\"2020-10-10\",\"2020-10-11\",\"2020-10-12\",\"2020-10-13\",\"2020-10-14\",\"2020-10-15\",\"2020-10-16\",\"2020-10-17\",\"2020-10-18\",\"2020-10-19\",\"2020-10-20\",\"2020-10-21\",\"2020-10-22\",\"2020-10-23\",\"2020-10-24\",\"2020-10-25\",\"2020-10-26\",\"2020-10-27\",\"2020-10-28\",\"2020-10-29\",\"2020-10-30\",\"2020-10-31\",\"2020-11-01\",\"2020-11-02\",\"2020-11-03\",\"2020-11-04\",\"2020-11-05\",\"2020-11-06\",\"2020-11-07\",\"2020-11-08\",\"2020-11-09\",\"2020-11-10\",\"2020-11-11\",\"2020-11-12\",\"2020-11-13\",\"2020-11-14\",\"2020-11-15\",\"2020-11-16\",\"2020-11-17\",\"2020-11-18\",\"2020-11-19\",\"2020-11-20\",\"2020-11-21\",\"2020-11-22\",\"2020-11-23\",\"2020-11-24\",\"2020-11-25\",\"2020-11-26\",\"2020-11-27\",\"2020-11-28\",\"2020-11-29\",\"2020-11-30\",\"2020-12-01\",\"2020-12-02\",\"2020-12-03\",\"2020-12-04\",\"2020-12-05\",\"2020-12-06\",\"2020-12-07\",\"2020-12-08\",\"2020-12-09\",\"2020-12-10\",\"2020-12-11\",\"2020-12-12\",\"2020-12-13\",\"2020-12-14\",\"2020-12-15\",\"2020-12-16\",\"2020-12-17\",\"2020-12-18\",\"2020-12-19\",\"2020-12-20\",\"2020-03-29\",\"2020-03-30\",\"2020-03-31\",\"2020-04-01\",\"2020-04-02\",\"2020-04-03\",\"2020-04-04\",\"2020-04-05\",\"2020-04-06\",\"2020-04-07\",\"2020-04-08\",\"2020-04-09\",\"2020-04-10\",\"2020-04-11\",\"2020-04-12\",\"2020-04-13\",\"2020-04-14\",\"2020-04-15\",\"2020-04-16\",\"2020-04-17\",\"2020-04-18\",\"2020-04-19\",\"2020-04-20\",\"2020-04-21\",\"2020-04-22\",\"2020-04-23\",\"2020-04-24\",\"2020-04-25\",\"2020-04-26\",\"2020-04-27\",\"2020-04-28\",\"2020-04-29\",\"2020-04-30\",\"2020-05-01\",\"2020-05-02\",\"2020-05-03\",\"2020-05-04\",\"2020-05-05\",\"2020-05-06\",\"2020-05-07\",\"2020-05-08\",\"2020-05-09\",\"2020-05-10\",\"2020-05-11\",\"2020-05-12\",\"2020-05-13\",\"2020-05-14\",\"2020-05-15\",\"2020-05-16\",\"2020-05-17\",\"2020-05-18\",\"2020-05-19\",\"2020-05-20\",\"2020-05-21\",\"2020-05-22\",\"2020-05-23\",\"2020-05-24\",\"2020-05-25\",\"2020-05-26\",\"2020-05-27\",\"2020-05-28\",\"2020-05-29\",\"2020-05-30\",\"2020-05-31\",\"2020-06-01\",\"2020-06-02\",\"2020-06-03\",\"2020-06-04\",\"2020-06-05\",\"2020-06-06\",\"2020-06-07\",\"2020-06-08\",\"2020-06-09\",\"2020-06-10\",\"2020-06-11\",\"2020-06-12\",\"2020-06-13\",\"2020-06-14\",\"2020-06-15\",\"2020-06-16\",\"2020-06-17\",\"2020-06-18\",\"2020-06-19\",\"2020-06-20\",\"2020-06-21\",\"2020-06-22\",\"2020-06-23\",\"2020-06-24\",\"2020-06-25\",\"2020-06-26\",\"2020-06-27\",\"2020-06-28\",\"2020-06-29\",\"2020-06-30\",\"2020-07-01\",\"2020-07-02\",\"2020-07-03\",\"2020-07-04\",\"2020-07-05\",\"2020-07-06\",\"2020-07-07\",\"2020-07-08\",\"2020-07-09\",\"2020-07-10\",\"2020-07-11\",\"2020-07-12\",\"2020-07-13\",\"2020-07-14\",\"2020-07-15\",\"2020-07-16\",\"2020-07-17\",\"2020-07-18\",\"2020-07-19\",\"2020-07-20\",\"2020-07-21\",\"2020-07-22\",\"2020-07-23\",\"2020-07-24\",\"2020-07-25\",\"2020-07-26\",\"2020-07-27\",\"2020-07-28\",\"2020-07-29\",\"2020-07-30\",\"2020-07-31\",\"2020-08-01\",\"2020-08-02\",\"2020-08-03\",\"2020-08-04\",\"2020-08-05\",\"2020-08-06\",\"2020-08-07\",\"2020-08-08\",\"2020-08-09\",\"2020-08-10\",\"2020-08-11\",\"2020-08-12\",\"2020-08-13\",\"2020-08-14\",\"2020-08-15\",\"2020-08-16\",\"2020-08-17\",\"2020-08-18\",\"2020-08-19\",\"2020-08-20\",\"2020-08-21\",\"2020-08-22\",\"2020-08-23\",\"2020-08-24\",\"2020-08-25\",\"2020-08-26\",\"2020-08-27\",\"2020-08-28\",\"2020-08-29\",\"2020-08-30\",\"2020-08-31\",\"2020-09-01\",\"2020-09-02\",\"2020-09-03\",\"2020-09-04\",\"2020-09-05\",\"2020-09-06\",\"2020-09-07\",\"2020-09-08\",\"2020-09-09\",\"2020-09-10\",\"2020-09-11\",\"2020-09-12\",\"2020-09-13\",\"2020-09-14\",\"2020-09-15\",\"2020-09-16\",\"2020-09-17\",\"2020-09-18\",\"2020-09-19\",\"2020-09-20\",\"2020-09-21\",\"2020-09-22\",\"2020-09-23\",\"2020-09-24\",\"2020-09-25\",\"2020-09-26\",\"2020-09-27\",\"2020-09-28\",\"2020-09-29\",\"2020-09-30\",\"2020-10-01\",\"2020-10-02\",\"2020-10-03\",\"2020-10-04\",\"2020-10-05\",\"2020-10-06\",\"2020-10-07\",\"2020-10-08\",\"2020-10-09\",\"2020-10-10\",\"2020-10-11\",\"2020-10-12\",\"2020-10-13\",\"2020-10-14\",\"2020-10-15\",\"2020-10-16\",\"2020-10-17\",\"2020-10-18\",\"2020-10-19\",\"2020-10-20\",\"2020-10-21\",\"2020-10-22\",\"2020-10-23\",\"2020-10-24\",\"2020-10-25\",\"2020-10-26\",\"2020-10-27\",\"2020-10-28\",\"2020-10-29\",\"2020-10-30\",\"2020-10-31\",\"2020-11-01\",\"2020-11-02\",\"2020-11-03\",\"2020-11-04\",\"2020-11-05\",\"2020-11-06\",\"2020-11-07\",\"2020-11-08\",\"2020-11-09\",\"2020-11-10\",\"2020-11-11\",\"2020-11-12\",\"2020-11-13\",\"2020-11-14\",\"2020-11-15\",\"2020-11-16\",\"2020-11-17\",\"2020-11-18\",\"2020-11-19\",\"2020-11-20\",\"2020-11-21\",\"2020-11-22\",\"2020-11-23\",\"2020-11-24\",\"2020-11-25\",\"2020-11-26\",\"2020-11-27\",\"2020-11-28\",\"2020-11-29\",\"2020-11-30\",\"2020-12-01\",\"2020-12-02\",\"2020-12-03\",\"2020-12-04\",\"2020-12-05\",\"2020-12-06\",\"2020-12-07\",\"2020-12-08\",\"2020-12-09\",\"2020-12-10\",\"2020-12-11\",\"2020-12-12\",\"2020-12-13\",\"2020-12-14\",\"2020-12-15\",\"2020-12-16\",\"2020-12-17\",\"2020-12-18\",\"2020-12-19\",\"2020-12-20\",\"2020-12-21\",\"2020-12-22\",\"2020-12-23\",\"2020-12-24\",\"2020-12-25\",\"2020-12-26\",\"2020-12-27\",\"2020-12-28\",\"2020-12-29\",\"2020-12-30\",\"2020-12-31\",\"2021-01-01\",\"2021-01-02\",\"2021-01-03\",\"2021-01-04\",\"2021-01-05\",\"2021-01-06\",\"2021-01-07\",\"2021-01-08\",\"2021-01-09\",\"2021-01-10\",\"2021-01-11\",\"2021-01-12\",\"2021-01-13\",\"2021-01-14\",\"2021-01-15\",\"2021-01-16\",\"2021-01-17\"]\n",
      "252/2: pd.Series(a).value_counts()\n",
      "252/3: import pandas as pd\n",
      "252/4: pd.Series(a).value_counts()\n",
      "252/5: pd.Series(a).value_counts()[:10]\n",
      "252/6: pd.Series(a).value_counts()[:20]\n",
      "252/7: pd.Series(a).value_counts()[:30]\n",
      "252/8: pd.Series(a).value_counts()[:50]\n",
      "252/9: pd.Series(a).value_counts().value_counts()\n",
      "252/10: s = pd.Series(a).value_counts()\n",
      "252/11: s[s==1]\n",
      "255/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "import pickle\n",
      "\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "255/2: human_db = tablas['db'].query('org == \"Homo sapiens\"')\n",
      "255/3: tablas['boxes_human']\n",
      "255/4: tablas['human_boxes']\n",
      "255/5: tablas['human_boxes'].box.value_counts()\n",
      "255/6: tablas['human_boxes'].drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "255/7: boxes = tablas['human_boxes'].copy()\n",
      "255/8: boxes\n",
      "255/9: boxes.merge(tablas['human_lc'].query('method == \"SEG_intermediate\"'))\n",
      "255/10: tablas['human_lc'].query('method == \"SEG_intermediate\"').merge(boxes.merge())\n",
      "255/11: tablas['human_lc'].query('method == \"SEG_intermediate\"').merge(boxes)\n",
      "255/12: tablas['human_lc'].query('method == \"SEG_intermediate\"')\n",
      "255/13: lc = tablas['human_lc'].query('method == \"SEG_intermediate\"')\n",
      "255/14: lc\n",
      "255/15: lc.merge(boxes)\n",
      "255/16: lc_box = lc.merge(boxes)\n",
      "255/17: lc_box\n",
      "255/18: lc_box.drop_duplicates([\"uniprot\",'start'])\n",
      "255/19: lc_box.drop_duplicates([\"uniprot\",'start','mlo_loc'])\n",
      "255/20: lc_box.drop_duplicates([\"uniprot\",'start'])\n",
      "255/21: lc_box.description = lc_boxe.description.map(eval)\n",
      "255/22: lc_box.description = lc_box.description.map(eval)\n",
      "255/23: lc_box\n",
      "255/24: lc_box.entropia\n",
      "255/25: lc_box.entropia[0]\n",
      "255/26: lc_box.entropia = lc_box.entropia.map(eval)\n",
      "255/27: lc_box.entropia = lc_box.entropia.map(mean)\n",
      "255/28: lc_box.entropia = lc_box.entropia.map(np.mean)\n",
      "255/29: import numpy as np\n",
      "255/30: lc_box.entropia = lc_box.entropia.map(np.mean)\n",
      "255/31: lc_box[['uniprot','subzonas',\"start\",'end','seq','largo','naas', 'entropia',\"mlo_loc\",'box']]\n",
      "255/32: lc = lc_box[['uniprot','subzonas',\"start\",'end','seq','largo','naas', 'entropia',\"mlo_loc\",'box']]\n",
      "255/33: lc\n",
      "255/34: lc_box\n",
      "255/35: lc = lc_box[['uniprot',\"description\",'subzonas',\"start\",'end','seq','largo','naas', 'entropia',\"mlo_loc\",'box']]\n",
      "255/36: lc_box\n",
      "255/37: lc\n",
      "255/38: lc.description.map(lambda x: x.split(\" \")[0])\n",
      "255/39:\n",
      "def proc_zona(lista):\n",
      "    salida = []\n",
      "    for zona in lista:\n",
      "        salida.append(\n",
      "        zona.split(\" \")[0]\n",
      "        )\n",
      "    salida = list(set(salida))\n",
      "255/40: lc.description.map(lambda x: proc_zona)\n",
      "255/41:\n",
      "def proc_zona(lista):\n",
      "    salida = []\n",
      "    for zona in lista:\n",
      "        salida.append(\n",
      "        zona.split(\" \")[0]\n",
      "        )\n",
      "    salida = list(set(salida))\n",
      "    return salida\n",
      "255/42: lc.description.map(lambda x: proc_zona)\n",
      "255/43: lc.description.map(proc_zona)\n",
      "255/44: lc.description.map(proc_zona).str.len()\n",
      "255/45: lc.description.map(proc_zona).str.len().value_counts()\n",
      "255/46: lc.description.map(proc_zona)\n",
      "255/47: lc['enrich'] = lc.description.map(proc_zona)\n",
      "255/48: lc\n",
      "255/49: lc.groupby([col in lc.columns if col not in [\"mlo_loc\",'box']]).reset_index()\n",
      "255/50: lc.groupby([col for col in lc.columns if col not in [\"mlo_loc\",'box']]).reset_index()\n",
      "255/51: lc.groupby([col for col in lc.columns if col not in [\"mlo_loc\",'box']])\n",
      "255/52: lc.groupby([col for col in lc.columns if col not in [\"mlo_loc\",'box']]).agg(list).reset_index()\n",
      "255/53: lc.groupby([col for col in lc.columns if col not in [\"mlo_loc\",'box']]).agg(list)\n",
      "255/54: lc\n",
      "255/55: lc[['uniprot','mlo_loc','box']]\n",
      "255/56: lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(list())\n",
      "255/57: lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(list)\n",
      "255/58: lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(list).reset_index\n",
      "255/59: lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(list).reset_index()\n",
      "255/60: lc.merge(lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(list).reset_index())\n",
      "255/61: lc.groupby([col for col in lc.columns if col not in [\"mlo_loc\",'box']]).reset_index()\n",
      "255/62: lc.groupby([col for col in lc.columns if col not in [\"mlo_loc\",'box']]).agg(\",\".join).reset_index()\n",
      "255/63: lc.groupby([col for col in lc.columns if col not in [\"mlo_loc\",'box','enrich']]).agg(\",\".join).reset_index()\n",
      "255/64: lc\n",
      "255/65: lc[['uniprot','mlo_loc','box']]\n",
      "255/66: lc[['uniprot','mlo_loc','box']].groupby('uniprot')\n",
      "255/67: lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(\" \".join)\n",
      "255/68: lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(lambda x: \" \".join(sorted(set(x)))\n",
      "255/69: lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(lambda x: \" \".join(sorted(set(x))))\n",
      "255/70: lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(lambda x: \", \".join(sorted(set(x))))\n",
      "255/71: mlos = lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(lambda x: \", \".join(sorted(set(x))))\n",
      "255/72: mlos\n",
      "255/73: mlos.columns = [\"mlo\",'boxes']\n",
      "255/74: lc.merge(mlos)\n",
      "255/75: lc.merge(mlos.reset_index())\n",
      "255/76: lc_red = lc.merge(mlos.reset_index())\n",
      "255/77: lc_red\n",
      "255/78: lc_red.sort_values('boxes','mlo','entropia')\n",
      "255/79: lc_red.sort_values(['boxes','mlo','entropia'])\n",
      "255/80: lc_red.groupby('uniprot').agg(list)\n",
      "255/81: lc.groupby('uniprot').agg(list)\n",
      "255/82: lc.groupby('uniprot').agg(list).reset_index()\n",
      "255/83: lc.groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'start','end','seq','largo','nass','entropia',\"enrich\",'mlo','box']]\n",
      "255/84: lc.groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'start','end','seq','largo','naas','entropia',\"enrich\"]]\n",
      "255/85: lc.groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'start','end','seq','largo','naas','entropia',\"enrich\"]].mereg(mlos.reset_index())\n",
      "255/86: lc.groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'start','end','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/87: tabla = lc.groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'start','end','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/88: tabla\n",
      "255/89: tabla.groupby([\"uniprot\",'mlo','boxes']).agg(\", \".join)\n",
      "255/90: tabla.groupby([\"uniprot\",'mlo','boxes'])#.agg(\", \".join)\n",
      "255/91: tabla.groupby([\"uniprot\",'mlo','boxes']).agg(\", \".join)\n",
      "255/92: tabla\n",
      "255/93: tabla.groupby(\"uniprot\").agg(\", \".join)\n",
      "255/94: tabla#.groupby(\"uniprot\").agg(\", \".join)\n",
      "255/95: tabla.groupby(\"uniprot\").agg(\", \".join)\n",
      "255/96: tabla.groupby(\"uniprot\").agg(list)\n",
      "255/97: tabla.groupby(\"uniprot\").map(lambda x: \",\".join(x))\n",
      "255/98: tabla.groupby(\"uniprot\").agg(lambda x: \",\".join(x))\n",
      "255/99: tabla.groupby(\"uniprot\").agg(lambda x: \",\".join(x[0]))\n",
      "255/100: tabla.groupby([\"uniprot\",'mlo','boxes']).agg(lambda x: \",\".join(x[0]))\n",
      "255/101: tabla#.groupby([\"uniprot\",'mlo','boxes']).agg(lambda x: \",\".join(x[0]))\n",
      "255/102: tabla.uniprot.value_counts()#.groupby([\"uniprot\",'mlo','boxes']).agg(lambda x: \",\".join(x[0]))\n",
      "255/103: tabla.uniprot.value_counts().value_counts()#.groupby([\"uniprot\",'mlo','boxes']).agg(lambda x: \",\".join(x[0]))\n",
      "255/104: tabla.uniprot.value_counts()#.value_counts()#.groupby([\"uniprot\",'mlo','boxes']).agg(lambda x: \",\".join(x[0]))\n",
      "255/105: tabla\n",
      "255/106: tabla['seq'] = tabla.map(\", \".join)\n",
      "255/107: tabla.seq.map(\", \".join)\n",
      "255/108: tabla\n",
      "255/109: mlos\n",
      "255/110: tabla = lc.groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'start','end','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/111: tabla\n",
      "255/112: tabla = lc.groupby('uniprot').agg(list).reset_index().drop_duplicates([\"uniprot\",'start','end'])[[\"uniprot\",'start','end','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/113: tabla = lc.drop_duplicates([\"uniprot\",'start','end']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'start','end','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/114: tabla\n",
      "255/115:  tabla.seq.map(\", \".join)\n",
      "255/116: .seq.map(\", \".join)\n",
      "255/117: tablas\n",
      "255/118: tabla\n",
      "255/119: lc\n",
      "255/120: lc[\"pos\"] = lc.start.astype(str) +\" - \" + lc.end.astype(str)\n",
      "255/121: tabla = lc.drop_duplicates([\"uniprot\",'pos']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'pos','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/122: tabla.groupby([\"uniprot\",'mlo','boxes']).agg(\", \".join)\n",
      "255/123: tabla\n",
      "255/124: tabla[\"seq\"] = tabla.seq.map(\", \".join)\n",
      "255/125: tabla\n",
      "255/126: tabla[\"pos\"] = tabla.seq.map(\"\\n\".join)\n",
      "255/127: tabla\n",
      "255/128: tabla = lc.drop_duplicates([\"uniprot\",'pos']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'pos','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/129: tabla[\"seq\"] = tabla.seq.map(\"\\n\".join)\n",
      "255/130: tabla[\"pos\"] = tabla.seq.map(\"\\n\".join)\n",
      "255/131: tabla\n",
      "255/132: tabla.to_csv('t.csv',index=False)\n",
      "255/133: tabla = lc.drop_duplicates([\"uniprot\",'pos']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'pos','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/134: tabla\n",
      "255/135:\n",
      "tabla[\"largo\"] = tabla.largo.map(\", \".join)\n",
      "tabla[\"naas\"] = tabla.naas.map(\", \".join)\n",
      "tabla[\"entropia\"] = tabla.pos.map(lambda x: \", \".join([str(round(i,2)) for i in x]))\n",
      "255/136:\n",
      "tabla[\"largo\"] = tabla.largo.map(\", \".join)\n",
      "tabla[\"naas\"] = tabla.naas.map(\", \".join)\n",
      "255/137: tabla\n",
      "255/138:\n",
      "tabla[\"largo\"] = tabla.largo.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "tabla[\"naas\"] = tabla.naas.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "tabla[\"entropia\"] = tabla.pos.map(lambda x: \", \".join([str(round(i,2)) for i in x]))\n",
      "255/139: tabla.entropia\n",
      "255/140: tabla.entropia[]\n",
      "255/141: tabla.entropia[0]\n",
      "255/142: tabla.entropia[0][0]\n",
      "255/143: round(tabla.entropia[0][0])\n",
      "255/144: round(tabla.entropia[0][0],2)\n",
      "255/145:\n",
      "tabla[\"largo\"] = tabla.largo.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "tabla[\"naas\"] = tabla.naas.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "255/146: tabla\n",
      "255/147: tabla[\"seq\"] = tabla.seq.map(\", \".join)\n",
      "255/148: tabla[\"pos\"] = tabla.pos.map(\", \".join)\n",
      "255/149: tabla\n",
      "255/150: tabla = lc.drop_duplicates([\"uniprot\",'pos']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'pos','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/151: tabla[\"seq\"] = tabla.seq.map(\", \".join)\n",
      "255/152: tabla[\"pos\"] = tabla.pos.map(\", \".join)\n",
      "255/153:\n",
      "tabla[\"largo\"] = tabla.largo.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "tabla[\"naas\"] = tabla.naas.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "255/154: tabla\n",
      "255/155: tabla[\"entropia\"] = tabla.pos.map(lambda x: \", \".join([str(round(i,2)) for i in x]))\n",
      "255/156: tabla[\"entropia\"] = tabla.entropia.map(lambda x: \", \".join([str(round(i,2)) for i in x]))\n",
      "255/157: tabla\n",
      "255/158: tabla = lc.drop_duplicates([\"uniprot\",'pos']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'pos','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/159: tabla[\"seq\"] = tabla.seq.map(\", \".join)\n",
      "255/160: tabla[\"pos\"] = tabla.pos.map(\", \".join)\n",
      "255/161:\n",
      "tabla[\"largo\"] = tabla.largo.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "tabla[\"naas\"] = tabla.naas.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "255/162: tabla[\"entropia\"] = tabla.entropia.map(lambda x: \", \".join([str(round(i,1)) for i in x]))\n",
      "255/163: tabla\n",
      "255/164: mlos\n",
      "255/165: mlos['mlo'] = mlos.mlo.map(tablas['translate'])\n",
      "255/166: tabla = lc.drop_duplicates([\"uniprot\",'pos']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'pos','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/167: tabla[\"seq\"] = tabla.seq.map(\", \".join)\n",
      "255/168: tabla[\"pos\"] = tabla.pos.map(\", \".join)\n",
      "255/169:\n",
      "tabla[\"largo\"] = tabla.largo.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "tabla[\"naas\"] = tabla.naas.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "255/170: tabla[\"entropia\"] = tabla.entropia.map(lambda x: \", \".join([str(round(i,1)) for i in x]))\n",
      "255/171: tabla\n",
      "255/172: mlos = lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(lambda x: \", \".join(sorted(set(x))))\n",
      "255/173: mlos.columns = [\"mlo\",'boxes']\n",
      "255/174: lc[\"pos\"] = lc.start.astype(str) +\" - \" + lc.end.astype(str)\n",
      "255/175: tabla = lc.drop_duplicates([\"uniprot\",'pos']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'pos','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/176: tabla[\"seq\"] = tabla.seq.map(\", \".join)\n",
      "255/177: tabla[\"pos\"] = tabla.pos.map(\", \".join)\n",
      "255/178:\n",
      "tabla[\"largo\"] = tabla.largo.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "tabla[\"naas\"] = tabla.naas.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "255/179: tabla[\"entropia\"] = tabla.entropia.map(lambda x: \", \".join([str(round(i,1)) for i in x]))\n",
      "255/180: tabla\n",
      "255/181:  mlos.mlo.map(tablas['translate'])>\n",
      "255/182:  mlos.mlo.map(tablas['translate'])\n",
      "255/183: mlos\n",
      "255/184:\n",
      "_ = lc[['uniprot','mlo_loc','box']]\n",
      "_.mlo_loc = _.mlo_loc.map(tablas['translate'])\n",
      "mlos = lc[['uniprot','mlo_loc','box']].groupby('uniprot').agg(lambda x: \", \".join(sorted(set(x))))\n",
      "255/185: mlos.columns = [\"mlo\",'boxes']\n",
      "255/186: tabla = lc.drop_duplicates([\"uniprot\",'pos']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'pos','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/187: tabla[\"seq\"] = tabla.seq.map(\", \".join)\n",
      "255/188: tabla[\"pos\"] = tabla.pos.map(\", \".join)\n",
      "255/189:\n",
      "tabla[\"largo\"] = tabla.largo.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "tabla[\"naas\"] = tabla.naas.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "255/190: tabla[\"entropia\"] = tabla.entropia.map(lambda x: \", \".join([str(round(i,1)) for i in x]))\n",
      "255/191: tabla\n",
      "255/192: mlos\n",
      "255/193:\n",
      "_ = lc[['uniprot','mlo_loc','box']]\n",
      "_.mlo_loc = _.mlo_loc.map(tablas['translate'])\n",
      "mlos = _.groupby('uniprot').agg(lambda x: \", \".join(sorted(set(x))))\n",
      "255/194: mlos.columns = [\"mlo\",'boxes']\n",
      "255/195: tabla = lc.drop_duplicates([\"uniprot\",'pos']).groupby('uniprot').agg(list).reset_index()[[\"uniprot\",'pos','seq','largo','naas','entropia',\"enrich\"]].merge(mlos.reset_index())\n",
      "255/196: tabla[\"seq\"] = tabla.seq.map(\", \".join)\n",
      "255/197: tabla[\"pos\"] = tabla.pos.map(\", \".join)\n",
      "255/198:\n",
      "tabla[\"largo\"] = tabla.largo.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "tabla[\"naas\"] = tabla.naas.map(lambda x: \", \".join([str(i) for i in x]))\n",
      "255/199: tabla[\"entropia\"] = tabla.entropia.map(lambda x: \", \".join([str(round(i,1)) for i in x]))\n",
      "255/200: tabla\n",
      "255/201: mlos\n",
      "255/202: tabla\n",
      "255/203: tabla['prot'] = tabla['uniprot'].map(tablas['gene_names_dict'])\n",
      "255/204: tblas\n",
      "255/205: tabla\n",
      "255/206: tabla[\"uniprot\",\"mlo\",'prot','pos','seq','largo','naas','entropia','boxes']\n",
      "255/207: tabla[[\"uniprot\",\"mlo\",'prot','pos','seq','largo','naas','entropia','boxes']]\n",
      "255/208: tabla[[\"uniprot\",\"mlo\",'prot','pos','seq','largo','naas','entropia','boxes']].sort_values(['boxes', 'mlo','entropia'])\n",
      "255/209: tabla[[\"uniprot\",\"mlo\",'prot','pos','seq','largo','naas','entropia','boxes']].sort_values(['boxes', 'mlo','largo'])\n",
      "255/210: tabla[[\"uniprot\",\"mlo\",'prot','pos','seq','largo','naas','entropia','boxes']].sort_values(['boxes', 'mlo'])\n",
      "255/211: tabla[[\"uniprot\",\"mlo\",'prot','pos','seq','largo','naas','entropia','boxes']].sort_values(['boxes', 'mlo','prot'])\n",
      "255/212: tabla[[\"uniprot\",'prot',\"mlo\",'pos','seq','largo','naas','entropia','boxes']].sort_values(['boxes', 'mlo','prot']).to_csv('tablas/tabla_lc_general.csv',index=False)\n",
      "255/213: tabla[[\"uniprot\",'prot',\"mlo\",'pos','seq','largo','naas','entropia','boxes']].sort_values(['boxes','prot']).to_csv('tablas/tabla_lc_general.csv',index=False)\n",
      "259/1:\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import matplotlib as mpl\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "259/2: from Bio import SeqIO, AlignIO\n",
      "259/3: ali = AlignIO.read(\"/home/fernando/git/llps/secuencias/fus_ort_aligned.fasta\", 'fasta')\n",
      "259/4: ali\n",
      "259/5: [aa for aa in str(secuencia.seq) for secuencia in ali]\n",
      "259/6:\n",
      "lista = []\n",
      "for secuencia in ali:\n",
      "    lista.append(\n",
      "        [aa for aa in str(secuencia.seq)]\n",
      "    )\n",
      "259/7: lista\n",
      "259/8: tab = np.Array(lista)\n",
      "259/9: tab = np.array(lista)\n",
      "259/10: tab.shape()\n",
      "259/11: tab\n",
      "259/12: tab.shape\n",
      "259/13: tab\n",
      "259/14:\n",
      "s = []\n",
      "for i in tab:\n",
      "    for e in i:\n",
      "        s.append(e)\n",
      "set(se)\n",
      "259/15:\n",
      "s = []\n",
      "for i in tab:\n",
      "    for e in i:\n",
      "        s.append(e)\n",
      "set(s)\n",
      "259/16: ali = AlignIO.read(\"/home/fernando/git/llps/secuencias/fus_ort_aligned.fasta\", 'fasta')\n",
      "259/17:\n",
      "lista = []\n",
      "for secuencia in ali:\n",
      "    lista.append(\n",
      "        [aa for aa in str(secuencia.seq)]\n",
      "    )\n",
      "259/18: tab = np.array(lista)\n",
      "259/19:\n",
      "s = []\n",
      "for i in tab:\n",
      "    for e in i:\n",
      "        s.append(e)\n",
      "set(s)\n",
      "259/20: !pip install bokeh\n",
      "259/21:\n",
      "\n",
      "\n",
      "unemployment.py\n",
      "\n",
      "from math import pi\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "from bokeh.io import show\n",
      "from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.unemployment1948 import data\n",
      "\n",
      "data['Year'] = data['Year'].astype(str)\n",
      "data = data.set_index('Year')\n",
      "data.drop('Annual', axis=1, inplace=True)\n",
      "data.columns.name = 'Month'\n",
      "\n",
      "years = list(data.index)\n",
      "months = list(data.columns)\n",
      "\n",
      "# reshape to 1D array or rates with a month and year for each row.\n",
      "df = pd.DataFrame(data.stack(), columns=['rate']).reset_index()\n",
      "\n",
      "# this is the colormap from the original NYTimes plot\n",
      "259/22:\n",
      "from math import pi\n",
      "\n",
      "import pandas as pd\n",
      "\n",
      "from bokeh.io import show\n",
      "from bokeh.models import BasicTicker, ColorBar, LinearColorMapper, PrintfTickFormatter\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.unemployment1948 import data\n",
      "\n",
      "data['Year'] = data['Year'].astype(str)\n",
      "data = data.set_index('Year')\n",
      "data.drop('Annual', axis=1, inplace=True)\n",
      "data.columns.name = 'Month'\n",
      "\n",
      "years = list(data.index)\n",
      "months = list(data.columns)\n",
      "\n",
      "# reshape to 1D array or rates with a month and year for each row.\n",
      "df = pd.DataFrame(data.stack(), columns=['rate']).reset_index()\n",
      "\n",
      "# this is the colormap from the original NYTimes plot\n",
      "259/23: df\n",
      "259/24:\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "df = elements.copy()\n",
      "df[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\n",
      "df[\"group\"] = df[\"group\"].astype(str)\n",
      "df[\"period\"] = [periods[x-1] for x in df.period]\n",
      "df = df[df.group != \"-\"]\n",
      "df = df[df.symbol != \"Lr\"]\n",
      "df = df[df.symbol != \"Lu\"]\n",
      "\n",
      "cmap = {\n",
      "    \"alkali metal\"         : \"#a6cee3\",\n",
      "    \"alkaline earth metal\" : \"#1f78b4\",\n",
      "    \"metal\"                : \"#d93b43\",\n",
      "    \"halogen\"              : \"#999d9a\",\n",
      "    \"metalloid\"            : \"#e08d49\",\n",
      "    \"noble gas\"            : \"#eaeaea\",\n",
      "    \"nonmetal\"             : \"#f1d4Af\",\n",
      "    \"transition metal\"     : \"#599d7A\",\n",
      "}\n",
      "\n",
      "TOOLTIPS = [\n",
      "    (\"Name\", \"@name\"),\n",
      "    (\"Atomic number\", \"@{atomic number}\"),\n",
      "    (\"Atomic mass\", \"@{atomic mass}\"),\n",
      "    (\"Type\", \"@metal\"),\n",
      "    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n",
      "    (\"Electronic configuration\", \"@{electronic configuration}\"),\n",
      "]\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=groups, y_range=list(reversed(periods)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "text_props = {\"source\": df, \"text_align\": \"left\", \"text_baseline\": \"middle\"}\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.text(x=x, y=\"period\", text=\"symbol\", text_font_style=\"bold\", **text_props)\n",
      "\n",
      "p.text(x=x, y=dodge(\"period\", 0.3, range=p.y_range), text=\"atomic number\",\n",
      "       text_font_size=\"11px\", **text_props)\n",
      "\n",
      "p.text(x=x, y=dodge(\"period\", -0.35, range=p.y_range), text=\"name\",\n",
      "       text_font_size=\"7px\", **text_props)\n",
      "\n",
      "p.text(x=x, y=dodge(\"period\", -0.2, range=p.y_range), text=\"atomic mass\",\n",
      "       text_font_size=\"7px\", **text_props)\n",
      "\n",
      "p.text(x=[\"3\", \"3\"], y=[\"VI\", \"VII\"], text=[\"LA\", \"AC\"], text_align=\"center\", text_baseline=\"middle\")\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/25:\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "df = elements.copy()\n",
      "df[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\n",
      "df[\"group\"] = df[\"group\"].astype(str)\n",
      "df[\"period\"] = [periods[x-1] for x in df.period]\n",
      "df = df[df.group != \"-\"]\n",
      "df = df[df.symbol != \"Lr\"]\n",
      "df = df[df.symbol != \"Lu\"]\n",
      "\n",
      "cmap = {\n",
      "    \"alkali metal\"         : \"#a6cee3\",\n",
      "    \"alkaline earth metal\" : \"#1f78b4\",\n",
      "    \"metal\"                : \"#d93b43\",\n",
      "    \"halogen\"              : \"#999d9a\",\n",
      "    \"metalloid\"            : \"#e08d49\",\n",
      "    \"noble gas\"            : \"#eaeaea\",\n",
      "    \"nonmetal\"             : \"#f1d4Af\",\n",
      "    \"transition metal\"     : \"#599d7A\",\n",
      "}\n",
      "\n",
      "TOOLTIPS = [\n",
      "    (\"Name\", \"@name\"),\n",
      "    (\"Atomic number\", \"@{atomic number}\"),\n",
      "    (\"Atomic mass\", \"@{atomic mass}\"),\n",
      "    (\"Type\", \"@metal\"),\n",
      "    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n",
      "    (\"Electronic configuration\", \"@{electronic configuration}\"),\n",
      "]\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=groups, y_range=list(reversed(periods)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "text_props = {\"source\": df, \"text_align\": \"left\", \"text_baseline\": \"middle\"}\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.text(x=x, y=\"period\", text=\"symbol\", text_font_style=\"bold\", **text_props)\n",
      "\n",
      "p.text(x=x, y=dodge(\"period\", 0.3, range=p.y_range), text=\"atomic number\",\n",
      "       text_font_size=\"11px\", **text_props)\n",
      "\n",
      "p.text(x=x, y=dodge(\"period\", -0.35, range=p.y_range), text=\"name\",\n",
      "       text_font_size=\"7px\", **text_props)\n",
      "\n",
      "p.text(x=x, y=dodge(\"period\", -0.2, range=p.y_range), text=\"atomic mass\",\n",
      "       text_font_size=\"7px\", **text_props)\n",
      "\n",
      "p.text(x=[\"3\", \"3\"], y=[\"VI\", \"VII\"], text=[\"LA\", \"AC\"], text_align=\"center\", text_baseline=\"middle\")\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "#show(p)\n",
      "259/26: df\n",
      "259/27:\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "df = elements.copy()\n",
      "df[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\n",
      "df[\"group\"] = df[\"group\"].astype(str)\n",
      "df[\"period\"] = [periods[x-1] for x in df.period]\n",
      "df = df[df.group != \"-\"]\n",
      "df = df[df.symbol != \"Lr\"]\n",
      "df = df[df.symbol != \"Lu\"]\n",
      "\n",
      "cmap = {\n",
      "    \"alkali metal\"         : \"#a6cee3\",\n",
      "    \"alkaline earth metal\" : \"#1f78b4\",\n",
      "    \"metal\"                : \"#d93b43\",\n",
      "    \"halogen\"              : \"#999d9a\",\n",
      "    \"metalloid\"            : \"#e08d49\",\n",
      "    \"noble gas\"            : \"#eaeaea\",\n",
      "    \"nonmetal\"             : \"#f1d4Af\",\n",
      "    \"transition metal\"     : \"#599d7A\",\n",
      "}\n",
      "\n",
      "TOOLTIPS = [\n",
      "    (\"Name\", \"@name\"),\n",
      "    (\"Atomic number\", \"@{atomic number}\"),\n",
      "    (\"Atomic mass\", \"@{atomic mass}\"),\n",
      "    (\"Type\", \"@metal\"),\n",
      "    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n",
      "    (\"Electronic configuration\", \"@{electronic configuration}\"),\n",
      "]\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=groups, y_range=list(reversed(periods)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "#show(p)\n",
      "259/28:\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "df = elements.copy()\n",
      "df[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\n",
      "df[\"group\"] = df[\"group\"].astype(str)\n",
      "df[\"period\"] = [periods[x-1] for x in df.period]\n",
      "df = df[df.group != \"-\"]\n",
      "df = df[df.symbol != \"Lr\"]\n",
      "df = df[df.symbol != \"Lu\"]\n",
      "\n",
      "cmap = {\n",
      "    \"alkali metal\"         : \"#a6cee3\",\n",
      "    \"alkaline earth metal\" : \"#1f78b4\",\n",
      "    \"metal\"                : \"#d93b43\",\n",
      "    \"halogen\"              : \"#999d9a\",\n",
      "    \"metalloid\"            : \"#e08d49\",\n",
      "    \"noble gas\"            : \"#eaeaea\",\n",
      "    \"nonmetal\"             : \"#f1d4Af\",\n",
      "    \"transition metal\"     : \"#599d7A\",\n",
      "}\n",
      "\n",
      "TOOLTIPS = [\n",
      "    (\"Name\", \"@name\"),\n",
      "    (\"Atomic number\", \"@{atomic number}\"),\n",
      "    (\"Atomic mass\", \"@{atomic mass}\"),\n",
      "    (\"Type\", \"@metal\"),\n",
      "    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n",
      "    (\"Electronic configuration\", \"@{electronic configuration}\"),\n",
      "]\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=groups, y_range=list(reversed(periods)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/29:\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "df = elements.copy()\n",
      "df[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\n",
      "df[\"group\"] = df[\"group\"].astype(str)\n",
      "df[\"period\"] = [periods[x-1] for x in df.period]\n",
      "df = df[df.group != \"-\"]\n",
      "df = df[df.symbol != \"Lr\"]\n",
      "df = df[df.symbol != \"Lu\"]\n",
      "\n",
      "cmap = {\n",
      "    \"alkali metal\"         : \"#a6cee3\",\n",
      "    \"alkaline earth metal\" : \"#1f78b4\",\n",
      "    \"metal\"                : \"#d93b43\",\n",
      "    \"halogen\"              : \"#999d9a\",\n",
      "    \"metalloid\"            : \"#e08d49\",\n",
      "    \"noble gas\"            : \"#eaeaea\",\n",
      "    \"nonmetal\"             : \"#f1d4Af\",\n",
      "    \"transition metal\"     : \"#599d7A\",\n",
      "}\n",
      "\n",
      "TOOLTIPS = [\n",
      "    (\"Name\", \"@name\"),\n",
      "    (\"Atomic number\", \"@{atomic number}\"),\n",
      "    (\"Atomic mass\", \"@{atomic mass}\"),\n",
      "    (\"Type\", \"@metal\"),\n",
      "    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n",
      "    (\"Electronic configuration\", \"@{electronic configuration}\"),\n",
      "]\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=groups, y_range=list(reversed(periods)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "show(p)\n",
      "259/30:\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "df = elements.copy()\n",
      "df[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\n",
      "df[\"group\"] = df[\"group\"].astype(str)\n",
      "df[\"period\"] = [periods[x-1] for x in df.period]\n",
      "df = df[df.group != \"-\"]\n",
      "df = df[df.symbol != \"Lr\"]\n",
      "df = df[df.symbol != \"Lu\"]\n",
      "\n",
      "cmap = {\n",
      "    \"alkali metal\"         : \"#a6cee3\",\n",
      "    \"alkaline earth metal\" : \"#1f78b4\",\n",
      "    \"metal\"                : \"#d93b43\",\n",
      "    \"halogen\"              : \"#999d9a\",\n",
      "    \"metalloid\"            : \"#e08d49\",\n",
      "    \"noble gas\"            : \"#eaeaea\",\n",
      "    \"nonmetal\"             : \"#f1d4Af\",\n",
      "    \"transition metal\"     : \"#599d7A\",\n",
      "}\n",
      "\n",
      "TOOLTIPS = [\n",
      "    (\"Name\", \"@name\"),\n",
      "    (\"Atomic number\", \"@{atomic number}\"),\n",
      "    (\"Atomic mass\", \"@{atomic mass}\"),\n",
      "    (\"Type\", \"@metal\"),\n",
      "    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n",
      "    (\"Electronic configuration\", \"@{electronic configuration}\"),\n",
      "]\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=groups, y_range=list(reversed(periods)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "df = elements.copy()\n",
      "df[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\n",
      "df[\"group\"] = df[\"group\"].astype(str)\n",
      "df[\"period\"] = [periods[x-1] for x in df.period]\n",
      "df = df[df.group != \"-\"]\n",
      "df = df[df.symbol != \"Lr\"]\n",
      "df = df[df.symbol != \"Lu\"]\n",
      "\n",
      "cmap = {\n",
      "    \"alkali metal\"         : \"#a6cee3\",\n",
      "    \"alkaline earth metal\" : \"#1f78b4\",\n",
      "    \"metal\"                : \"#d93b43\",\n",
      "    \"halogen\"              : \"#999d9a\",\n",
      "    \"metalloid\"            : \"#e08d49\",\n",
      "    \"noble gas\"            : \"#eaeaea\",\n",
      "    \"nonmetal\"             : \"#f1d4Af\",\n",
      "    \"transition metal\"     : \"#599d7A\",\n",
      "}\n",
      "\n",
      "TOOLTIPS = [\n",
      "    (\"Name\", \"@name\"),\n",
      "    (\"Atomic number\", \"@{atomic number}\"),\n",
      "    (\"Atomic mass\", \"@{atomic mass}\"),\n",
      "    (\"Type\", \"@metal\"),\n",
      "    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n",
      "    (\"Electronic configuration\", \"@{electronic configuration}\"),\n",
      "]\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=groups, y_range=list(reversed(periods)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "text_props = {\"source\": df, \"text_align\": \"left\", \"text_baseline\": \"middle\"}\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "\n",
      "\n",
      "show(p)\n",
      "259/31:\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "df = elements.copy()\n",
      "df[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\n",
      "df[\"group\"] = df[\"group\"].astype(str)\n",
      "df[\"period\"] = [periods[x-1] for x in df.period]\n",
      "df = df[df.group != \"-\"]\n",
      "df = df[df.symbol != \"Lr\"]\n",
      "df = df[df.symbol != \"Lu\"]\n",
      "\n",
      "cmap = {\n",
      "    \"alkali metal\"         : \"#a6cee3\",\n",
      "    \"alkaline earth metal\" : \"#1f78b4\",\n",
      "    \"metal\"                : \"#d93b43\",\n",
      "    \"halogen\"              : \"#999d9a\",\n",
      "    \"metalloid\"            : \"#e08d49\",\n",
      "    \"noble gas\"            : \"#eaeaea\",\n",
      "    \"nonmetal\"             : \"#f1d4Af\",\n",
      "    \"transition metal\"     : \"#599d7A\",\n",
      "}\n",
      "\n",
      "TOOLTIPS = [\n",
      "    (\"Name\", \"@name\"),\n",
      "    (\"Atomic number\", \"@{atomic number}\"),\n",
      "    (\"Atomic mass\", \"@{atomic mass}\"),\n",
      "    (\"Type\", \"@metal\"),\n",
      "    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n",
      "    (\"Electronic configuration\", \"@{electronic configuration}\"),\n",
      "]\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=groups, y_range=list(reversed(periods)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "df = elements.copy()\n",
      "df[\"atomic mass\"] = df[\"atomic mass\"].astype(str)\n",
      "df[\"group\"] = df[\"group\"].astype(str)\n",
      "df[\"period\"] = [periods[x-1] for x in df.period]\n",
      "df = df[df.group != \"-\"]\n",
      "df = df[df.symbol != \"Lr\"]\n",
      "df = df[df.symbol != \"Lu\"]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"N\"         : \"green\",\n",
      "    \"N\"         : \"green\",\n",
      "    \"N\"         : \"green\",\n",
      "    \"N\"         : \"green\",\n",
      "    \"alkaline earth metal\" : \"#1f78b4\",\n",
      "    \"metal\"                : \"#d93b43\",\n",
      "    \"halogen\"              : \"#999d9a\",\n",
      "    \"metalloid\"            : \"#e08d49\",\n",
      "    \"noble gas\"            : \"#eaeaea\",\n",
      "    \"nonmetal\"             : \"#f1d4Af\",\n",
      "    \"transition metal\"     : \"#599d7A\",\n",
      "}\n",
      "\n",
      "TOOLTIPS = [\n",
      "    (\"Name\", \"@name\"),\n",
      "    (\"Atomic number\", \"@{atomic number}\"),\n",
      "    (\"Atomic mass\", \"@{atomic mass}\"),\n",
      "    (\"Type\", \"@metal\"),\n",
      "    (\"CPK color\", \"$color[hex, swatch]:CPK\"),\n",
      "    (\"Electronic configuration\", \"@{electronic configuration}\"),\n",
      "]\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=groups, y_range=list(reversed(periods)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"group\", \"period\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "text_props = {\"source\": df, \"text_align\": \"left\", \"text_baseline\": \"middle\"}\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "\n",
      "\n",
      "show(p)\n",
      "259/32: tab\n",
      "259/33: pd.DataFrame(tab)\n",
      "259/34:\n",
      "df = pd.DataFrame(tab)\n",
      "df.index = [seq.id for seq in ali]\n",
      "259/35: df\n",
      "259/36: df.iloc[:10,700:900]\n",
      "259/37: df = df.iloc[:10,700:900]\n",
      "259/38: df\n",
      "259/39: df.melt()\n",
      "259/40: df\n",
      "259/41: df.reset_index().melt()\n",
      "259/42: df.reset_index().melt(id_vars='index')\n",
      "259/43:\n",
      "df = df.reset_index().melt(id_vars='index')\n",
      "df.columns = ['uniprot', 'pos','aa']\n",
      "259/44: df\n",
      "259/45:\n",
      "d = {{seq.id:pos} for pos,seq in enumerate(ali)}\n",
      "df[\"x\"] = df.uniprot.map(d)\n",
      "259/46: d\n",
      "259/47:\n",
      "d = {{seq.id:pos} for pos,seq in enumerate(ali)}\n",
      "#df[\"x\"] = df.uniprot.map(d)\n",
      "259/48:\n",
      "d = {seq.id:pos for pos,seq in enumerate(ali)}\n",
      "df[\"x\"] = df.uniprot.map(d)\n",
      "259/49: d\n",
      "259/50: df\n",
      "259/51:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=len(df), y_range=len(df.columns),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"x\", \"res\", 0.95, 0.95, source=df.melt(), fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "\n",
      "\n",
      "show(p)\n",
      "259/52: df['y'] = df.pos - 700\n",
      "259/53:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=len(df), y_range=len(df.columns),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df.melt(), fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "\n",
      "\n",
      "show(p)\n",
      "259/54:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=(0, len(df)), y_range=(0, len(df.columns)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df.melt(), fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "\n",
      "\n",
      "show(p)\n",
      "259/55:\n",
      "df = pd.DataFrame(tab)\n",
      "df.index = [seq.id for seq in ali]\n",
      "259/56: #df = df.iloc[:10,700:900]\n",
      "259/57:\n",
      "df = df.reset_index().melt(id_vars='index')\n",
      "df.columns = ['uniprot', 'pos','aa']\n",
      "259/58:\n",
      "d = {seq.id:pos for pos,seq in enumerate(ali)}\n",
      "df[\"x\"] = df.uniprot.map(d)\n",
      "259/59: df['y'] = df.pos - 1\n",
      "259/60:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=(0, len(df)), y_range=(0, len(df.columns)),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df.melt(), fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "\n",
      "\n",
      "show(p)\n",
      "259/61: df\n",
      "259/62: df\n",
      "259/63: ali.get_alignment_length\n",
      "259/64: ali.get_alignment_length()\n",
      "259/65:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=(0, len(ali)), y_range=(0, ali.get_alignment_length()),\n",
      "           tools=\"hover\", toolbar_location=None, tooltips=TOOLTIPS)\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df.melt(), fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "\n",
      "\n",
      "show(p)\n",
      "259/66:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=(0, len(ali)), y_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df.melt(), fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "\n",
      "\n",
      "show(p)\n",
      "259/67:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=(0, len(ali)), y_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df.melt(), fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "#x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "#p.outline_line_color = None\n",
      "#p.grid.grid_line_color = None\n",
      "#p.axis.axis_line_color = None\n",
      "#p.axis.major_tick_line_color = None\n",
      "#p.axis.major_label_standoff = 0\n",
      "#p.legend.orientation = \"horizontal\"\n",
      "#p.legend.location =\"top_center\"\n",
      "#p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/68:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=(0, len(ali)), y_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "#x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "#p.outline_line_color = None\n",
      "#p.grid.grid_line_color = None\n",
      "#p.axis.axis_line_color = None\n",
      "#p.axis.major_tick_line_color = None\n",
      "#p.axis.major_label_standoff = 0\n",
      "#p.legend.orientation = \"horizontal\"\n",
      "#p.legend.location =\"top_center\"\n",
      "#p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/69:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=(0, len(ali)), y_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "#r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "#           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "#x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "#p.outline_line_color = None\n",
      "#p.grid.grid_line_color = None\n",
      "#p.axis.axis_line_color = None\n",
      "#p.axis.major_tick_line_color = None\n",
      "#p.axis.major_label_standoff = 0\n",
      "#p.legend.orientation = \"horizontal\"\n",
      "#p.legend.location =\"top_center\"\n",
      "#p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/70:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           x_range=(0, len(ali)), y_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "#r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "#           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "#x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "#p.outline_line_color = None\n",
      "#p.grid.grid_line_color = None\n",
      "#p.axis.axis_line_color = None\n",
      "#p.axis.major_tick_line_color = None\n",
      "#p.axis.major_label_standoff = 0\n",
      "#p.legend.orientation = \"horizontal\"\n",
      "#p.legend.location =\"top_center\"\n",
      "#p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/71:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           y_range=(0, len(ali)), x_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "#r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"metal\",\n",
      "#           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "#x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "#p.outline_line_color = None\n",
      "#p.grid.grid_line_color = None\n",
      "#p.axis.axis_line_color = None\n",
      "#p.axis.major_tick_line_color = None\n",
      "#p.axis.major_label_standoff = 0\n",
      "#p.legend.orientation = \"horizontal\"\n",
      "#p.legend.location =\"top_center\"\n",
      "#p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/72:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           y_range=(0, len(ali)), x_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"aa\",\n",
      "#           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "#x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "#p.outline_line_color = None\n",
      "#p.grid.grid_line_color = None\n",
      "#p.axis.axis_line_color = None\n",
      "#p.axis.major_tick_line_color = None\n",
      "#p.axis.major_label_standoff = 0\n",
      "#p.legend.orientation = \"horizontal\"\n",
      "#p.legend.location =\"top_center\"\n",
      "#p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/73:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           y_range=(0, len(ali)), x_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"aa\",\n",
      "           color=factor_cmap('metal', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "#x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "#p.outline_line_color = None\n",
      "#p.grid.grid_line_color = None\n",
      "#p.axis.axis_line_color = None\n",
      "#p.axis.major_tick_line_color = None\n",
      "#p.axis.major_label_standoff = 0\n",
      "#p.legend.orientation = \"horizontal\"\n",
      "#p.legend.location =\"top_center\"\n",
      "#p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/74:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           y_range=(0, len(ali)), x_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "r = p.rect(\"x\", \"y\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"aa\",\n",
      "           color=factor_cmap('aa', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "#x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "#p.outline_line_color = None\n",
      "#p.grid.grid_line_color = None\n",
      "#p.axis.axis_line_color = None\n",
      "#p.axis.major_tick_line_color = None\n",
      "#p.axis.major_label_standoff = 0\n",
      "#p.legend.orientation = \"horizontal\"\n",
      "#p.legend.location =\"top_center\"\n",
      "#p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/75:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           y_range=(0, len(ali)), x_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "r = p.rect(\"y\", \"x\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"aa\",\n",
      "           color=factor_cmap('aa', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "#x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "#p.outline_line_color = None\n",
      "#p.grid.grid_line_color = None\n",
      "#p.axis.axis_line_color = None\n",
      "#p.axis.major_tick_line_color = None\n",
      "#p.axis.major_label_standoff = 0\n",
      "#p.legend.orientation = \"horizontal\"\n",
      "#p.legend.location =\"top_center\"\n",
      "#p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/76:\n",
      "\n",
      "\n",
      "from bokeh.io import output_file, show\n",
      "from bokeh.plotting import figure\n",
      "from bokeh.sampledata.periodic_table import elements\n",
      "from bokeh.transform import dodge, factor_cmap\n",
      "\n",
      "output_file(\"periodic.html\")\n",
      "\n",
      "periods = [\"I\", \"II\", \"III\", \"IV\", \"V\", \"VI\", \"VII\"]\n",
      "groups = [str(x) for x in range(1, 19)]\n",
      "\n",
      "cmap = {\n",
      "    \"A\"         : \"blue\",\n",
      "    \"I\"         : \"blue\",\n",
      "    \"L\"         : \"blue\",\n",
      "    \"M\"         : \"blue\",\n",
      "    \"F\"         : \"blue\",\n",
      "    \"W\"         : \"blue\",\n",
      "    \"V\"         : \"blue\",\n",
      "    \"R\"         : \"red\",\n",
      "    \"K\"         : \"red\",\n",
      "    \"Q\"         : \"green\",\n",
      "    \"S\"         : \"green\",\n",
      "    \"T\"         : \"green\",\n",
      "    \"C\"         : \"pink\",\n",
      "    \"G\" : \"pink\",\n",
      "    \"P\"                : \"yellow\",\n",
      "    \"H\"              : \"cyan\",\n",
      "    \"Y\"            : \"cyan\",\n",
      "    \"-\"            : \"grey\",\n",
      "\n",
      "}\n",
      "\n",
      "\n",
      "p = figure(title=\"Periodic Table (omitting LA and AC Series)\", plot_width=1000, plot_height=450,\n",
      "           y_range=(0, len(ali)), x_range=(0, ali.get_alignment_length()))\n",
      "\n",
      "r = p.rect(\"y\", \"x\", 0.95, 0.95, source=df, fill_alpha=0.6, legend_field=\"aa\",\n",
      "           color=factor_cmap('aa', palette=list(cmap.values()), factors=list(cmap.keys())))\n",
      "\n",
      "\n",
      "\n",
      "x = dodge(\"group\", -0.4, range=p.x_range)\n",
      "\n",
      "p.outline_line_color = None\n",
      "p.grid.grid_line_color = None\n",
      "p.axis.axis_line_color = None\n",
      "p.axis.major_tick_line_color = None\n",
      "p.axis.major_label_standoff = 0\n",
      "p.legend.orientation = \"horizontal\"\n",
      "p.legend.location =\"top_center\"\n",
      "p.hover.renderers = [r] # only hover element boxes\n",
      "\n",
      "show(p)\n",
      "259/77:\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "fig = go.Figure(data=go.Heatmap(\n",
      "                   z=[[1, None, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, -10, 20]],\n",
      "                   x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n",
      "                   y=['Morning', 'Afternoon', 'Evening'],\n",
      "                   hoverongaps = False))\n",
      "fig.show()\n",
      "259/78: import chart_studio.plotly as py\n",
      "259/79:\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "fig = go.Figure(data=go.Heatmap(\n",
      "                   z=[[1, None, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, -10, 20]],\n",
      "                   x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n",
      "                   y=['Morning', 'Afternoon', 'Evening'],\n",
      "                   hoverongaps = False))\n",
      "iplot(fig)\n",
      "259/80: !pip install chart_studio\n",
      "259/81: import chart_studio.plotly as py\n",
      "259/82:\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "fig = go.Figure(data=go.Heatmap(\n",
      "                   z=[[1, None, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, -10, 20]],\n",
      "                   x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n",
      "                   y=['Morning', 'Afternoon', 'Evening'],\n",
      "                   hoverongaps = False))\n",
      "py.iplot(fig)\n",
      "259/83:\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "fig = go.Figure(data=go.Heatmap(\n",
      "                   z=[[1, None, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, -10, 20]],\n",
      "                   x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n",
      "                   y=['Morning', 'Afternoon', 'Evening'],\n",
      "                   hoverongaps = False))\n",
      "plotly.io.show(fig)\n",
      "259/84:\n",
      "import plotly\n",
      "import plotly.graph_objects as go\n",
      "\n",
      "fig = go.Figure(data=go.Heatmap(\n",
      "                   z=[[1, None, 30, 50, 1], [20, 1, 60, 80, 30], [30, 60, 1, -10, 20]],\n",
      "                   x=['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday'],\n",
      "                   y=['Morning', 'Afternoon', 'Evening'],\n",
      "                   hoverongaps = False))\n",
      "plotly.io.show(fig)\n",
      "259/85: lista\n",
      "259/86: %history\n",
      "263/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "import pickle\n",
      "\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "263/2: human_db = tablas['db'].query('org == \"Homo sapiens\"')\n",
      "263/3: tablas['human_dis_box1']\n",
      "263/4: tablas['human_dz_box1']\n",
      "263/5: tablas['human_dz_box1'].to_csv(\"~/idrs_box1_mobidb.csv\", index=False)\n",
      "263/6: tablas['human_lc'].query('method == \"SEG_intermediate\"')\n",
      "263/7: tablas['human_lc'].query('method == \"SEG_intermediate\"')[['uniprot','start','end','seq','largo']]\n",
      "263/8: tablas['human_lc'].query('method == \"SEG_intermediate\"')[['uniprot','start','end','seq','largo']].to_csv('~/lc_zones_box1.csv',index=False)\n",
      "263/9: a = pd.read_html('https://www.ncbi.nlm.nih.gov/snp/rs1085307052')\n",
      "263/10: a\n",
      "263/11: a[-1]\n",
      "263/12: a[-2]\n",
      "263/13: a[-3]\n",
      "263/14: a\n",
      "263/15:\n",
      "for i in a:\n",
      "    display(a)\n",
      "263/16:\n",
      "for i in a:\n",
      "    print(i.head())\n",
      "263/17: a[1]\n",
      "264/1: import pandas as pd\n",
      "264/2: df = pd.read_csv('/home/fernando/Downloads/ViewingActivity.csv')\n",
      "264/3: df\n",
      "264/4: df.Duration\n",
      "264/5: df.Duration.asdate()\n",
      "264/6: df.Duration.as_date()\n",
      "264/7: df\n",
      "264/8: df.Duration.as_date().Title.value_counts()\n",
      "264/9: df.Duration.Title.value_counts()\n",
      "264/10: df.Title.value_counts()\n",
      "264/11: df#.Title.value_counts()\n",
      "264/12: df.query('Profile Name == \"Fer\"')#.Title.value_counts()\n",
      "264/13: df[df['Profile Name'] == \"Fer\"]#.Title.value_counts()\n",
      "264/14: df[df['Profile Name'] == \"Fer\"].Title.value_counts()\n",
      "264/15: df[df['Profile Name'] == \"Fer\"].Title.value_counts()[:20]\n",
      "266/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "266/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "266/3:\n",
      "dominios = tablas['human_dominios'].copy()\n",
      "dominios_boxes = dominios[[\"uniprot\",'domain']].merge(boxes[['uniprot','box']].drop_duplicates())\n",
      "266/4:\n",
      "# DOminios vistos en box 1\n",
      "dominios_box1 = ['RRM_1','PLD','Helicase_C','DEAD','WD40','Pkinase','LSM','zf-RanBP','SH3_1','SH2']\n",
      "266/5: boxes = tablas['human_boxes'][['uniprot','box']].drop_duplicates()\n",
      "266/6: boxes.box.value_counts()\n",
      "266/7:\n",
      "dominios = tablas['human_dominios'].copy()\n",
      "dominios_boxes = dominios[[\"uniprot\",'domain']].merge(boxes[['uniprot','box']].drop_duplicates())\n",
      "266/8:\n",
      "# DOminios vistos en box 1\n",
      "dominios_box1 = ['RRM_1','PLD','Helicase_C','DEAD','WD40','Pkinase','LSM','zf-RanBP','SH3_1','SH2']\n",
      "266/9: dominios_pivot = dominios_boxes[dominios_boxes.domain.isin(dominios_box1)].drop_duplicates([\"uniprot\",'box','domain']).pivot_table(index=\"domain\", columns='box', aggfunc='size')\n",
      "266/10: #dominios_pivot['suma'] = dominios_pivot.sum(1)\n",
      "266/11: boxes.box.value_counts()\n",
      "266/12:\n",
      "dominios_pivot_freq = dominios_pivot.copy()\n",
      "for col in dominios_pivot_freq.columns:\n",
      "    dominios_pivot_freq[col] = dominios_pivot[col] / boxes.box.value_counts()[col]\n",
      "266/13:\n",
      "colors = tablas['colors_3b']\n",
      "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n",
      "266/14:\n",
      "for box in ['box1','box2','box3']:\n",
      "    print(box)\n",
      "266/15:\n",
      "ax = dominios_pivot_freq.sort_values('box1',ascending=False).plot.bar(figsize=(9,3), edgecolor='white', width=0.8,rot=0)\n",
      "\n",
      "\n",
      "plt.legend(['Box 1', 'Box 2', 'Box 3'])\n",
      "266/16:\n",
      "sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/swiss_prot.tab', sep='\\t')\n",
      "\n",
      "dfs = []\n",
      "for df in pd.read_csv('/home/fernando/Descargas/Pfam-A.regions.tsv.gz',chunksize=500000, compression='gzip',sep='\\t'):\n",
      "\n",
      "    df = df[df.pfamseq_acc.isin(sp.Entry)]\n",
      "    dfs.append(df)   \n",
      "\n",
      "dominios_sp = pd.concat(dfs)\n",
      "266/17:\n",
      "plaac = pd.read_csv(\"/home/fernando/datos/plaac_swiss_prot_human.tsv\",sep='\\t')\n",
      "plaac[\"uniprot\"] = plaac.SEQid.str.split(\"|\").str[1]\n",
      "plaac = plaac[~plaac.COREscore.isna()]\n",
      "\n",
      "plaac = plaac[['uniprot','COREstart','COREend']].set_axis(['uniprot','seq_start','seq_end'],1)\n",
      "\n",
      "plaac['domain'] = 'PLD'\n",
      "\n",
      "dominios = pd.concat([dominios_sp,plaac], ignore_index=True)\n",
      "266/18:\n",
      "ax = dominios_pivot_freq.sort_values('box1',ascending=False).plot.bar(figsize=(9,3), edgecolor='white', width=0.8,rot=0)\n",
      "\n",
      "\n",
      "plt.legend(['Elementals', 'Clients', 'Regulators'])\n",
      "266/19:\n",
      "ax = dominios_pivot_freq.sort_values('box1',ascending=False).plot.bar(figsize=(9,3), edgecolor='white', width=0.8,rot=0)\n",
      "\n",
      "\n",
      "plt.legend(['Elementals', 'Clients', 'Regulators'])\n",
      "plt.savefig('~/plot.svg')\n",
      "266/20:\n",
      "ax = dominios_pivot_freq.sort_values('box1',ascending=False).plot.bar(figsize=(9,3), edgecolor='white', width=0.8,rot=0)\n",
      "\n",
      "\n",
      "plt.legend(['Elementals', 'Clients', 'Regulators'])\n",
      "plt.savefig('plot.svg')\n",
      "266/21: dominios_pivot_freq.sort_values('box1',ascending=False)\n",
      "266/22:\n",
      "to_plot = dominios_pivot_freq.sort_values('box1',ascending=False)\n",
      "to_plot['Swiss-Prot'] = 0\n",
      "266/23:\n",
      "to_plot = dominios_pivot_freq.sort_values('box1',ascending=False)\n",
      "to_plot['Swiss-Prot'] = 0\n",
      "266/24:\n",
      "ax = dominios_pivot_freq.sort_values('box1',ascending=False).plot.bar(figsize=(9,3), edgecolor='white', width=0.8,rot=0)\n",
      "\n",
      "\n",
      "plt.legend(['Elementals', 'Clients', 'Regulators', 'Swiss-Prot'])\n",
      "plt.savefig('plot.svg')\n",
      "266/25:\n",
      "ax = to_plot.plot.bar(figsize=(9,3), edgecolor='white', width=0.8,rot=0)\n",
      "\n",
      "\n",
      "plt.legend(['Elementals', 'Clients', 'Regulators', 'Swiss-Prot'])\n",
      "plt.savefig('plot.svg')\n",
      "266/26:\n",
      "colors = tablas['colors_3b'].copy()\n",
      "colors.append(\"silver\")#[0.5,0.5,0.5])\n",
      "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n",
      "266/27:\n",
      "ax = to_plot.plot.bar(figsize=(9,3), edgecolor='white', width=0.8,rot=0)\n",
      "\n",
      "\n",
      "plt.legend(['Elementals', 'Clients', 'Regulators', 'Swiss-Prot'])\n",
      "plt.savefig('plot.svg')\n",
      "267/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "267/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "267/3: boxes = tablas['human_boxes'].drop_duplicates(['uniprot','box'])\n",
      "267/4: unis = {b:boxes.query('box == @b').uniprot.tolist() for b in boxes.box.unique()}\n",
      "267/5:\n",
      "unis['sp'] = tablas['human_uniprots_sp']\n",
      "unis['usp'] = [u for u in tablas['human_uniprots_sp'] if u not in tablas['db'].uniprot.tolist()]\n",
      "267/6: [len(i) for i in unis.values()]\n",
      "267/7:\n",
      "dfs_list = []\n",
      "for box,us in unis.items():\n",
      "    subset = ptms_sp[ptms_sp.uniprot.isin(us)]\n",
      "    df = subset.groupby('uniprot').agg('size').reset_index()\n",
      "    df.columns = ['uniprot','counts']\n",
      "    df['source'] = box\n",
      "    dfs_list.append(df)\n",
      "todo = pd.concat(dfs_list)\n",
      "267/8: tablas['human_db']\n",
      "267/9: boxes\n",
      "267/10: boxes.drop_duplicates(['uniprot','box'])\n",
      "267/11: boxes.drop_duplicates(['uniprot','box']).uniprot.value_counts()\n",
      "267/12:\n",
      "dups = boxes.drop_duplicates(['uniprot','box']).uniprot.value_counts()\n",
      "dups[dups > 1]\n",
      "267/13:\n",
      "dups = boxes.drop_duplicates(['uniprot','box']).uniprot.value_counts()\n",
      "dups.to_frame()\n",
      "267/14:\n",
      "dups = boxes.drop_duplicates(['uniprot','box']).uniprot.value_counts()\n",
      "dups = dups.to_frame().reset_index()\n",
      "267/15: dusp\n",
      "267/16: dups\n",
      "267/17:\n",
      "dups = boxes.drop_duplicates(['uniprot','box']).uniprot.value_counts()\n",
      "dups = dups.to_frame().reset_index()\n",
      "dups.columns = ['uniprot','nboxes']\n",
      "267/18: dups = boxes.merge(dups)\n",
      "267/19: dusp\n",
      "267/20: dups\n",
      "267/21: dups.sort_values('nboxes', ascending=False)\n",
      "267/22: dups['prot'] = dups.uniprot.map(tablas['gene_names_dict'])\n",
      "267/23: dups.sort_values('nboxes', ascending=False)\n",
      "267/24: dups.sort_values('nboxes', ascending=False).head(50)\n",
      "267/25: dups.sort_values('nboxes', ascending=False).head(50).sort_values('prot')\n",
      "267/26: dups[dups.uniprot.isin(tablas['db'].query('db == \"phasepro\"').uniprot)].query('nboxes > 1')\n",
      "267/27: tablas['db'].query('uniprot == \"Q8NE35\"').query('db == \"phasepro\"')\n",
      "267/28: tablas['human_db']\n",
      "267/29: tablas\n",
      "267/30: tablas.keys()\n",
      "267/31: tablas.query('mlo_loc == \"nuclear_pore_complex\"')\n",
      "267/32: tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"')\n",
      "267/33: tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"')\n",
      "267/34: tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"').uniprot.to_list()\n",
      "267/35: tablas['human_dominios']tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"').uniprot.to_list()\n",
      "267/36: tablas['human_dominios']#tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"').uniprot.to_list()\n",
      "267/37: tablas['human_dominios'][tabas['human_dominios'].uniprot.isin(tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"').uniprot.to_list())]\n",
      "267/38: tablas['human_dominios'][tablas['human_dominios'].uniprot.isin(tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"').uniprot.to_list())]\n",
      "267/39: tablas['human_dominios'][tablas['human_dominios'].uniprot.isin(tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"').uniprot.to_list())].domain.value_counts()\n",
      "267/40: tablas['human_dominios'][tablas['human_dominios'].uniprot.isin(tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"').uniprot.to_list())]#.domain.value_counts()\n",
      "267/41: tablas['human_dominios'][tablas['human_dominios'].uniprot.isin(tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"').uniprot.to_list())].drop_duplicates([\"uniprot\",'domain'])#.domain.value_counts()\n",
      "267/42: tablas['human_dominios'][tablas['human_dominios'].uniprot.isin(tablas[\"db\"].query('mlo_loc == \"nuclear_pore_complex\"').query('org_short == \"Homo sapiens\"').uniprot.to_list())].drop_duplicates([\"uniprot\",'domain']).domain.value_counts()\n",
      "267/43: tablas['db'].query('uniprot == \"P62826\"')\n",
      "267/44: dups\n",
      "267/45: tablas.keys()\n",
      "267/46: tablas.keys('human_dominios')\n",
      "267/47: tablas['human_dominios']\n",
      "267/48: tablas['human_dominios'].query('domain == \"RanBP\"')\n",
      "267/49: tablas['human_dominios'].query('domain == \"zf-RanBP\"')\n",
      "267/50: tablas['human_dominios'].query('domain == \"zf-RanBP\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict']))\n",
      "267/51: tablas['human_lc']\n",
      "267/52: tablas['human_lc'].query('uniprot == \"P49792\"')\n",
      "267/53: tablas['human_dominios'].query('domain == \"zf-RanBP\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).prot.unique.tolist()\n",
      "267/54: tablas['human_dominios'].query('domain == \"zf-RanBP\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).prot.unique.to_list()\n",
      "267/55: tablas['human_dominios'].query('domain == \"zf-RanBP\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).prot.unique().to_list()\n",
      "267/56: tablas['human_dominios'].query('domain == \"zf-RanBP\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).prot.unique().tolist()\n",
      "267/57: rbps = tablas['human_dominios'].query('domain == \"zf-RanBP\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).prot.unique().tolist()\n",
      "267/58: boxes\n",
      "267/59: boxes[boxes.uniprot.isin(rbps)]\n",
      "267/60: boxes\n",
      "267/61: rbps = tablas['human_dominios'].query('domain == \"zf-RanBP\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).uniprot.unique().tolist()\n",
      "267/62: boxes[boxes.uniprot.isin(rbps)]\n",
      "267/63: boxes[boxes.uniprot.isin(rbps)].assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict']))\n",
      "267/64: boxes[boxes.uniprot.isin(rbps)].assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).sort_values('box')\n",
      "267/65: tablas['db'].query('uniprot == \"P49792\"')\n",
      "267/66: boxes.query('uniprot == \"P49792\"')\n",
      "267/67: boxes.query('uniprot == \"P62826\"')\n",
      "267/68: tablas['db'].query('uniprot == \"P62826\"')\n",
      "267/69: boxes\n",
      "267/70: boxes.box.value_counts()\n",
      "267/71: tablas['human_dominios']\n",
      "267/72: tablas['human_dominios'].query('domain == \"HMG_box\"')\n",
      "267/73: tablas['human_dominios'].query('domain == \"HMG_box\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict']))\n",
      "267/74: tablas['human_dominios'].query('domain == \"HMG_box\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).drop_duplicates([\"uniprot\",'mlo_loc','domain'])\n",
      "267/75: tablas['human_dominios'].query('domain == \"HMG_box\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).drop_duplicates([\"uniprot\",'mlo_loc','domain'])[['uniprot','mlo_loc','prot','box']]\n",
      "267/76: tablas['human_dominios'].query('domain == \"HMG_box\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).drop_duplicates([\"uniprot\",'mlo_loc','domain'])[['uniprot','mlo_loc','prot','box']].sort_values(['box','domain'])\n",
      "267/77: tablas['human_dominios'].query('domain == \"HMG_box\"').assign(prot = lambda x: x['uniprot'].map(tablas['gene_names_dict'])).drop_duplicates([\"uniprot\",'mlo_loc','domain'])[['uniprot','mlo_loc','prot','box']].sort_values(['box','prot'])\n",
      "267/78: tablas['db'].query('uniprot == \"P23497\"')\n",
      "268/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "268/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "268/3: boxes = tablas['human_boxes']\n",
      "268/4: boxes.box.value_counts()\n",
      "268/5: db = boxes.merge(tablas['tidy'][[\"uniprot\",'org']]).query('org == \"Homo sapiens\"')\n",
      "268/6: db = db.drop_duplicates()\n",
      "268/7: db.drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "268/8: db.drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "268/9: db.query('box == \"box1\"').mlo_loc.value_counts()\n",
      "268/10:\n",
      "mlos = db.query('box == \"box1\"').drop_duplicates([\"uniprot\",'mlo_loc']).mlo_loc.value_counts().index.tolist()\n",
      "mlos.append(mlos.pop(mlos.index('other')))\n",
      "mlos\n",
      "268/11: box_counts = db.drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "268/12:\n",
      "pivot = db.drop_duplicates().pivot_table(index='mlo_loc',columns='box',aggfunc='size')\n",
      "pivot[\"total\"] = pivot.sum(1)\n",
      "268/13:\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['total']\n",
      "268/14: pivot = pivot.reindex(reversed(mlos))\n",
      "268/15: pivot = pivot.set_index(pivot.index.map(tablas['translate']))\n",
      "268/16: pivot.columns = ['Box 1', 'Box 2', 'Box 3','total']\n",
      "268/17:\n",
      "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
      "colors = [i for i in colors if i != \"#ff7f0e\"]\n",
      "#colors[0] = \"#3273a099\"\n",
      "mpl.rcParams['axes.prop_cycle'] = mpl.cycler(color=colors)\n",
      "268/18:\n",
      "ax = pivot.drop(columns=\"total\").plot.barh(stacked=True, width=.75)\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "#plt.savefig('figuras/2_mlo_counts.svg')\n",
      "268/19: pivot\n",
      "268/20:\n",
      "ax = pivot.reindex([tablas[\"translate\"][i] for i in reversed(mlos)]).drop(columns=\"total\").sort_values(\"Box 1\").drop('Other').plot.barh(stacked=True, width=.75,color=tablas['colors_3b'])\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "268/21:\n",
      "ax = pivot.reindex([tablas[\"translate\"][i] for i in reversed(mlos)]).drop(columns=\"total\").sort_values(\"Box 1\").drop('Other').plot.barh(stacked=True, width=.75,color=tablas['colors_3b'])\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend([\"Elementals\", 'Clients','Regulators']loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "plt.savefig('mlos.svg')\n",
      "268/22:\n",
      "ax = pivot.reindex([tablas[\"translate\"][i] for i in reversed(mlos)]).drop(columns=\"total\").sort_values(\"Box 1\").drop('Other').plot.barh(stacked=True, width=.75,color=tablas['colors_3b'])\n",
      "# Put a legend below current axis\n",
      "plt.yticks(fontsize=14)\n",
      "ax.legend([\"Elementals\", 'Clients','Regulators'],loc='upper center', bbox_to_anchor=(0.5, -0.1),\n",
      "          fancybox=True, shadow=True, ncol=5)\n",
      "plt.savefig('mlos.svg')\n",
      "271/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "import pickle\n",
      "\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "271/2: human_db = tablas['db'].query('org == \"Homo sapiens\"')\n",
      "271/3: %history\n",
      "271/4: %history -g\n",
      "271/5: lc = tablas['human_lc'].query('method == \"SEG_intermediate\"')\n",
      "271/6: boxes = tablas['human_boxes'].copy()\n",
      "271/7: lc_box = lc.merge(boxes)\n",
      "271/8: lc_box.description = lc_box.description.map(eval)\n",
      "271/9: lc_box.entropia = lc_box.entropia.map(eval)\n",
      "271/10: lc_box.entropia = lc_box.entropia.map(np.mean)\n",
      "271/11: import numpy as np\n",
      "271/12: lc_box.entropia = lc_box.entropia.map(np.mean)\n",
      "271/13: lc = lc_box[['uniprot',\"description\",'subzonas',\"start\",'end','seq','largo','naas', 'entropia',\"mlo_loc\",'box']]\n",
      "271/14: lc\n",
      "271/15: lc.explode('description')\n",
      "271/16: lcexp = lc.explode('description')\n",
      "271/17: lcexp\n",
      "271/18: lcexp[\"rich\"] = lcexp.description.str.split(' ').str[0]\n",
      "271/19: lcexp\n",
      "271/20: lcexp[lcexp.rich.contains('P')]\n",
      "271/21: lcexp[lcexp.rich.str.contains('P')]\n",
      "271/22: lcexp[lcexp.rich.str.contains('P')].drop_duplicates(])\n",
      "271/23: lcexp[lcexp.rich.str.contains('P')].drop_duplicates([\"uniprot\",'rich','box'])\n",
      "271/24: lcexp[lcexp.rich.str.contains('P')].drop_duplicates([\"uniprot\",'rich','box']).box.value_counts()\n",
      "271/25: prich = lcexp[lcexp.rich.str.contains('P')].drop_duplicates([\"uniprot\",'rich','box']).box.value_counts()\n",
      "271/26: prich = lcexp[lcexp.rich.str.contains('P')].drop_duplicates([\"uniprot\",'box']).box.value_counts()\n",
      "271/27: prich\n",
      "271/28: prich\n",
      "271/29: prich = lcexp[lcexp.rich.str.contains('P')].drop_duplicates([\"uniprot\",'box'])#.box.value_counts()\n",
      "271/30: prich\n",
      "271/31: prich = lcexp[lcexp.rich.str.contains('P')].drop_duplicates([\"uniprot\",'mlo_loc','box'])#.box.value_counts()\n",
      "271/32: prich\n",
      "271/33: prich[\"prot\"] = prich.uniprot.map(tablas['gene_names_dict'])\n",
      "271/34: prich\n",
      "271/35: prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x)))\n",
      "271/36: prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x))).prot.to_frame()\n",
      "271/37: prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x)))#.prot.to_frame()\n",
      "271/38: prich[\"prot\"] = prich.uniprot.map(tablas['gene_names_dict'])\n",
      "271/39: prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x)))#.prot.to_frame()\n",
      "271/40: prich\n",
      "271/41: prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x.fillna(\"\"))))#.prot.to_frame()\n",
      "271/42: prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x.fillna(\"\")))).prot.to_frame()\n",
      "271/43: prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x.fillna(\"\")))).prot#.to_frame()\n",
      "271/44: for i,e in prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x.fillna(\"\")))).prot: print(i,e)\n",
      "271/45: for i,e in prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x.fillna(\"\")))).prot.items(): print(i,e)\n",
      "271/46: for i,e in prich.groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x.fillna(\"\")))).prot.items(): print(i,'\\n',e,'\\n\\n')\n",
      "271/47: for i,e in prich.query('box == \"box1\"').groupby('mlo_loc').agg(lambda x: \", \".join(sorted(x.fillna(\"\")))).prot.items(): print(i,'\\n',e,'\\n\\n')\n",
      "274/1:\n",
      "    import pickle\n",
      "    import matplotlib.pyplot as plt\n",
      "    import seaborn as sns\n",
      "    import pandas as pd\n",
      "    import matplotlib as mpl\n",
      "274/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "274/3: tablas['human_disorder']\n",
      "274/4: tablas['human_dc']\n",
      "274/5: tablas.keys()\n",
      "274/6: tablas['human_dis']\n",
      "274/7: tablas['human_boxes']\n",
      "274/8: tablas['human_dis'].drop(columns='box')\n",
      "274/9: tablas['human_dis'].drop(columns='box').merge(tablas['human_boxes'])\n",
      "274/10: dis = tablas['human_dis'].drop(columns='box').merge(tablas['human_boxes'])\n",
      "274/11: dis\n",
      "274/12: dis.query('uniprot == \"A0A0A0MT08\"')\n",
      "274/13: tablas['human_boxes'].uniprot.value_counes()\n",
      "274/14: tablas['human_boxes'].uniprot.value_counts()\n",
      "274/15: dis.query('uniprot == \"P61978\"')\n",
      "274/16: dis.query('uniprot == \"Q01844\"')\n",
      "274/17: dis.query('uniprot == \"P61978\"')\n",
      "274/18: tablas['human_boxes'].uniprot.value_counts()[:20]\n",
      "274/19: dis.query('uniprot == \"P22626\"')\n",
      "274/20: dis.query('uniprot == \"P23396\"')\n",
      "274/21: todo = tablas['human_dis'].drop(columns='box').merge(tablas['human_boxes'])\n",
      "274/22:\n",
      "# Una barra por MLO, con las proporciones de proteinas High, Middle o Low disorder\n",
      "todo['estructura'] =  todo.dc.map(lambda x: 'Low' if x < 0.3 else ( 'High' if x >= .7 else 'Middle'))\n",
      "todo['mlo_label'] = todo['mlo_loc'].map(lab_translate)\n",
      "274/23:\n",
      "# Una barra por MLO, con las proporciones de proteinas High, Middle o Low disorder\n",
      "todo['estructura'] =  todo.dc.map(lambda x: 'Low' if x < 0.3 else ( 'High' if x >= .7 else 'Middle'))\n",
      "todo['mlo_label'] = todo['mlo_loc'].map(tablas['translate'])\n",
      "274/24: todo\n",
      "274/25: dis = todo.copy()\n",
      "274/26: todo = dis.query('box == \"box1\"')\n",
      "274/27:\n",
      "pivot = todo.pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"dc\"].sort_values('High')\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/28:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/29:\n",
      "# load control table\n",
      "control = pd.read_csv('/home/fernando/git/mlo/todo/tablas/control_disorder.csv')\n",
      "control = control[['uniprot', 'mlo', 'dc']].rename(columns={'mlo':'mlo_loc'})\n",
      "#control = control[~control.uniprot.isin(db.uniprot)]\n",
      "control['box'] = 'Swiss-Prot'\n",
      "control = control[['uniprot','mlo_loc','box','dc']]\n",
      "274/30: control\n",
      "274/31: pd.concat(todo, control)\n",
      "274/32: pd.concat([todo, control])\n",
      "274/33: control['mlo_label'] = 'Swiss-Prot'\n",
      "274/34: todo.merge([todo,control])\n",
      "274/35: control\n",
      "274/36: todo\n",
      "274/37: todo.concat([todo,control])\n",
      "274/38: pd.concat([todo,control])\n",
      "274/39: todo = pd.concat([todo,control])\n",
      "274/40: dis = todo.copy()\n",
      "274/41: todO\n",
      "274/42: todo\n",
      "274/43: todo = dis[dis.box.isin(['box1','Swiss-Prot'])]\n",
      "274/44:\n",
      "pivot = todo.pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"dc\"].sort_values('High')\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/45:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/46:\n",
      "pivot = todo.pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"dc\"].sort_values('High')\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/47:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/48: pivot\n",
      "274/49: todo\n",
      "274/50:\n",
      "# Una barra por MLO, con las proporciones de proteinas High, Middle o Low disorder\n",
      "todo['estructura'] =  todo.dc.map(lambda x: 'Low' if x < 0.3 else ( 'High' if x >= .7 else 'Middle'))\n",
      "#todo['mlo_label'] = todo['mlo_loc'].map(tablas['translate'])\n",
      "274/51:\n",
      "pivot = todo.pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"dc\"].sort_values('High')\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/52:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/53: todo\n",
      "274/54:\n",
      "pivot = todo.pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"dc\"].sort_values('High')\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/55:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/56: pivot\n",
      "274/57: pivot\n",
      "274/58: todo\n",
      "274/59: todo.query('estructura').value_counts(dropna=False)\n",
      "274/60: todo.query('mlo_label == \"Swiss-Prot\"').estructura.value_counts(dropna=False)\n",
      "274/61: pivot = todo.pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"dc\"].sort_values('High')\n",
      "274/62: pivot\n",
      "274/63:\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/64: pivot.sum()\n",
      "274/65: pivot.sum(1)\n",
      "274/66:\n",
      "pivot.sum(2\n",
      ")\n",
      "274/67:\n",
      "pivot.sum(1\n",
      ")\n",
      "274/68: pivot\n",
      "274/69: pivot = todo.pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"dc\"].sort_values('High')\n",
      "274/70:\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "274/71: pivot\n",
      "274/72: pivot = todo.pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"dc\"].sort_values('High')\n",
      "274/73: pivot\n",
      "274/74: pivot.sum()\n",
      "274/75: pivot.sum(1)\n",
      "274/76: pivot['All'] = pivot.suma(1)\n",
      "274/77: pivot['All'] = pivot.sum(1)\n",
      "274/78:\n",
      "pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/79:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/80: pivot = todo.pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"dc\"].sort_values('High')\n",
      "274/81: pivot['All'] = pivot.sum(1)\n",
      "274/82:\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "274/83:\n",
      "pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/84:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/85: todO\n",
      "274/86: todo\n",
      "274/87: todo[['uniprot','estructura','box','mlo_label']]\n",
      "274/88: todo[['uniprot','estructura','box','mlo_label']].dropna()\n",
      "274/89: todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)\n",
      "274/90: todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='count', margins=True)[\"box\"]\n",
      "274/91: todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size', margins=True)\n",
      "274/92: todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "274/93: pivot = todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "274/94: pivot['All'] = pivot.sum(1)\n",
      "274/95:\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "274/96:\n",
      "pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/97:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/98:\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/99:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/100: dis = todo.copy()\n",
      "274/101: todo = dis[dis.box.isin(['box1','Swiss-Prot'])]\n",
      "274/102: pivot = todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "274/103: pivot['All'] = pivot.sum(1)\n",
      "274/104:\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "274/105:\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "274/106:\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/107:\n",
      "pivot = todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3.svg')\n",
      "274/108: todo = dis[dis.box.isin(['box2','Swiss-Prot'])]\n",
      "274/109:\n",
      "pivot = todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('figuras/disorder_3_box2.svg')\n",
      "274/110: pivot\n",
      "274/111: dis\n",
      "274/112: dis.box.value_counts()\n",
      "274/113: todo = tablas['human_dis'].drop(columns='box').merge(tablas['human_boxes'])\n",
      "274/114: control['mlo_label'] = 'Swiss-Prot'\n",
      "274/115: todo = pd.concat([todo,control])\n",
      "274/116: dis = todo.copy()\n",
      "274/117:\n",
      "# Una barra por MLO, con las proporciones de proteinas High, Middle o Low disorder\n",
      "todo['estructura'] =  todo.dc.map(lambda x: 'Low' if x < 0.3 else ( 'High' if x >= .7 else 'Middle'))\n",
      "#todo['mlo_label'] = todo['mlo_loc'].map(tablas['translate'])\n",
      "274/118: dis = todo.copy()\n",
      "274/119: todo = dis[dis.box.isin(['box1','Swiss-Prot'])]\n",
      "274/120:\n",
      "pivot = todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('figuras/disorder_3_box1.svg')\n",
      "274/121: dis\n",
      "274/122: todo = tablas['human_dis'].drop(columns='box').merge(tablas['human_boxes'])\n",
      "274/123: control['mlo_label'] = 'Swiss-Prot'\n",
      "274/124: todo = pd.concat([todo,control])\n",
      "274/125: todo\n",
      "274/126: todo.box.value_counts()\n",
      "274/127: dis = todo.copy()\n",
      "274/128: to_plot = dis[dis.box.isin(['box2','Swiss-Prot'])]\n",
      "274/129:\n",
      "pivot = to_plot[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('figuras/disorder_3_box2.svg')\n",
      "274/130:\n",
      "# Una barra por MLO, con las proporciones de proteinas High, Middle o Low disorder\n",
      "todo['estructura'] =  todo.dc.map(lambda x: 'Low' if x < 0.3 else ( 'High' if x >= .7 else 'Middle'))\n",
      "#todo['mlo_label'] = todo['mlo_loc'].map(tablas['translate'])\n",
      "274/131: dis = todo.copy()\n",
      "274/132: to_plot = dis[dis.box.isin(['box2','Swiss-Prot'])]\n",
      "274/133:\n",
      "pivot = to_plot[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('figuras/disorder_3_box2.svg')\n",
      "274/134: pivot\n",
      "274/135: dc = tablas['human_dis'].drop(columns='box').merge(tablas['human_boxes'])\n",
      "274/136: control['mlo_label'] = 'Swiss-Prot'\n",
      "274/137: todo = pd.concat([dc,control])\n",
      "274/138:\n",
      "# Una barra por MLO, con las proporciones de proteinas High, Middle o Low disorder\n",
      "todo['estructura'] =  todo.dc.map(lambda x: 'Low' if x < 0.3 else ( 'High' if x >= .7 else 'Middle'))\n",
      "#todo['mlo_label'] = todo['mlo_loc'].map(tablas['translate'])\n",
      "274/139: dis = todo.copy()\n",
      "274/140: dis\n",
      "274/141: dis.box.value_counts()\n",
      "274/142: to_plot = dis[dis.box.isin(['box2','Swiss-Prot'])]\n",
      "274/143:\n",
      "pivot = to_plot[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('figuras/disorder_3_box2.svg')\n",
      "274/144: pivot\n",
      "274/145: to_plot = dis[dis.box.isin(['box2','Swiss-Prot'])]\n",
      "274/146: to_plot\n",
      "274/147:\n",
      "pivot = to_plot[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "pivot\n",
      "274/148: to_plot\n",
      "274/149: dc = tablas['human_dis'].drop(columns='box').merge(tablas['human_boxes'])\n",
      "274/150: control['mlo_label'] = 'Swiss-Prot'\n",
      "274/151: todo = pd.concat([dc,control])\n",
      "274/152: dis = todo.copy()\n",
      "274/153:\n",
      "# Una barra por MLO, con las proporciones de proteinas High, Middle o Low disorder\n",
      "todo['estructura'] =  todo.dc.map(lambda x: 'Low' if x < 0.3 else ( 'High' if x >= .7 else 'Middle'))\n",
      "todo['mlo_label'] = todo['mlo_loc'].map(tablas['translate'])\n",
      "274/154: dis = todo.copy()\n",
      "274/155: to_plot = dis[dis.box.isin(['box2','Swiss-Prot'])][['uniprot','dc','estructura','mlo_label']]\n",
      "274/156: to_plot\n",
      "274/157:\n",
      "mlo_labels = tablas['translate']\n",
      "mlo_labels['control'] = 'Swiss-Prot'\n",
      "274/158: todo = pd.concat([dc,control])\n",
      "274/159: dis = todo.copy()\n",
      "274/160:\n",
      "# Una barra por MLO, con las proporciones de proteinas High, Middle o Low disorder\n",
      "todo['estructura'] =  todo.dc.map(lambda x: 'Low' if x < 0.3 else ( 'High' if x >= .7 else 'Middle'))\n",
      "todo['mlo_label'] = todo['mlo_loc'].map(tablas['translate'])\n",
      "274/161: dis = todo.copy()\n",
      "274/162: to_plot = dis[dis.box.isin(['box2','Swiss-Prot'])][['uniprot','dc','estructura','mlo_label']]\n",
      "274/163: to_plot\n",
      "274/164:\n",
      "pivot = to_plot[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "pivot\n",
      "274/165:\n",
      "pivot = to_plot[['uniprot','estructura','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "pivot\n",
      "274/166: to_plot = dis[dis.box.isin(['box1','Swiss-Prot'])][['uniprot','dc','estructura','mlo_label']]\n",
      "274/167: to_plot\n",
      "274/168:\n",
      "pivot = to_plot[['uniprot','estructura','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "pivot\n",
      "274/169:\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('figuras/disorder_3_box2.svg')\n",
      "274/170:\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3_box2.svg')\n",
      "274/171:\n",
      "pivot = to_plot[['uniprot','estructura','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "pivot\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3_box2.svg')\n",
      "274/172:\n",
      "pivot = to_plot[['uniprot','estructura','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "pivot\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('~/svgs/disorder_3_box1.svg')\n",
      "274/173:\n",
      "pivot = to_plot[['uniprot','estructura','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "pivot\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "plt.savefig('disorder_3_box1.svg')\n",
      "274/174: !ls\n",
      "274/175: pivot\n",
      "274/176: pivot.index\n",
      "274/177: orden = pivot.index\n",
      "274/178: orden = pivot.index.tolist()\n",
      "274/179: to_plot = dis[dis.box.isin(['box1','Swiss-Prot'])][['uniprot','dc','estructura','mlo_label']]\n",
      "274/180: to_plot = dis[dis.box.isin(['box2','Swiss-Prot'])][['uniprot','dc','estructura','mlo_label']]\n",
      "274/181:\n",
      "pivot = todo[['uniprot','estructura','box','mlo_label']].dropna().pivot_table(index='mlo_label',columns='estructura', aggfunc='size')\n",
      "\n",
      "pivot['All'] = pivot.sum(1)\n",
      "\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot['All']\n",
      "\n",
      "#pivot = pivot.drop('All').drop(columns='All')\n",
      "pivot = pivot[['Low','Middle','High']]\n",
      "\n",
      "# Ploting\n",
      "fig,ax = plt.subplots(figsize=(2,4))\n",
      "pivot.drop('Other').sort_values('Low', ascending=False).plot.barh(stacked=True,color=['steelblue','salmon','red'],width=.8, ax=ax)\n",
      "ax.legend(bbox_to_anchor=(1, 0.5))\n",
      "ax.spines['top'].set_visible(False)\n",
      "ax.spines['right'].set_visible(False)\n",
      "ax.spines['bottom'].set_visible(False)\n",
      "ax.spines['left'].set_visible(False)\n",
      "ax.set_yticklabels(ax.get_yticklabels(),size=12);\n",
      "\n",
      "plt.tight_layout()\n",
      "#plt.savefig('figuras/disorder_3_box2.svg')\n",
      "274/182: pivot\n",
      "278/1: import pandas as pd\n",
      "278/2: df = pd.read_csv('~/Downloads/corte63.csv', sep=';')\n",
      "278/3: df\n",
      "278/4: df['P6']\n",
      "278/5: df['P6'].vaue_counts()\n",
      "278/6: df['P6'].value_counts()\n",
      "278/7: df['P6'].value_counts(dropna=False)\n",
      "278/8: df['P6'].value_counts() / df['P6'].sum()\n",
      "285/1: import pandas as pd\n",
      "285/2: df = pd.read_csv('/home/fernando/Downloads/corte63.csv')\n",
      "285/3: df\n",
      "285/4: df = pd.read_csv('/home/fernando/Downloads/corte63.csv', sep=';')\n",
      "285/5: df\n",
      "285/6: df.columns\n",
      "285/7: df.iloc[:,-10:]\n",
      "285/8: df.REGION2.value_counts()\n",
      "285/9: df['P6'].value_counts()\n",
      "285/10: df['P6'].value_counts() / df['P6'].value_counts().sum()\n",
      "291/1:\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "import pickle\n",
      "\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "291/2: human_db = tablas['db'].query('org == \"Homo sapiens\"')\n",
      "291/3: dis = tablas['human_dis']\n",
      "291/4: dis\n",
      "291/5: dis['prot'] = dis.uniprot.map('gene_names_dict')\n",
      "291/6: dis\n",
      "291/7: dis = dis[['uniprot','box', 'dc']].drop_duplicates()\n",
      "291/8: dis\n",
      "291/9:\n",
      "for box in dis.box.unique():\n",
      "    sns.ditsplot(dis.query('box == @box').dc)\n",
      "291/10:\n",
      "for box in dis.box.unique():\n",
      "    sns.distplot(dis.query('box == @box').dc)\n",
      "291/11:\n",
      "for box in dis.box.unique():\n",
      "    sns.distplot(dis.query('box == @box').dc, hist=False)\n",
      "291/12:\n",
      "for box in dis.box.unique():\n",
      "    sns.distplot(dis.query('box == @box').dc, hist=False, label=box)\n",
      "291/13: dis = tablas['human_dis']\n",
      "291/14: dis\n",
      "291/15: dis.drop(columns=['box']).merge(tabas['human_boxes'])\n",
      "291/16: dis.drop(columns=['box']).merge(tablas['human_boxes'])\n",
      "291/17: dis = dis.drop(columns=['box']).merge(tablas['human_boxes'])\n",
      "291/18: dis = dis[['uniprot','box', 'dc']].drop_duplicates()\n",
      "291/19:\n",
      "for box in dis.box.unique():\n",
      "    sns.distplot(dis.query('box == @box').dc, hist=False, label=box)\n",
      "291/20:\n",
      "control = pd.read_csv('/home/fernando/git/mlo/todo/tablas/control_disorder.csv')\n",
      "control = control[['uniprot', 'mlo', 'dc']].rename(columns={'mlo':'mlo_loc'})\n",
      "291/21: control\n",
      "291/22:\n",
      "control = pd.read_csv('/home/fernando/git/mlo/todo/tablas/control_disorder.csv')\n",
      "control = control[['uniprot', 'mlo', 'dc']].rename(columns={'mlo':'box'})\n",
      "291/23: control\n",
      "291/24: dis\n",
      "291/25: dis\n",
      "291/26:\n",
      "control_excluded = control[control.uniprot.isin(dis.uniprot)]\n",
      "control_excluded['box'] = 'control-excluded'\n",
      "291/27: pd.concat(dis, control, control_excluded)\n",
      "291/28: pd.concat([dis, control, control_excluded])\n",
      "291/29: disorder = pd.concat([dis, control, control_excluded])\n",
      "291/30:\n",
      "for box in disorder.box.unique():\n",
      "    sns.distplot(disorder.query('box == @box').dc, hist=False, label=box)\n",
      "291/31: nuevas_tablas = {}\n",
      "291/32: nuevas_tablas['disorder'] = disorder\n",
      "291/33: nuevas_tablas['disorder_boxes'] = disorder\n",
      "291/34: dis = tablas['human_dis']\n",
      "291/35: dis = dis.drop(columns=['box']).merge(tablas['human_boxes'])\n",
      "291/36: ## BY BOX\n",
      "291/37: dis\n",
      "291/38: disorder_mlos = disorder[['uniprot', 'mlo_loc','dc']].drop_duplicates()\n",
      "291/39: disorder_mlos = dis[['uniprot', 'mlo_loc','dc']].drop_duplicates()\n",
      "291/40: disorder_mlos\n",
      "291/41:\n",
      "for mlo in disorder.mlo_loc.unique():\n",
      "    sns.distplot(disorder.query('box == @mlo').dc, hist=False, label=mlo)\n",
      "291/42:\n",
      "for mlo in disorder_mlos.mlo_loc.unique():\n",
      "    sns.distplot(disorder_mlos.query('box == @mlo').dc, hist=False, label=mlo)\n",
      "291/43:\n",
      "for mlo in disorder_mlos.mlo_loc.unique():\n",
      "    sns.distplot(disorder_mlos.query('mlo_loc == @mlo').dc, hist=False, label=mlo)\n",
      "291/44:\n",
      "_ = plt.figure(figsize=(12,6))\n",
      "for mlo in disorder_mlos.mlo_loc.unique():\n",
      "    sns.distplot(disorder_mlos.query('mlo_loc == @mlo').dc, hist=False, label=mlo)\n",
      "291/45:\n",
      "_ = plt.figure(figsize=(9,4))\n",
      "for mlo in disorder_mlos.mlo_loc.unique():\n",
      "    sns.distplot(disorder_mlos.query('mlo_loc == @mlo').dc, hist=False, label=mlo)\n",
      "291/46:\n",
      "_ = plt.figure(figsize=(10,4))\n",
      "for mlo in disorder_mlos.mlo_loc.unique():\n",
      "    sns.distplot(disorder_mlos.query('mlo_loc == @mlo').dc, hist=False, label=mlo)\n",
      "291/47: sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc')\n",
      "291/48:\n",
      "_ = plt.figure(figsize=(12,6))\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc')\n",
      "291/49:\n",
      "_ = plt.figure(figsize=(14,6))\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc')\n",
      "291/50:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc')\n",
      "291/51:\n",
      "disorder_mlos = dis[['uniprot', 'mlo_loc','dc']].drop_duplicates()\n",
      "disorder_mlos = pd.concat([disorder_mlos, control.rename(columns = {'box':'mlo_loc'}), control.rename(columns = {'box':'mlo_loc'})])\n",
      "291/52:\n",
      "_ = plt.figure(figsize=(10,4))\n",
      "for mlo in disorder_mlos.mlo_loc.unique():\n",
      "    sns.distplot(disorder_mlos.query('mlo_loc == @mlo').dc, hist=False, label=mlo)\n",
      "291/53:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc')\n",
      "291/54:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc', notch=True)\n",
      "291/55:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc', notch=True, title='All BOXES')\n",
      "291/56:\n",
      "_ = plt.figure(figsize=(20,6), title='Todos los boxes juntos')\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc', notch=True)\n",
      "291/57:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc', notch=True)\n",
      "plt.title('Todos los boxes juntos')\n",
      "291/58:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc', notch=True)\n",
      "plt.title('Todos los boxes juntos');\n",
      "291/59:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_mlos[disorder_mlos.uniprot.isin(dis.query('box == \"box1\"'))], x='mlo_loc', y='dc', notch=True)\n",
      "plt.title('Todos los boxes juntos');\n",
      "291/60:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_mlos[disorder_mlos.uniprot.isin(dis.query('box == \"box1\"').uniprot)], x='mlo_loc', y='dc', notch=True)\n",
      "plt.title('Box 1');\n",
      "291/61: dis\n",
      "291/62: dis\n",
      "291/63: dis[['uniprot','mlo_loc','dc']].drop_duplicates()\n",
      "291/64: dis[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts()\n",
      "291/65: dis[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/66: nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/67: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(mlos))\n",
      "291/68: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/69: disorder_nmlos\n",
      "291/70: control_excluded\n",
      "291/71:\n",
      "control_excluded = control[~control.uniprot.isin(dis.uniprot)]\n",
      "control_excluded['box'] = 'control-excluded'\n",
      "291/72: disorder = pd.concat([dis, control, control_excluded])\n",
      "291/73:\n",
      "for box in disorder.box.unique():\n",
      "    sns.distplot(disorder.query('box == @box').dc, hist=False, label=box)\n",
      "291/74:\n",
      "disorder_mlos = dis[['uniprot', 'mlo_loc','dc']].drop_duplicates()\n",
      "disorder_mlos = pd.concat([disorder_mlos, control.rename(columns = {'box':'mlo_loc'}), control_excluded.rename(columns = {'box':'mlo_loc'})])\n",
      "291/75:\n",
      "_ = plt.figure(figsize=(10,4))\n",
      "for mlo in disorder_mlos.mlo_loc.unique():\n",
      "    sns.distplot(disorder_mlos.query('mlo_loc == @mlo').dc, hist=False, label=mlo)\n",
      "291/76: nuevas_tablas['disorder_boxes'] = disorder\n",
      "291/77:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_mlos, x='mlo_loc', y='dc', notch=True)\n",
      "plt.title('Todos los boxes juntos');\n",
      "291/78: nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/79: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/80: control_excluded\n",
      "291/81: disorder_nmlos\n",
      "291/82: pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box','mlo_loc'})])\n",
      "291/83: pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})])\n",
      "291/84: pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})]).fillna(0)\n",
      "291/85: disorder_mlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})]).fillna(0)\n",
      "291/86: disorder_mlos\n",
      "291/87: disorder_mlos.drop_duplicates(\"uniprot\")\n",
      "291/88: disorder_mlos = disorder_mlos.drop_duplicates(\"uniprot\")\n",
      "291/89:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/90: nmlos = dis.query('box == \"box1\"')[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/91: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/92: disorder_mlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})]).fillna(0)\n",
      "291/93: disorder_mlos = disorder_mlos.drop_duplicates(\"uniprot\")\n",
      "291/94:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/95: disorder_mlos = disorder_mlos.drop_duplicates([\"uniprot\",'nmlos'])\n",
      "291/96:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/97: nmlos = dis.query('box == \"box1\"')[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/98: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/99: disorder_mlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})]).fillna(0)\n",
      "291/100: disorder_mlos = disorder_mlos.drop_duplicates([\"uniprot\",'nmlos'])\n",
      "291/101:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/102: disorder_mlos\n",
      "291/103: disorder_mlos.nmlos.astype(int).astype(str)\n",
      "291/104: disorder_mlos.nmlos = disorder_mlos.nmlos.astype(int).astype(str)\n",
      "291/105:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/106: disorder_nmlos\n",
      "291/107: disorder_mlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})]).fillna(0)\n",
      "291/108: disorder_nmlos\n",
      "291/109: disorder_mlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})]).fillna(0)\n",
      "291/110: disorder_nmlos\n",
      "291/111: disorder_mlos.nmlos = disorder_mlos.fillna(0).nmlos.astype(int).astype(str)\n",
      "291/112: nmlos = dis.query('box == \"box1\"')[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/113: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/114: disorder_mlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})])\n",
      "291/115: disorder_mlos = disorder_mlos.drop_duplicates([\"uniprot\",'nmlos'])\n",
      "291/116: disorder_mlos.nmlos = disorder_mlos.fillna(0).nmlos.astype(int).astype(str)\n",
      "291/117:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/118: disorder_nmlos\n",
      "291/119: nmlos = dis.query('box == \"box1\"')[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/120: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/121: disorder_nmlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})])\n",
      "291/122: disorder_nmlos = disorder_nmlos.drop_duplicates([\"uniprot\",'nmlos'])\n",
      "291/123: disorder_nmlos.nmlos = disorder_nmlos.fillna(0).nmlos.astype(int).astype(str)\n",
      "291/124:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/125: disorder_nmlos\n",
      "291/126: nmlos = dis.query('box == \"box1\"')[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/127: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/128: disorder_nmlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})])\n",
      "291/129: disorder_nmlos = disorder_nmlos.drop_duplicates([\"uniprot\",'nmlos'])\n",
      "291/130: disorder_nmlos\n",
      "291/131: disorder_nmlos.fillna(0).nmlos.map(lambda x: x if x < 3 else 3)\n",
      "291/132: disorder_nmlos = disorder_nmlos.fillna(0).nmlos.map(lambda x: x if x < 3 else 3)\n",
      "291/133: disorder_nmlos.nmlos = disorder_nmlos.fillna(0).nmlos.astype(int).astype(str)\n",
      "291/134:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/135: disorder_nmlos.nmlos = disorder_nmlos.fillna(0).nmlos.map(lambda x: x if x < 3 else 3)\n",
      "291/136: nmlos = dis.query('box == \"box1\"')[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/137: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/138: disorder_nmlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})])\n",
      "291/139: disorder_nmlos = disorder_nmlos.drop_duplicates([\"uniprot\",'nmlos'])\n",
      "291/140: disorder_nmlos.nmlos = disorder_nmlos.fillna(0).nmlos.map(lambda x: x if x < 3 else 3)\n",
      "291/141: disorder_nmlos.nmlos = disorder_nmlos.fillna(0).nmlos.astype(int).astype(str)\n",
      "291/142:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/143: nmlos = dis.query('box == \"box1\"')[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/144: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/145: disorder_nmlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})])\n",
      "291/146: disorder_nmlos = disorder_nmlos.drop_duplicates([\"uniprot\",'nmlos'])\n",
      "291/147:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/148: disorder_nmlos.head()\n",
      "291/149: disorder_nmlos[[\"uniprot\",'nmlos','dc']].head()\n",
      "291/150: nmlos = dis.query('box == \"box1\"')[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/151: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/152: disorder_nmlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})])\n",
      "291/153: disorder_nmlos = disorder_nmlos.drop_duplicates([\"uniprot\",'nmlos'])\n",
      "291/154:\n",
      "# TABLA\n",
      "disorder_nmlos[[\"uniprot\",'nmlos','dc']].head()\n",
      "291/155:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "291/156: disorder_nmlos.nmlos = disorder_nmlos.fillna(0).nmlos.map(lambda x: x if x < 3 else 3)\n",
      "291/157: disorder_nmlos.nmlos = disorder_nmlos.fillna(0).nmlos.astype(int).astype(str)\n",
      "291/158:\n",
      "# TABLA\n",
      "disorder_nmlos[[\"uniprot\",'nmlos','dc']].head()\n",
      "291/159:\n",
      "_ = plt.figure(figsize=(20,6))\n",
      "sns.boxplot(data=disorder_nmlos, x='nmlos', y='dc', notch=True)\n",
      "plt.title('nmlos');\n",
      "295/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import ast\n",
      "#import altair as alt\n",
      "import numpy as np\n",
      "295/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print('tablas:',\"\\n\\t\".join([i for i in tablas.keys()]))\n",
      "295/3:\n",
      "lc = tablas['human_lc_box1'].query('method == \"SEG_intermediate\"').copy()\n",
      "\n",
      "db = tablas['human_box1'][['uniprot','mlo_loc']].drop_duplicates()\n",
      "295/4: lc_db = lc.merge(tablas['human_box1'][['uniprot','mlo_loc']], how='left').drop_duplicates()\n",
      "295/5: plaac = tablas['human_dominios_box1'].query('source == \"plaac\"')\n",
      "295/6:\n",
      "# PLAAC\n",
      "plaac['gene_name'] = plaac.uniprot.map(tablas['gene_names_dict'])\n",
      "\n",
      "print('cantidad de proteinas con PLD:',len(plaac.uniprot.unique()))\n",
      "295/7: plaac_control = pd.read_csv(\"/home/fernando/datos/plaac_swiss_prot_human.tsv\",sep='\\t')\n",
      "295/8: plaac_control[\"uniprot\"] = plaac_control.SEQid.str.split(\"|\").str[1]\n",
      "295/9: plaac_control = plaac_control[~plaac_control.COREscore.isna()]\n",
      "295/10:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      "295/11:\n",
      "print('Proteinas plaac que no tienen LC segun Seg')\n",
      "print(\", \".join([u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]))\n",
      "print(\", \".join([tablas['gene_names_dict'][uniprot] for uniprot in [u for u in plaac.uniprot.unique() if u not in lc.uniprot.tolist()]]))\n",
      "295/12:\n",
      "sp = pd.read_csv(\"/home/fernando/datos/human_swiss_prot.tab\",sep='\\t')\n",
      "sp.columns = ['uniprot','seq','largo']\n",
      "295/13:\n",
      "boxes = tablas['human_boxes'][['uniprot','box']].drop_duplicates()\n",
      "boxes.box.value_counts()\n",
      "295/14: control = sp[['uniprot','seq']]\n",
      "295/15: base = control.merge(boxes,how='left').fillna(\"control_excluded\")\n",
      "295/16: base.box.value_counts()\n",
      "295/17: base.box.value_counts()\n",
      "295/18: control['box'] = 'control'\n",
      "295/19: base = pd.concat([base,control])\n",
      "295/20:\n",
      "from Bio import SeqIO\n",
      "lcs = SeqIO.parse(\"/home/fernando/seg/swissprot_seg.fasta\",'fasta')\n",
      "lista = [[i.id.split('|')[1],str(i.seq).upper()] for i in lcs]\n",
      "lc_control = pd.DataFrame(lista,columns=['uniprot','seq'])\n",
      "295/21: base['clase'] = 'no_lc'\n",
      "295/22:\n",
      "import re\n",
      "base.loc[base.uniprot.isin(lc_control.uniprot),'clase'] = 'lc'\n",
      "base.loc[base.seq.map(lambda x: re.findall(r'RGG[A-Z]{0,4}RGG[A-Z]{0,4}RGG[A-Z]{0,4}',x)).astype(bool),'clase'] = 'featured'\n",
      "base.loc[base.uniprot.isin(plaac_control.uniprot), 'clase'] = 'featured'\n",
      "295/23: base.clase.value_counts()\n",
      "295/24: base.clase.value_counts()\n",
      "295/25:\n",
      "pivot = base.pivot_table(index='box',columns='clase',aggfunc='size')\n",
      "pivot['suma'] = pivot.sum(1)\n",
      "for col in pivot.columns:\n",
      "    pivot[col] = pivot[col] / pivot.suma\n",
      "pivot = pivot.fillna(0)\n",
      "295/26:\n",
      "ax = pivot.sort_values('suma')[[\"featured\",'lc','no_lc']].reindex(['control_excluded','control','box3','box2','box1']).drop('control_excluded').plot.barh(figsize=(6,2),\n",
      "                                                                    stacked=True, width=.8,#edgecolor='white',\n",
      "                                                                   color=['seagreen','darkseagreen','lightgrey'], fontsize=14,);                                                                                                                                 \n",
      "                                                                   \n",
      "ax.set_yticklabels(['Control','Box 3','Box 2','Box 1'],fontsize=14);\n",
      "plt.legend(['Featured LC', 'LC', 'No LC'], fontsize=12, bbox_to_anchor=(1.05, 1),ncol=3, loc='upper left');\n",
      "#plt.savefig('figuras/5_lc.svg')\n",
      "295/27: pivot\n",
      "295/28: pivot.to_csv('lc_boxes.csv')\n",
      "291/160: pd.read_cvs('boxes/lc_boxes.csv')\n",
      "291/161: !pwd\n",
      "291/162: !pwd boxes\n",
      "291/163: !lsboxes\n",
      "291/164: !ls boxes\n",
      "291/165: pd.read_cvsv('boxes/lc_boxes.csv')\n",
      "291/166: lc = pd.read_csv('boxes/lc_boxes.csv')\n",
      "291/167: lc\n",
      "291/168: lc.drop(columns='suma')\n",
      "291/169: lc = lc.drop(columns='suma')\n",
      "291/170: lc\n",
      "291/171: nmlos = dis.query('box == \"box1\"')[['uniprot','mlo_loc','dc']].drop_duplicates().uniprot.value_counts().to_dict()\n",
      "291/172: disorder_nmlos = dis[['uniprot','mlo_loc','dc']].drop_duplicates().assign(nmlos = lambda df: df.uniprot.map(nmlos))\n",
      "291/173: disorder_nmlos = pd.concat([disorder_nmlos, control_excluded.rename(columns = {'box':'mlo_loc'})])\n",
      "291/174: disorder_nmlos = disorder_nmlos.drop_duplicates([\"uniprot\",'nmlos'])\n",
      "291/175:\n",
      "# TABLA\n",
      "disorder_nmlos[[\"uniprot\",'nmlos','dc']].head()\n",
      "291/176: nuevas_tablas['disorder_nmlos'] = disorder_nmlos\n",
      "291/177: nuevas_tablas['disorder_mlos'] = disorder_mlos\n",
      "291/178: nuevas_tablas['lc_boxes'] = lc\n",
      "291/179: !pwd\n",
      "291/180:\n",
      "with open('tablas_pickle_colab.pkl', 'bw') as f:\n",
      "    pickle.dump(nuevas_tablas)\n",
      "291/181:\n",
      "with open('tablas_pickle_colab.pkl', 'bw') as f:\n",
      "    pickle.dump(f,nuevas_tablas)\n",
      "291/182:\n",
      "with open('tablas_pickle_colab.pkl', 'bw') as f:\n",
      "    pickle.dump(nuevas_tablas,f)\n",
      "291/183: tablas['disorder_box']\n",
      "291/184: tablas['disorder_boxes']\n",
      "291/185: nuevas_tablas['disorder_boxes']\n",
      "291/186: nuevas_tablas['disorder_boxes'][['uniprot','dc','box']]\n",
      "291/187: nuevas_tablas['disorder_boxes'][['uniprot','dc','box']].drop_duplicates()\n",
      "291/188: nuevas_tablas['disorder_boxes'][['uniprot','dc','box']].drop_duplicates().to_csv('disorder_by_box.csv',index=False)\n",
      "291/189: tablas['human_dominios']\n",
      "291/190: tablas['human_dominios'].to_csv('~/tablas_dominios.csv', index=False)\n",
      "291/191: tablas\n",
      "291/192: tablas[\"human_dominios\"]\n",
      "300/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "300/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "302/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "import matplotlib as mpl\n",
      "302/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "302/3:\n",
      "sp = pd.read_csv('/home/fernando/git/mlo/todo/tablas/swiss_prot.tab', sep='\\t')\n",
      "\n",
      "dfs = []\n",
      "for df in pd.read_csv('/home/fernando/Descargas/Pfam-A.regions.tsv.gz',chunksize=500000, compression='gzip',sep='\\t'):\n",
      "\n",
      "    df = df[df.pfamseq_acc.isin(sp.Entry)]\n",
      "    dfs.append(df)   \n",
      "\n",
      "dominios_sp = pd.concat(dfs)\n",
      "308/1:\n",
      "## Algunos delos tipos básicos de datos en Python son:\n",
      "* **int** - para representar números enteros <br/>\n",
      "```python\n",
      "entero = 40``` \n",
      "* **long** - para representar números enteros muy largos. _(Solo en Python 2.x)_<br/>\n",
      "```python \n",
      "ent_long =  8947842156455143213546```\n",
      "* **float** - para representar números decimales <br/>\n",
      "```python \n",
      "flotante = 3.14\n",
      "```\n",
      "* **bool** - para representar valores de verdad <br/>\n",
      "```python\n",
      "booleano = True\n",
      "```\n",
      "* **complex** - para representar números complejos<br/>\n",
      "```python\n",
      "complejo = 1 + 2j```\n",
      "315/1: 4 + 12\n",
      "320/1: ## Listas\n",
      "320/2: organismos = ['Homo sapiens', 'Mus musculus', 'Drosophila melanogaster', 'Arabidopsis thaliana', \"Xenopus laevis\"]\n",
      "320/3: organismos = ['Homo sapiens', 'Mus musculus', 'Drosophila melanogaster', 'Arabidopsis thaliana', \"Xenopus laevis\"]\n",
      "320/4:\n",
      "print(organismos)\n",
      "print(type(organismos))\n",
      "320/5: organismos[3]\n",
      "320/6: tambien se pueden definir mediante la función list()\n",
      "320/7: list('Homo sapiens', 'Mus musculus', 'Drosophila melanogaster', 'Arabidopsis thaliana', \"Xenopus laevis\")\n",
      "320/8:\n",
      "from IPython.display import IFrame\n",
      "IFrame(\"https://storage.googleapis.com/plos-corpus-prod/10.1371/journal.pcbi.1004867/1/pcbi.1004867.pdf?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=wombat-sa%40plos-prod.iam.gserviceaccount.com%2F20210220%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210220T035931Z&X-Goog-Expires=3600&X-Goog-SignedHeaders=host&X-Goog-Signature=74f9e53c47f238beeb6026380f9779c5840cdacb38c1ea66ea025a76156bc2f343fb007dc42db5869d128852e729673a9fbe68cdaf6034baa4bce20a4e539b9f2cde2cc43062c867d2d26f05bd8313e8bff5b648df96687fe278c91a46358408ac3d938f06311666b6f683cf2e61b0508ebeff581973758ff04ea82562636749f9961cb0b3a8d76c8f8642cdd2818a99897eb64b8389676e28f15353e1e810b9a6ae38c12251ed55e7af31cae9ae0ee28090253d7458973e597b0658118d105a5af05de44d7b85eedfd210985f3d454a9b519875253f157568bc6c95f64fa287ab450efbfbd99a07a1736d6e5c8fc8cff5176fec9f3390aa81b91c3efd57afd8\", width=600, height=300)\n",
      "320/9: print('El tercer organismo de la lista es:', organismos[3])\n",
      "320/10: print('Tenemos', len(organismos), 'en la lista.')\n",
      "320/11:\n",
      "# recordemos que podemos asignar el largo de la lista a una variable para luego trabajar con ella\n",
      "cantidad_organismos = len(organismos)\n",
      "print('Tenemos', len(organismos), 'en la lista.')\n",
      "320/12: print(\"Antes de agregar elementos\", organismos)\n",
      "320/13: print(\"Antes de agregar elementos:\", organismos)\n",
      "320/14:\n",
      "print(\"Antes de agregar elementos:\", organismos)\n",
      "organismos.append('Schizosaccharomyces pombe')\n",
      "print(organismos)\n",
      "organismos.insert(3, 'Gallus gallus')\n",
      "print(orgnismos)\n",
      "320/15:\n",
      "print(\"Antes de agregar elementos:\", organismos)\n",
      "organismos.append('Schizosaccharomyces pombe')\n",
      "print(organismos)\n",
      "organismos.insert(3, 'Gallus gallus')\n",
      "print(organismos)\n",
      "320/16:\n",
      "print(\"Antes de agregar elementos:\", organismos)\n",
      "organismos.append('Schizosaccharomyces pombe')\n",
      "print(organismos)\n",
      "organismos.insert(3, 'Gallus gallus')\n",
      "print(organismos)\n",
      "320/17:\n",
      "print(\"Antes de agregar elementos:\", organismos)\n",
      "organismos.append('Schizosaccharomyces pombe')\n",
      "print(organismos)\n",
      "organismos.insert(3, 'Gallus gallus')\n",
      "print(organismos)\n",
      "320/18: organismos = ['Homo sapiens', 'Mus musculus', 'Drosophila melanogaster', 'Arabidopsis thaliana', \"Xenopus laevis\"]\n",
      "320/19:\n",
      "print(\"Antes de agregar elementos:\", organismos)\n",
      "organismos.append('Schizosaccharomyces pombe')\n",
      "print(organismos)\n",
      "organismos.insert(3, 'Gallus gallus')\n",
      "print(organismos)\n",
      "320/20: organismos = ['Homo sapiens', 'Mus musculus', 'Drosophila melanogaster', 'Arabidopsis thaliana', \"Xenopus laevis\"]\n",
      "320/21:\n",
      "print(organismos)\n",
      "organismos.append('Schizosaccharomyces pombe')\n",
      "print(organismos)\n",
      "organismos.insert(3, 'Gallus gallus')\n",
      "print(organismos)\n",
      "320/22: ['1','2','3','2'].remove('2')\n",
      "320/23:\n",
      "a = ['1','2','3','2']\n",
      "a.remove('2')\n",
      "320/24: a\n",
      "320/25: [1,2,3].pop(-1)\n",
      "320/26:\n",
      "print(organismos)\n",
      "organismos.remove(\"Xenopus laevis\")\n",
      "print(organismos)\n",
      "organismos.pop(3)\n",
      "print(organismos)\n",
      "organismos.pop(-1)\n",
      "print(organismos)\n",
      "320/27: print(sorted(organismos))\n",
      "320/28: organismos = ['Homo sapiens', 'Mus musculus', 'Drosophila melanogaster', 'Arabidopsis thaliana', \"Xenopus laevis\"]\n",
      "320/29: print(sorted(organismos))\n",
      "320/30: print(organismos.sorted())\n",
      "320/31: print(organismos.sort())\n",
      "320/32: # Qué paso?\n",
      "320/33: Utilizando los índices y la asignación podemos modificar los elementos de la lista\n",
      "320/34:\n",
      "organimos2 = organismos.copy() # copiamos la lista para no modificar la original\n",
      "print(organismos2)\n",
      "320/35:\n",
      "organismos2 = organismos.copy() # copiamos la lista para no modificar la original\n",
      "print(organismos2)\n",
      "320/36:\n",
      "organimos2[0] = organimos2[-1]\n",
      "print(organimos2)\n",
      "320/37:\n",
      "from IPython.display import IFrame\n",
      "display(IFrame('https://htmlpreview.github.io/?https://raw.githubusercontent.com/Ferorti/escuela2019/master/pres.html', width='100%', height=500))\n",
      "321/1: pi = 3.14\n",
      "321/2: !pwd\n",
      "321/3: ! head../images/svg/variables.svg\n",
      "321/4: int(3.2)\n",
      "321/5: int(3.9)\n",
      "321/6: int(\"3.14\")\n",
      "321/7: int(2.3)\n",
      "321/8:\n",
      "Partiendo de:\n",
      "```python\n",
      "a = 1.0\n",
      "b = '1'\n",
      "c = '1.1'\n",
      "```\n",
      "En cuales de las siguientes opciones obtendríamos ``2.0``?\n",
      "\n",
      "```python\n",
      "a + float(b)\n",
      "float(b) + float(c)\n",
      "a + int(c)\n",
      "a + int(float(c))\n",
      "int(a) + int(float(c))\n",
      "2.0 * b\n",
      "```\n",
      "321/9:\n",
      "Partiendo de:\n",
      "```python\n",
      "a = 1.0\n",
      "b = '1'\n",
      "c = '1.1'\n",
      "```\n",
      "En cuales de las siguientes opciones obtendríamos ``2.0``?\n",
      "\n",
      "```python\n",
      "a + float(b)\n",
      "float(b) + float(c)\n",
      "a + int(c)\n",
      "a + int(float(c))\n",
      "int(a) + int(float(c))\n",
      "2.0 * b\n",
      "```\n",
      "\n",
      "## Listas\n",
      "321/10:\n",
      "a = 1.0\n",
      "b = '1'\n",
      "c = '1.1'\n",
      "\n",
      "print(a + float(b))\n",
      "print(float(b) + float(c))\n",
      "print(a + int(c))\n",
      "print(a + int(float(c)))\n",
      "print(int(a) + int(float(c)))\n",
      "321/11:\n",
      "a = 1.0\n",
      "b = '1'\n",
      "c = '1.1'\n",
      "\n",
      "print(a + float(b))\n",
      "print(float(b) + float(c))\n",
      "\n",
      "print(a + int(float(c)))\n",
      "print(int(a) + int(float(c)))\n",
      "321/12:\n",
      "%%html\n",
      "<iframe id=\"inlineFrameExample\"\n",
      "    title=\"Inline Frame Example\"\n",
      "    width=\"300\"\n",
      "    height=\"200\"\n",
      "    src=\"https://www.openstreetmap.org/export/embed.html?bbox=-0.004017949104309083%2C51.47612752641776%2C0.00030577182769775396%2C51.478569861898606&layer=mapnik\">\n",
      "</iframe>\n",
      "321/13:\n",
      "%%html\n",
      "<iframe \n",
      "    title=\"Inline Frame Example\"\n",
      "    width=\"300\"\n",
      "    height=\"200\"\n",
      "    src=\"https://docs.python.org/3/library/functions.html\">\n",
      "</iframe>\n",
      "321/14:\n",
      "%%html\n",
      "<iframe \n",
      "    title=\"Inline Frame Example\"\n",
      "    width=\"600\"\n",
      "    height=\"800\"\n",
      "    src=\"https://docs.python.org/3/library/functions.html\">\n",
      "</iframe>\n",
      "321/15: ### FUNCIONES BUILT IN\n",
      "321/16: sum(2,3)\n",
      "321/17: sum([2,3])\n",
      "321/18: abs(2)\n",
      "321/19: abs(-2)\n",
      "322/1: Los ``strings`` son secuencias **ordenadas** de caracteres.\n",
      "322/2: string = \"Curso de Python\" # o 'Curso de Python'\n",
      "322/3: primer_caracter = string[0] # Recordar que los índices comienzan con 0\n",
      "322/4: string[0] # Recordar que los índices comienzan con 0\n",
      "322/5: primer_caracter = string[0]\n",
      "322/6:\n",
      "primer_caracter = string[0]\n",
      "print(primer_caracter)\n",
      "322/7: palabra = \"Curso de Python\" # o 'Curso de Python'\n",
      "322/8: texto = \"Curso de Python\" # o 'Curso de Python'\n",
      "322/9: palabra[0] # Recordar que los índices comienzan con 0\n",
      "322/10:\n",
      "primer_caracter = texto[0]\n",
      "print(primer_caracter)\n",
      "322/11: texto[-15]\n",
      "322/12: # Completar\n",
      "322/13:\n",
      "Además de acceder a los caracteres, podemos obtener subsecuencias de un string.\n",
      "+ Para esto se utiliza también la notación de corchetes [ ], pero necesitamos dos índices que indiquen desde donde hasta donde queremos acceder, separadas con dos puntos :\n",
      "``palabra = [start:stop]``  # desde el indice start incluido, hasta el indice stop no incluido\n",
      "322/14: palabra = texto[9:14]\n",
      "322/15: palabra\n",
      "322/16: palabra = texto[9:15]\n",
      "322/17: palabra\n",
      "322/18:\n",
      "print(texto[9])\n",
      "print(texto[14])\n",
      "322/19: print(texto[9:14])\n",
      "322/20: print(texto[9:15])\n",
      "322/21: print(texto[9:14])\n",
      "322/22: ![](../images/svg/string7.svg)\n",
      "322/23: ![](../images/svg/string7.svg)\n",
      "322/24: texto_largo = \"Quinta escuela de verano de bioinformatica de la A2B2C\"\n",
      "322/25: texto_largo = \"Quinta escuela de verano de bioinformatica de la A2B2C\"\n",
      "322/26: True\n",
      "322/27: not True\n",
      "322/28: True or False\n",
      "322/29: False or False\n",
      "322/30:\n",
      "print('True or True:', True or False)\n",
      "print('False or False:', False or False)\n",
      "print('True and False:', True and False)\n",
      "322/31:\n",
      "print('False or False:', False or False)\n",
      "print('True or True:', True or False)\n",
      "print('True and False:', True and False)\n",
      "322/32:\n",
      "print('False or True:', False or False)\n",
      "print('True or True:', True or False)\n",
      "print('True and False:', True and False)\n",
      "322/33:\n",
      "print('False or True:', False or True)\n",
      "print('True or True:', True or True)\n",
      "print('True and False:', True and False)\n",
      "322/34:\n",
      "print('False or True:', False or True)\n",
      "print('True or True:', True or True)\n",
      "print('True and False:', True and False)\n",
      "print('not True:', not True)\n",
      "322/35: True and False or True\n",
      "322/36: True and False or True and False\n",
      "322/37: True and False or True\n",
      "322/38: True and False or False\n",
      "322/39: 3 > 5\n",
      "322/40:\n",
      "suma = 3 > 5\n",
      "print(suma)\n",
      "print(type(suma))\n",
      "322/41:\n",
      "print('3 < 5: ', 3 < 5)\n",
      "print('100 == 100', 100 == 100)\n",
      "print('100 == 100 and 3 < 5', 100 == 100 and 3 < 5)\n",
      "print('100 == 100 or 3 < 5', 100 == 100 and 3 < 5)\n",
      "322/42:\n",
      "print('3 < 5: ', 3 < 5)\n",
      "print('100 == 100', 100 == 100)\n",
      "print('100 == 100 and 10 < 5', 100 == 100 and 3 < 5)\n",
      "print('100 == 100 or 3 < 5', 100 == 100 and 3 < 5)\n",
      "322/43:\n",
      "print('3 < 5: ', 3 < 5)\n",
      "print('100 == 100', 100 == 100)\n",
      "print('100 == 100 and 10 < 5', 100 == 100 and 10 < 5)\n",
      "print('100 == 100 or 3 < 5', 100 == 100 and 3 < 5)\n",
      "322/44: print(10 < 3 > 2)\n",
      "322/45: print(True and False or True)\n",
      "322/46: print(True and (False or True))\n",
      "322/47:\n",
      "x = 10\n",
      "x_mayor_a_0 = x > 0\n",
      "letra = 'Z'\n",
      "letra_es_z = letra == \"Z\"\n",
      "print(x_mayor_a_0)\n",
      "print(letra_es_z)\n",
      "322/48:\n",
      "x = 10\n",
      "x_mayor_a_0 = x > 0\n",
      "letra = 'F'\n",
      "letra_es_z = letra == \"Z\"\n",
      "print(x_mayor_a_0)\n",
      "print(letra_es_z)\n",
      "322/49:\n",
      "x = 4\n",
      "x_mayor_a_0 = x > 0\n",
      "letra = 'F'\n",
      "letra_es_z = letra == \"Z\"\n",
      "print(x_mayor_a_0)\n",
      "print(letra_es_z)\n",
      "322/50:\n",
      "x = 10\n",
      "x_mayor_a_0 = x > 0\n",
      "letra = 'Z'\n",
      "letra_es_z = letra == \"Z\"\n",
      "print(x_mayor_a_0)\n",
      "print(letra_es_z)\n",
      "322/51: x_mayor_a_0 and letra_es_z\n",
      "322/52:\n",
      "print('3 < 5: ', 3 < 5)\n",
      "print('100 == 100', 100 == 100)\n",
      "322/53:\n",
      "print('100 == 100 and 10 < 5', 100 == 100 and 10 < 5)\n",
      "print('100 == 100 or 3 < 5', 100 == 100 and 3 < 5)\n",
      "324/1: ---\n",
      "327/1:\n",
      "# Esta linea no sera ejecutada por el interprete de Python\n",
      "pi = 3.14 # Esta linea tampoco seroa tenida en cuenta\n",
      "327/2:\n",
      "# Esta linea no sera ejecutada por el interprete de Python\n",
      "pi = 3.14 # Esta linea tampoco seroa tenida en cuenta\n",
      "print(pi)\n",
      "327/3:\n",
      "# Defino cuanto vale pi\n",
      "pi = 3.14 # Pi es igual a 3.14\n",
      "print(pi) # Imprimo el valor\n",
      "327/4: Además de valores, variables y tipos de datos, ya vimos algunas cosas que todavía no explicamos.\n",
      "327/5:\n",
      "los valores que les pasamos a una función se donominan argumentos, pueden ser uno, varios o ninguno, dependiendo la función.\n",
      "Los argumentos se escriben entre paréntesis y si no hay arguementos se escriben los paréntisis solos:\n",
      "327/6: print(\"pi es\", 3.14)\n",
      "327/7:\n",
      "print(\"pi es\", 3.14) # Dos argumentos\n",
      "print()              # Ningun argumento\n",
      "int(3.13)            # Un argumento\n",
      "327/8: len(23)\n",
      "327/9:\n",
      "print(1, \"comienzo.\") # Dos argumentos\n",
      "print()              # Ningun argumento\n",
      "print(\"final\")            # Un argumento\n",
      "327/10: las funciones en python devuelven valores, esos valores generalmente se imprimen en la pantalla o o se guardan en un variables:\n",
      "327/11: len('ATGGCGT')\n",
      "327/12:\n",
      "len('ATGGCGT') # la funcion len devuelve 7, si no hacemos nada con eso, ese valor se pierde\n",
      "print(len('ATGGCGT'))\n",
      "largo = len('ATGGCGT')\n",
      "print(largo * 10)\n",
      "327/13:\n",
      "seq = 'ATGGCGT'\n",
      "\n",
      "len(seq) # la funcion len devuelve 7, si no hacemos nada con eso, ese valor se pierde\n",
      "print('largo de la secuencia', len(seq))\n",
      "\n",
      "largo = len('ATGGCGT')\n",
      "print(largo * 10)\n",
      "327/14:\n",
      "seq = 'ATGGCGT'\n",
      "\n",
      "len(seq) # la funcion len devuelve 7, si no hacemos nada con eso, ese valor se pierde\n",
      "print('largo de la secuencia', len(seq))\n",
      "\n",
      "largo = len('ATGGCGT')\n",
      "print('largo de la secuencia por 10: ', largo * 10)\n",
      "327/15: pint(seq)\n",
      "327/16: print(seq)\n",
      "327/17: largo = print(seq)\n",
      "327/18: largo\n",
      "327/19: type(largo)\n",
      "327/20:\n",
      "# Que pasa con esto?\n",
      "largo = print(seq)\n",
      "327/21: max(2,3)\n",
      "327/22: max(2,3,5)\n",
      "327/23: max(12,3,61, 2e20)\n",
      "327/24: max(12,3,61, 2.3e3)\n",
      "327/25: print(max(12,3,61, 2.3e3))\n",
      "327/26:\n",
      "print(max(12,3,61, 2.3e3))\n",
      "print(max(30, 'A'))\n",
      "327/27:\n",
      "print(max(12,3,61, 2.3e3))\n",
      "print(max('B', 'A'))\n",
      "327/28:\n",
      "print(max(12,3,61, 2.3e3)) # Devuelve número más grande\n",
      "print(max('B', 'A'))       # ??\n",
      "327/29:\n",
      "print(max(12,3,61, 2.3e3)) # Devuelve número más grande\n",
      "print(max('B', 'A', \"H\", 'z'))       # ??\n",
      "327/30:\n",
      "print(max(12,3,61, 2.3e3)) # Devuelve número más grande\n",
      "print(max('B', 'A', \"H\", 'm'))       # ??\n",
      "327/31:\n",
      "print(max(12,3,61, 2.3e3)) # Devuelve número más grande\n",
      "print(max('B', 'A', \"H\", 'm', 'M'))       # ??\n",
      "327/32: max('m','M')\n",
      "327/33:\n",
      "print(max(12,3,61, 2.3e3)) # Devuelve número más grande\n",
      "print(max(12,3,61, 2e3))\n",
      "print(max('B', 'A', \"H\", 'm', 'M'))       # ??\n",
      "327/34:\n",
      "print(max(12,3,61, 2.3e3)) # Devuelve número más grande\n",
      "print(max(12,3,61, 8))\n",
      "print(max('B', 'A', \"H\", 'm', 'M'))       # ??\n",
      "327/35:\n",
      "seq = 'ATGGCGT'\n",
      "\n",
      "len(seq) # la funcion len devuelve 7, si no hacemos nada con eso, ese valor se pierde\n",
      "\n",
      "print('largo de la secuencia', len(seq)) # imprime en pantalla\n",
      "largo = len('ATGGCGT')                   # guarda en otra variable\n",
      "float(len(seq))                          # se lo pasa como argumento a otra funcion\n",
      "327/36:\n",
      "print(max(12,3,61, 2.3e3)) # Devuelve un float\n",
      "print(max(12,3,61, 8))     # Devuelve un integer\n",
      "print(max('B', 'A', \"H\", 'm', 'M'))       # ?? Devuelve un string\n",
      "327/37:\n",
      "#\n",
      "max(3, 'A')\n",
      "327/38:\n",
      "print(max(100, 30))\n",
      "print(abs(.29))\n",
      "327/39:\n",
      "print(max(100, 30))\n",
      "print(abs(-.29))\n",
      "327/40:\n",
      "print(max(100, 30))\n",
      "print(abs(- 4.29))\n",
      "327/41:\n",
      "print(max(100, 30))\n",
      "\n",
      "print(abs(-4.29))\n",
      "327/42:\n",
      "Qué pasa si ejecutamos el siguiente código\n",
      "```python\n",
      "max(3, 'A')\n",
      "```\n",
      "327/43:\n",
      "Qué pasa si ejecutamos el siguiente código\n",
      "```python\n",
      "max(3, 'A')\n",
      "```\n",
      "327/44: texto = \"Python\n",
      "327/45: print(TEXTO)\n",
      "327/46: print (texto)\n",
      "327/47: print (\"Hola\")\n",
      "327/48: len (\"Hola\")\n",
      "327/49:\n",
      "texto = 'Ptyhon'\n",
      "texto[8]\n",
      "327/50:\n",
      "texto = 'Python'\n",
      "texto[8]\n",
      "327/51:  'Python'[4]\n",
      "327/52:\n",
      "texto = 'Python'\n",
      "texto[3] = 'H'\n",
      "327/53: Podemos pedir ayuda utilizando la funcion built-in ``help()`` pansando el nombre de la función como argumento\n",
      "327/54: Podemos pedir ayuda utilizando la funcion built-in ``help()`` pansando el nombre de la función como argumento\n",
      "327/55: help(len)\n",
      "327/56: help(print)\n",
      "327/57: # help obtuvimos rapidamente los argumentos principales que recibe la función print, y una breve descripción de algunos argumentos opcionales\n",
      "327/58: # help obtuvimos rapidamente los argumentos principales que recibe la función print, y una breve descripción de algunos argumentos opcionales\n",
      "327/59: # help obtuvimos rapidamente los argumentos principales que recibe la función print, y una breve descripción de algunos argumentos opcionales\n",
      "327/60: ??print\n",
      "327/61: ??print()\n",
      "327/62: ??print\n",
      "327/63: ??print,\n",
      "327/64: ??print\n",
      "327/65: ??str\n",
      "327/66:\n",
      "<iframe id=\"inlineFrameExample\"\n",
      "    title=\"Inline Frame Example\"\n",
      "    width=\"300\"\n",
      "    height=\"200\"\n",
      "    src=\"https://www.openstreetmap.org/export/embed.html?bbox=-0.004017949104309083%2C51.47612752641776%2C0.00030577182769775396%2C51.478569861898606&layer=mapnik\">\n",
      "</iframe>\n",
      "327/67: from ipython.display import Iframe\n",
      "327/68: from Ipython.display import Iframe\n",
      "327/69:\n",
      "from IPython.display. import IFrame\n",
      "IFrame(src=\"https://stackoverflow.com/questions/642154/how-to-convert-strings-into-integers-in-python\", width='70%', height='400')\n",
      "327/70:\n",
      "from IPython.display import IFrame\n",
      "IFrame(src=\"https://stackoverflow.com/questions/642154/how-to-convert-strings-into-integers-in-python\", width='70%', height='400')\n",
      "327/71:\n",
      "from IPython.display import IFrame\n",
      "displa(IFrame(src=\"https://stackoverflow.com/questions/642154/how-to-convert-strings-into-integers-in-python\", width='70%', height='400'))\n",
      "327/72:\n",
      "from IPython.display import IFrame\n",
      "display(IFrame(src=\"https://stackoverflow.com/questions/642154/how-to-convert-strings-into-integers-in-python\", width='70%', height='400'))\n",
      "327/73: ??len\n",
      "327/74: ??float\n",
      "327/75: ??sum\n",
      "327/76:\n",
      "tomensé un ratito para revisar la ayuda de las funciones que uilizamos hasta ahora.\n",
      "Por ejemplo\n",
      "327/77: ??sum\n",
      "327/78: ??max\n",
      "327/79: ??len\n",
      "327/80: ??float\n",
      "323/1: genes = ['P53', 'KRAS', 'BRCA1', 'CDKN2A', \"BRCA2\"]\n",
      "323/2:\n",
      "print(genes)\n",
      "print(type(genes))\n",
      "323/3: print('El tercer gen de la lista es:', genes[3])\n",
      "323/4:\n",
      "genes2 = genes.copy() # copiamos la lista para no modificar la original\n",
      "print(genes2)\n",
      "323/5:\n",
      "genes2[2] = 'MYC'\n",
      "print(genes2)\n",
      "323/6:\n",
      "genes2 = genes.copy() # copiamos la lista para no modificar la original\n",
      "print(genes2)\n",
      "323/7:\n",
      "genes2[2] = 'MYC'\n",
      "print(genes2)\n",
      "323/8: genes = ['P53', 'KRAS', 'BRCA1', 'CDKN2A', \"BRCA2\"]\n",
      "323/9:\n",
      "print(genes)\n",
      "print(type(genes))\n",
      "323/10: print('El tercer gen de la lista es:', genes[3])\n",
      "323/11:\n",
      "genes2 = genes.copy() # copiamos la lista para no modificar la original\n",
      "print(genes2)\n",
      "323/12:\n",
      "genes2[2] = 'MYC'\n",
      "print(genes2)\n",
      "323/13:\n",
      "genes[0] = genes[-1]\n",
      "print(organimos2)\n",
      "323/14:\n",
      "genes[0] = genes[-1]\n",
      "print(genes)\n",
      "323/15:\n",
      "genes2[0] = genes[-1]\n",
      "print(genes2)\n",
      "323/16: genes = ['P53', 'KRAS', 'BRCA1', 'CDKN2A', \"BRCA2\"]\n",
      "323/17:\n",
      "print(genes)\n",
      "print(type(genes))\n",
      "323/18: print('El tercer gen de la lista es:', genes[3])\n",
      "323/19:\n",
      "print('El primer gen de la lista es:', _____ )\n",
      "print('El segundo gen de la lista es:', _____ )\n",
      "print('El gen de la lista es:', genes[__])\n",
      "323/20:\n",
      "genes2 = genes.copy() # copiamos la lista para no modificar la original\n",
      "print(genes2)\n",
      "323/21:\n",
      "genes2[2] = 'MYC'\n",
      "print(genes2)\n",
      "323/22:\n",
      "genes2[0] = genes[-1]\n",
      "print(genes2)\n",
      "323/23: print('Tenemos', len(genes), 'en la lista.')\n",
      "323/24:\n",
      "# recordemos que podemos asignar el largo de la lista a una variable para luego trabajar con ella\n",
      "cantidad_genes = len(genes)\n",
      "print('Tenemos', len(genes), 'en la lista.')\n",
      "323/25:\n",
      "print(genes)\n",
      "genes.append('RB1')\n",
      "print(genes)\n",
      "genes.insert(3, 'EGFR')\n",
      "print(genes)\n",
      "323/26:\n",
      "print(genes)\n",
      "genes.remove(\"BRCA2\")\n",
      "print(genes)\n",
      "genes.pop(3)\n",
      "print(genes)\n",
      "genes.pop(-1)\n",
      "print(genes)\n",
      "323/27:\n",
      "print(genes)\n",
      "genes.remove(\"BRCA2\")\n",
      "print(genes)\n",
      "323/28: genes = ['P53', 'KRAS', 'BRCA1', 'CDKN2A', \"BRCA2\"]\n",
      "323/29:\n",
      "print(genes)\n",
      "print(type(genes))\n",
      "323/30: print('El tercer gen de la lista es:', genes[3])\n",
      "323/31:\n",
      "print('El primer gen de la lista es:', _____ )\n",
      "print('El segundo gen de la lista es:', _____ )\n",
      "print('El gen de la lista es:', genes[__])\n",
      "323/32:\n",
      "genes2 = genes.copy() # copiamos la lista para no modificar la original\n",
      "print(genes2)\n",
      "323/33:\n",
      "genes2[2] = 'MYC'\n",
      "print(genes2)\n",
      "323/34:\n",
      "genes2[0] = genes[-1]\n",
      "print(genes2)\n",
      "323/35: print('Tenemos', len(genes), 'en la lista.')\n",
      "323/36:\n",
      "# recordemos que podemos asignar el largo de la lista a una variable para luego trabajar con ella\n",
      "cantidad_genes = len(genes)\n",
      "print('Tenemos', len(genes), 'en la lista.')\n",
      "323/37:\n",
      "print(genes)\n",
      "genes.append('RB1')\n",
      "print(genes)\n",
      "genes.insert(3, 'EGFR')\n",
      "print(genes)\n",
      "323/38:\n",
      "print(genes)\n",
      "genes.remove(\"BRCA2\")\n",
      "print(genes)\n",
      "323/39:\n",
      "print(genes)\n",
      "genes.pop(3)\n",
      "323/40:\n",
      "print(genes)\n",
      "genes.pop(-1)\n",
      "print(genes)\n",
      "323/41: print(sorted(genes))\n",
      "323/42: print(genes.sort())\n",
      "323/43: genes = ['P53', 'KRAS', 'BRCA1', 'CDKN2A', \"BRCA2\"]\n",
      "323/44:\n",
      "print(genes)\n",
      "genes.append('RB1')\n",
      "print(genes)\n",
      "323/45:\n",
      "genes.insert(3, 'EGFR')\n",
      "print(genes)\n",
      "323/46: genes\n",
      "323/47: genes = ['P53', 'KRAS', 'BRCA1', 'EGFR', 'CDKN2A', 'BRCA2', 'RB1']\n",
      "323/48: genes = ['P53', 'KRAS', 'BRCA1', 'EGFR', 'CDKN2A', 'BRCA2', 'RB1']\n",
      "323/49:\n",
      "supresores = [\"APC\",\"VHL\", \"BRCA1\",\"DCC\",\"SMAD4\",\"CDKN2A\",\"SMAD2\",\"MEN1\", \"NF1\",\"P53\",\"PTEN\",\"NF2\",\"WT1\",\"BRCA2\"]\n",
      "# Completar\n",
      "323/50:\n",
      "supresores = [\"APC\",\"VHL\", \"BRCA1\",\"DCC\",\"SMAD4\",\"CDKN2A\",\"SMAD2\",\"MEN1\", \"NF1\",\"P53\",\"PTEN\",\"NF2\",\"WT1\",\"BRCA2\"]\n",
      "oncogenes = [\"MCF2\",\"ALK\",\"EGFR\",\"HST\",\"ABL\",\"KRAS\",\"RET\",\"TSC2\",\"TRK\",\"MYC\"]\n",
      "# Completar \n",
      "\n",
      "# NO se, es medio falopa\n",
      "328/1: import pandas as pd\n",
      "328/2: !head ../data/swiss_prot.tab\n",
      "328/3: dataframe = pd.read_csv('../data/swiss-prot.tab')\n",
      "328/4: dataframe = pd.read_csv('../data/swiss_prot.tab')\n",
      "328/5: dataframe = pd.read_csv('../data/swiss_prot.tab', sep='\\t')\n",
      "328/6: dataframe\n",
      "328/7: df = dataframe\n",
      "328/8: df.shape()\n",
      "328/9: df.shape\n",
      "328/10: df.info()\n",
      "328/11: df.decribe()\n",
      "328/12: df.describe()\n",
      "328/13: pd.read_html('https://www.worldometers.info/coronavirus/')\n",
      "328/14: data = pd.read_csv('https://covid.ourworldindata.org/data/owid-covid-data.csv')\n",
      "328/15: daa\n",
      "328/16: data\n",
      "328/17: data.info()\n",
      "328/18: last_report = data.info()\n",
      "328/19: data.describe()\n",
      "328/20: data\n",
      "328/21: data.loc[data.location == \"2021-02-20\"]\n",
      "328/22: data.info()\n",
      "328/23: data.loc[data.date == \"2021-02-20\"]\n",
      "328/24: df = data.loc[data.date == \"2021-02-20\"]\n",
      "328/25: df\n",
      "328/26: df.head()\n",
      "328/27: df.head(10)\n",
      "328/28: df.head(10)\n",
      "328/29: df.info()\n",
      "328/30: df.head()\n",
      "328/31: data.loc[data.date == \"2021-02-10\"]\n",
      "328/32: data.loc[data.date == \"2021-02-10\"].info()\n",
      "328/33: data.loc[data.date == \"2021-02-15\"].info()\n",
      "328/34: data.loc[data.date == \"2021-02-18\"].info()\n",
      "328/35: df = data.loc[data.date == \"2021-02-18\"]\n",
      "328/36: df.head()\n",
      "328/37: df.info()\n",
      "328/38: df.isna()\n",
      "328/39: df.isna().sum()\n",
      "328/40: df.isna().sum() / df.shape[0]\n",
      "328/41: df.shape[0] - df.isna().sum() / df.shape[0]\n",
      "328/42: (df.shape[0] - df.isna().sum()) / df.shape[0]\n",
      "328/43: (df.shape[0] - df.isna().sum())*100 / df.shape[0]\n",
      "328/44: round((df.shape[0] - df.isna().sum())*100) / df.shape[0]\n",
      "328/45: ((df.shape[0] - df.isna().sum())*100).round(1) / df.shape[0]\n",
      "328/46: ((df.shape[0] - df.isna().sum())*100).round(2) / df.shape[0]\n",
      "328/47: ((df.shape[0] - df.isna().sum())*100) / df.shape[0]).round()\n",
      "328/48: (((df.shape[0] - df.isna().sum())*100) / df.shape[0]).round()\n",
      "328/49: (((df.shape[0] - df.isna().sum())*100) / df.shape[0]).round(0)\n",
      "328/50: (((df.shape[0] - df.isna().sum())*100) / df.shape[0]).round(-1)\n",
      "328/51: (((df.shape[0] - df.isna().sum())*100) / df.shape[0]).round()\n",
      "328/52: df.info()\n",
      "328/53: df.info() * 10\n",
      "328/54: df.info() * 20\n",
      "328/55: df.info()\n",
      "328/56: df\n",
      "328/57: columnas_importantes = ['iso_code', 'continent', 'location', 'total_cases','new_cases', 'total_deaths', 'new_deats', 'life_expectancy', 'hospital_beds_per_thousand','human_development_index', 'gdp_per_capita']\n",
      "328/58:\n",
      "# Quedarnos con un subset de columnas de la tabla\n",
      "subset = df.loc[columnas_importantes, :]  #version 1\n",
      "subset = df[columnas_importantes]\n",
      "328/59:\n",
      "# Quedarnos con un subset de columnas de la tabla\n",
      "subset = df.loc[: , columnas_importantes]  #version 1\n",
      "subset = df[columnas_importantes]\n",
      "328/60: df[columnas_importantes]\n",
      "328/61: columnas_importantes\n",
      "328/62: columnas_importantes = ['iso_code', 'continent', 'location', 'total_cases','new_cases', 'total_deaths', 'new_deaths', 'life_expectancy', 'hospital_beds_per_thousand','human_development_index', 'gdp_per_capita']\n",
      "328/63:\n",
      "# Quedarnos con un subset de columnas de la tabla\n",
      "subset = df.loc[: , columnas_importantes]  #version 1\n",
      "subset = df[columnas_importantes]\n",
      "328/64:\n",
      "# Quedarnos con un subset de columnas de la tabla\n",
      "subset1 = df.loc[: , columnas_importantes]  #version 1\n",
      "subset2 = df[columnas_importantes]\n",
      "328/65: subset1\n",
      "328/66: subset2\n",
      "328/67: subset2 is subset1\n",
      "328/68: subset2.equal(subset1)\n",
      "328/69: subset2.equals(subset1)\n",
      "328/70: subset2.equals(subset1.head())\n",
      "328/71: subset2.contains(subset1.head())\n",
      "328/72: subset2.equals(subset1.head())\n",
      "328/73:\n",
      "# Quedarnos con un subset de columnas de la tabla\n",
      "subset1 = df.loc[: , columnas_importantes]  # version 1\n",
      "subset2 = df[columnas_importantes]          # version 2\n",
      "328/74: df = subset2.copy()\n",
      "328/75:\n",
      "# Quedarnos con un subset de columnas de la tabla\n",
      "subset1 = df.loc[: , columnas_importantes]  # version 1\n",
      "subset2 = df[columnas_importantes]          # version 2\n",
      "328/76: df = subset2\n",
      "328/77: df is subset2\n",
      "328/78: subset2.copy()\n",
      "328/79: subset2.copy() is subset2\n",
      "328/80: df = subset2.copy()\n",
      "328/81: df is subset2\n",
      "328/82: df.head()\n",
      "328/83: subset1.head()\n",
      "328/84: df == subset1\n",
      "328/85: df.equals(subset1)\n",
      "328/86: df.equals(subset2)\n",
      "328/87: subset1.equals(subset2)\n",
      "328/88: df\n",
      "328/89: df.info()\n",
      "328/90: df.isna()\n",
      "328/91: df[df.isna()]\n",
      "328/92: df.values()[df.isna()]\n",
      "328/93: df.values[df.isna()]\n",
      "328/94: df\n",
      "328/95: df['new_cases']\n",
      "328/96: df.loc[:, 'new_cases']\n",
      "328/97: df\n",
      "328/98: df.loc[:50 ,: ]\n",
      "328/99: df.iloc[:50 ,: ]\n",
      "328/100: df.loc[df.continet == \"South America\", 'new_cases']\n",
      "328/101: df.loc[df.continent == \"South America\", 'new_cases']\n",
      "328/102: df.set_index('location')\n",
      "328/103: df\n",
      "328/104: df.set_index('location', inplace=True)\n",
      "328/105: df\n",
      "328/106: df.loc[df.continent == \"South America\", 'new_cases']\n",
      "328/107: nuevos_casos = df.loc[df.continent == \"South America\", 'new_cases']\n",
      "328/108: nuevos_casos\n",
      "328/109: nuevos_casos.plot.barh()\n",
      "328/110: nuevos_casos.sort_values().plot.barh()\n",
      "328/111: nuevos_casos.sort_values(ascending=False).plot.barh()\n",
      "328/112: nuevos_casos.sort_values().plot.barh()\n",
      "328/113:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,56747,4,32,42,423,423,423,4,23354,47,5,867,83,24,312341,14]\n",
      "pares = []\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "pares\n",
      "328/114:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,56747,4,32,42,423,423,423,4,23354,47,5,867,83,24,312341,14]\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.counts(numero)\n",
      "pares\n",
      "repeticiones\n",
      "328/115:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,56747,4,32,42,423,423,423,4,23354,47,5,867,83,24,312341,14]\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "repeticiones\n",
      "328/116:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856]\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "*repeticiones,\n",
      "328/117:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856]\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "*repeticiones,\n",
      "328/118:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856]\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "*repeticiones.items(),\n",
      "328/119:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856]\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "repeticiones\n",
      "328/120:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856]\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "for numero, rep in repeticiones.items():\n",
      "    if rep > 1:\n",
      "        print(numero, rep)\n",
      "328/121:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * 1000000\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "for numero, rep in repeticiones.items():\n",
      "    if rep > 1:\n",
      "        print(numero, rep)\n",
      "328/122:\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * 10\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "for numero, rep in repeticiones.items():\n",
      "    if rep > 1:\n",
      "        print(numero, rep)\n",
      "328/123:\n",
      "n =  10\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "for numero, rep in repeticiones.items():\n",
      "    if rep > n:\n",
      "        print(numero, rep)\n",
      "328/124:\n",
      "n =  100\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "for numero, rep in repeticiones.items():\n",
      "    if rep > n:\n",
      "        print(numero, rep)\n",
      "328/125:\n",
      "n =  1000\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "for numero, rep in repeticiones.items():\n",
      "    if rep > n:\n",
      "        print(numero, rep)\n",
      "328/126:\n",
      "n =  500\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "repeticiones = {}\n",
      "for numero in numeros:\n",
      "    if numero % 2 == 0:\n",
      "        pares.append(numero)\n",
      "    repeticiones[numero] = numeros.count(numero)\n",
      "pares\n",
      "for numero, rep in repeticiones.items():\n",
      "    if rep > n:\n",
      "        print(numero, rep)\n",
      "328/127: fibo = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
      "328/128: fibo * 2\n",
      "328/129: fibo -1\n",
      "328/130: fibo ** 2\n",
      "328/131:\n",
      "fibo2\n",
      "for n in fibo:\n",
      "    fibo2.append(n ** 2)\n",
      "328/132:\n",
      "fibo2 = fibo\n",
      "for n in fibo:\n",
      "    fibo2.append(n ** 2)\n",
      "328/133:\n",
      "fibo2 = fibo\n",
      "for n in fibo:\n",
      "    fibo2.append(n ** 2)\n",
      "328/134:\n",
      "fibo2 = fibo\n",
      "for n in fibo:\n",
      "    print(n)\n",
      "    fibo2.append(n ** 2)\n",
      "328/135:\n",
      "fibo2 = fibo.copy()\n",
      "for n in fibo:\n",
      "    print(n)\n",
      "    fibo2.append(n ** 2)\n",
      "328/136: fibo = [0, 1, 1, 2, 3, 5, 8, 13, 21, 34]\n",
      "328/137:\n",
      "fibo2 = fibo.copy()\n",
      "for n in fibo:\n",
      "    print(n)\n",
      "    fibo2.append(n ** 2)\n",
      "328/138: fibo\n",
      "328/139:\n",
      "fibo\n",
      "print(fibo2)\n",
      "328/140:\n",
      "fibo\n",
      "\n",
      "print(fibo2)\n",
      "328/141:\n",
      "print(fibo)\n",
      "\n",
      "print(fibo2)\n",
      "328/142:\n",
      "fibo2 = []\n",
      "for n in fibo:\n",
      "    print(n)\n",
      "    fibo2.append(n ** 2)\n",
      "328/143:\n",
      "print(fibo)\n",
      "\n",
      "print(fibo2)\n",
      "328/144: fibo2 - fibo\n",
      "328/145: fibo2 + fibo\n",
      "328/146: fibo2 * fibo\n",
      "328/147: import pandas as pd\n",
      "328/148: pd.Series()\n",
      "328/149: pd.Series(fibo)\n",
      "328/150:\n",
      "fibo_serie = pd.Series(fibo)\n",
      "print(fibo_serie)\n",
      "328/151: fibo\n",
      "328/152: fibo + 1\n",
      "328/153: fibo * 2\n",
      "328/154: fibo == 3\n",
      "328/155: fibo_serie\n",
      "328/156: fibo_serie + 10\n",
      "328/157: fibo_serie - 2\n",
      "328/158: fibo_serie * 10\n",
      "328/159: fibo_serie ** 10\n",
      "328/160: fibo_serie ++ fibo_serie\n",
      "328/161: fibo_serie * 2\n",
      "328/162: fibo_serie[3]\n",
      "328/163: fibo_serie[3] * 3\n",
      "328/164: fibo_serie * fibo_serie[3]\n",
      "328/165: fibo_serie * fibo_serie[4]\n",
      "328/166: fibo_serie.sort_values()\n",
      "328/167: fibo_serie.sort_values(ascending=True)\n",
      "328/168: fibo_serie.sort_values(ascending=False)\n",
      "328/169: fibo\n",
      "328/170: fibo == 5\n",
      "328/171: fibo_serie\n",
      "328/172: fibo_serie == 5\n",
      "328/173: fibo_serie == 5 # nos devuelve otra serie con valores True o False dependiendo los elementos de la serie que cumplen la condicion\n",
      "328/174: fibo_serie == 10 # nos devuelve otra serie con valores True o False dependiendo los elementos de la serie que cumplen la condicion\n",
      "328/175: fibo_serie\n",
      "328/176: fibo_serie == 50 # nos devuelve otra serie con valores True o False dependiendo los elementos de la serie que cumplen la condicion\n",
      "328/177: fibo_serie == 5 # nos devuelve otra serie con valores True o False dependiendo los elementos de la serie que cumplen la condicion\n",
      "328/178: fibo_serie == 5 # nos devuelve otra serie con valores True o False dependiendo los elementos de la serie que cumplen la condicion\n",
      "328/179:\n",
      "n =  500\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "for n in numeros:\n",
      "    pass\n",
      "328/180:\n",
      "n =  5000\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "for n in numeros:\n",
      "    pass\n",
      "328/181:\n",
      "n =  50000\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "for n in numeros:\n",
      "    pass\n",
      "328/182:\n",
      "n =  500\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "potencia = []\n",
      "for num in numeros:\n",
      "    potencia.append(num ** 2)\n",
      "328/183: numeros\n",
      "328/184: potencia\n",
      "328/185:\n",
      "n =  5000\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "potencia = []\n",
      "for num in numeros:\n",
      "    potencia.append(num ** 2)\n",
      "328/186:\n",
      "n =  50000\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "potencia = []\n",
      "for num in numeros:\n",
      "    potencia.append(num ** 2)\n",
      "328/187:\n",
      "n =  500000\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "potencia = []\n",
      "for num in numeros:\n",
      "    potencia.append(num ** 2)\n",
      "328/188: potencia\n",
      "328/189:\n",
      "n =  500000\n",
      "numeros = [2,4,5,67,21,4323,356,231,1235,75,56,1,125,8456,121,3,423,3456,31,1235,75,5656747,\n",
      "           4,32,42,423,423,423,4,23354,47,5,867,83,22,423,423,4,23354,4724,312341,14, 7,23,6,34,212,67,\n",
      "           7,12313,53,6,676,7567,456,241235,365,756856] * n\n",
      "#potencia = []\n",
      "#for num in numeros:\n",
      "#    potencia.append(num ** 2)\n",
      "serie = pd.Series(numeros)\n",
      "potencia = serie ** 2\n",
      "328/190: srie\n",
      "328/191: serie\n",
      "328/192: serie * 10\n",
      "328/193: serie * 500\n",
      "328/194: serie * 100000\n",
      "328/195:\n",
      "for n in numeros:\n",
      "    n ** 100000\n",
      "329/1: import pandas as pd\n",
      "329/2: pd.read_xlsx('/home/fernando/Downloads/Inscripci%C3%B3n%20a%20Escuela%20de%20Verano%20Bioinform%C3%A1tica%20FIL-A2B2C%202021%20(Responses).xlsx')\n",
      "329/3: pd.read_xlx('/home/fernando/Downloads/Inscripci%C3%B3n%20a%20Escuela%20de%20Verano%20Bioinform%C3%A1tica%20FIL-A2B2C%202021%20(Responses).xlsx')\n",
      "329/4: pd.read_xls('/home/fernando/Downloads/Inscripci%C3%B3n%20a%20Escuela%20de%20Verano%20Bioinform%C3%A1tica%20FIL-A2B2C%202021%20(Responses).xlsx')\n",
      "329/5: pd.read_xlsx('/home/fernando/Downloads/Inscripci%C3%B3n%20a%20Escuela%20de%20Verano%20Bioinform%C3%A1tica%20FIL-A2B2C%202021%20(Responses).xlsx')\n",
      "329/6: pd.read_excel('/home/fernando/Downloads/Inscripci%C3%B3n%20a%20Escuela%20de%20Verano%20Bioinform%C3%A1tica%20FIL-A2B2C%202021%20(Responses).xlsx')\n",
      "329/7: data = pd.read_excel('/home/fernando/Downloads/escuela.xlsx')\n",
      "329/8: data\n",
      "329/9: data.iloc[0,:]\n",
      "335/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "335/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "335/3: tablas['human_box1']\n",
      "335/4: tablas['human_box1'].drop_duplicates(\"uniprot\")\n",
      "335/5: for i in tablas['human_box1'].drop_duplicates(\"uniprot\").uniprot.unique():print(i)\n",
      "335/6: for i in tablas['human_box1'].drop_duplicates(\"uniprot\").uniprot.unique():print(i.strip())\n",
      "335/7: tabla = pd.read_excel('/home/fernando/Downloads/pancsa_farahi_drivers.xlsx')\n",
      "335/8: talba\n",
      "335/9: tabla[0]\n",
      "335/10: tabla\n",
      "336/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "336/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "336/3: pasnca = pd.read_csv('/home/fernando/Documents/pansca.csv', header=None)\n",
      "336/4: pansca\n",
      "336/5: pan = pd.read_csv('/home/fernando/Documents/pansca.csv', header=None)\n",
      "336/6: pansca\n",
      "336/7: pan\n",
      "336/8: pan.rename(columns={0:\"uniprot\"})\n",
      "336/9: pan = pan.rename(columns={0:\"uniprot\"})\n",
      "336/10: pan_u = pan.uniprot.tolist()\n",
      "336/11: far  = pan = pd.read_csv('/home/fernando/Documents/df1.csv', header=None)\n",
      "336/12:\n",
      "pan = pd.read_csv('/home/fernando/Documents/pansca.csv', header=None)\n",
      "pan\n",
      "336/13:\n",
      "pan = pd.read_csv('/home/fernando/Documents/pansca.csv', header=None)\n",
      "pan\n",
      "336/14: pan = pan.rename(columns={0:\"uniprot\", 8:'org'})\n",
      "336/15:\n",
      "pan_u = pan.uniprot.tolist()\n",
      "panhs_u = pan.query('org == \"Homo sapiens\"').uniprot.tolist()\n",
      "336/16: len(pan_u)\n",
      "336/17: len(panhs_u)\n",
      "336/18: far\n",
      "336/19: far  = pan = pd.read_csv('/home/fernando/Documents/df1.csv')\n",
      "336/20: far\n",
      "336/21: far_u = far[\"UniProt AC\"].tolist()\n",
      "336/22: len(far_u)\n",
      "336/23: marg = tablas['human_box1'].uniprot.unique().tolist()\n",
      "336/24: mar = tablas['human_box1'].uniprot.unique().tolist()\n",
      "336/25: set(mar).intersection(set(panhs_u))\n",
      "336/26: len(set(mar).intersection(set(panhs_u)))\n",
      "336/27:\n",
      "pan = pd.read_csv('/home/fernando/Documents/pansca.csv', header=None)\n",
      "pan\n",
      "336/28: pan = pan.rename(columns={0:\"uniprot\", 8:'org'})\n",
      "336/29:\n",
      "pan_u = pan.uniprot.tolist()\n",
      "panhs_u = pan.query('org == \"Homo sapiens\"').uniprot.tolist()\n",
      "336/30: len(pan_u)\n",
      "336/31: len(panhs_u)\n",
      "336/32: far  = pan = pd.read_csv('/home/fernando/Documents/df1.csv')\n",
      "336/33: far_u = far[\"UniProt AC\"].tolist()\n",
      "336/34: len(far_u)\n",
      "336/35: mar = tablas['human_box1'].uniprot.unique().tolist()\n",
      "336/36: len(set(mar).intersection(set(panhs_u)))\n",
      "336/37: len(set(mar).intersection(set(far)))\n",
      "336/38: far\n",
      "336/39: len(set(mar).intersection(set(far_u)))\n",
      "338/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "338/2:\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "338/3: tablas['human_dis']\n",
      "338/4: tablas['human_dis'].query('box == \"box1\"')\n",
      "338/5: box1 = tablas['human_dis'].query('box == \"box1\"')\n",
      "338/6: bo1[box1.db.isin(['phasepro','drllps_scaffolds'])]\n",
      "338/7: box1[box1.db.isin(['phasepro','drllps_scaffolds'])]\n",
      "338/8: box1[box1.db.isin(['phasepro','drllps_scaffolds'])].uniprots\n",
      "338/9: box1[box1.db.isin(['phasepro','drllps_scaffolds'])].uniprot\n",
      "338/10: box1['tmp'] = ''\n",
      "338/11: box1\n",
      "338/12: box1[box1.uniprot.isin(box1.db.isin(['phasepro','drllps_scaffolds'])].uniprot)]\n",
      "338/13: box1[box1.uniprot.isin(box1.db.isin(['phasepro','drllps_scaffolds']).uniprot)]\n",
      "338/14: box1[box1.uniprot.isin(box1[box1.db.isin(['phasepro','drllps_scaffolds'])].uniprot ), 'tmp'] = '_sc'\n",
      "338/15: box1.loc[box1.uniprot.isin(box1[box1.db.isin(['phasepro','drllps_scaffolds'])].uniprot ), 'tmp'] = '_sc'\n",
      "338/16: box1\n",
      "338/17: box1.tmp.value_counts()\n",
      "338/18: box1['DB'] = box1.db + box1.tmp\n",
      "338/19: box1\n",
      "338/20: len(box1.uniprot.unique())\n",
      "338/21: sns.boxplot(data = box1, x='DB', y='dc')\n",
      "338/22:\n",
      "_ = plt.subplots(figsize=(14,5))\n",
      "sns.boxplot(data = box1, x='DB', y='dc')\n",
      "339/1:\n",
      "import pickle\n",
      "import matplotlib.pyplot as plt\n",
      "import seaborn as sns\n",
      "import pandas as pd\n",
      "339/2:\n",
      "with open('tablas_FINAL.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "print(tablas['info'])\n",
      "339/3:\n",
      "with open('tablas_FINAL.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "tablas.keys()\n",
      "339/4:\n",
      "# Pickle con la mayoria de las tablas\n",
      "with open('/home/fernando/git/mlo/todo/tablas/tablas_pickle_final.pkl', 'br') as f:\n",
      "    tablas = pickle.load(f)\n",
      "339/5: tablas['human_db']\n",
      "339/6: tablas['db']\n",
      "339/7: tablas['human_db']\n",
      "339/8: tablas['info']\n",
      "339/9: print(tablas['info'])\n",
      "339/10: tablas['db'].query('org == \"Homo sapiens\"')\n",
      "339/11: human = tablas['db'].query('org == \"Homo sapiens\"')\n",
      "339/12: tablas['human_boxes']\n",
      "339/13: human.merge(tablas['human_boxes'])\n",
      "339/14: human = human.merge(tablas['human_boxes'])\n",
      "339/15: human\n",
      "339/16: human.merge(tablas['human_dis'])\n",
      "339/17: tablas['human_dis']\n",
      "339/18: tablas['human_dis'].drop_duplicates(\"uniprot\")\n",
      "339/19: human.drop_duplicates(\"uniprot\")\n",
      "339/20: human.merge(tablas['human_dis'], how='left')\n",
      "339/21: human.merge(tablas['human_dis'][['uniprot','dc']])\n",
      "339/22: human#.merge(tablas['human_dis'][['uniprot','dc']])\n",
      "339/23: tablas['human_dis'].set_index('uniprot')['dc']\n",
      "339/24: human['dc'] = human.uniprot.map(tablas['human_dis'].set_index('uniprot')['dc'].todict())\n",
      "339/25: human['dc'] = human.uniprot.map(tablas['human_dis'].set_index('uniprot')['dc'].to_dict())\n",
      "339/26: human\n",
      "339/27: human.boxplot('dc','db')\n",
      "339/28: human.boxplot('dc','db', figsize = (12,4) )\n",
      "339/29: scaffolds = human[human.db.isin(['phasepro', 'drllps_scaffolds'])]\n",
      "339/30: scaffolds = human[human.db.isin(['phasepro', 'drllps_scaffolds'])].uniprot.unique().tolist()\n",
      "339/31: scaffolds\n",
      "339/32: human.loc[(human.db == \"phasepdb_lt\") & ~human.uniprot.isin(scaffolds)]\n",
      "339/33: human.loc[(human.db == \"phasepdb_lt\") & ~human.uniprot.isin(scaffolds), 'db'] == \"phasepdb_lt_no_scaffold\"\n",
      "339/34: human.loc[(human.db == \"phasepdb_lt\") & ~human.uniprot.isin(scaffolds), 'db'] = \"phasepdb_lt_no_scaffold\"\n",
      "339/35: human.boxplot('dc','db', figsize = (12,4) )\n",
      "339/36: box1 = human.query('box == \"box1\"')\n",
      "339/37: len(box1.uniprot.unique())\n",
      "339/38: !ls\n",
      "339/39: !ls ../../\n",
      "339/40: !ls ../todos/tablas\n",
      "339/41: !ls ../\n",
      "339/42: !ls ../todo/tablas\n",
      "339/43: seqs = pd.read_csv(\"../todo/tablas/secuencias_box1.fasta\")\n",
      "339/44: seqs = pd.read_csv(\"../todo/tablas/secuencias.csv\")\n",
      "339/45: seqs\n",
      "339/46:\n",
      "for i, row in seqs.iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(i.row)\n",
      "339/47:\n",
      "for i, row in seqs.iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/48:\n",
      "for i, row in seqs[seqs.uniprot.isin(box1.uniprot)].iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/49:\n",
      "scores {}\n",
      "with open('pscore.tab') as f:\n",
      "    for line in f.readlines():\n",
      "        print(line)\n",
      "339/50:\n",
      "scores = {}\n",
      "with open('pscore.tab') as f:\n",
      "    for line in f.readlines():\n",
      "        print(line)\n",
      "339/51:\n",
      "scores = {}\n",
      "with open('pscore.tab') as f:\n",
      "    for line in f.readlines():\n",
      "        print(line.split(' '))\n",
      "        break\n",
      "339/52:\n",
      "scores = {}\n",
      "with open('pscore.tab') as f:\n",
      "    for line in f.readlines():\n",
      "        print([i for i in line.split(' ') if i])\n",
      "        break\n",
      "339/53:\n",
      "scores = {}\n",
      "with open('pscore.tab') as f:\n",
      "    for line in f.readlines():\n",
      "        _ = [i for i in line.split(' ') if i]\n",
      "        scores[_[-1].strip()] = float(i[1])\n",
      "339/54:\n",
      "scores = {}\n",
      "with open('pscore.tab') as f:\n",
      "    for line in f.readlines():\n",
      "        _ = [i for i in line.split(' ') if i]\n",
      "        scores[_[-1].strip()] = float(_[1])\n",
      "339/55: scores\n",
      "339/56:\n",
      "scores = {}\n",
      "with open('pscore.tab') as f:\n",
      "    for line in f.readlines():\n",
      "        _ = [i for i in line.split(' ') if i]\n",
      "        scores[_[-1].split('|')[1]] = float(_[1])\n",
      "339/57: scores\n",
      "339/58: box1['pscore'] = box1.uniprot.map(scores)\n",
      "339/59: box1\n",
      "339/60: box1.boxplot('db', 'pscore')\n",
      "339/61: box1.boxplot('db', 'pscore')\n",
      "339/62: box1#.boxplot('db', 'pscore')\n",
      "339/63: box1.isna().sum()\n",
      "339/64: box.boxplot(x='db', y='pscore')\n",
      "339/65: box1.boxplot(x='db', y='pscore')\n",
      "339/66: box1.boxplot('pscore', 'db')\n",
      "339/67: box1.boxplot('pscore', 'db', figisze=(10,5))\n",
      "339/68: box1.boxplot('pscore', 'db', figsze=(10,5))\n",
      "339/69: box1.boxplot('pscore', 'db', figsize=(10,5))\n",
      "339/70: box1.boxplot('pscore', 'db', figsize=(14,5))\n",
      "339/71: sns.scatterplot(data=box1, x='dis', y='pscore')\n",
      "339/72: sns.scatterplot(data=box1, x='dc', y='pscore')\n",
      "339/73: sns.scatterplot(data=box1, x='dc', y='pscore', hue='db')\n",
      "339/74: sns.violintplot(data=box1, x='db', y='pscore', hue='db')\n",
      "339/75: sns.violinplot(data=box1, x='db', y='pscore', hue='db')\n",
      "339/76:\n",
      "_ = plt.subplots(figsize=(14,5))\n",
      "sns.violinplot(data=box1, x='db', y='pscore', hue='db')\n",
      "339/77:\n",
      "_ = plt.subplots(figsize=(14,5))\n",
      "sns.violinplot(data=box1[box1.db.str.startswith('phasepdb')], x='db', y='pscore', hue='db')\n",
      "339/78: %history\n",
      "339/79:\n",
      "for i, row in seqs[seqs.uniprot.isin(box1.uniprot)].iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/80:\n",
      "for i, row in seqs[seqs.uniprot.isin(box1.uniprot)][:99].iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/81:\n",
      "for i, row in seqs[seqs.uniprot.isin(box1.uniprot)][99:200].iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/82:\n",
      "for i, row in seqs[seqs.uniprot.isin(box1.uniprot)][200:300].iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/83:\n",
      "for i, row in seqs[seqs.uniprot.isin(box1.uniprot)][300:400].iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/84:\n",
      "for i, row in seqs[seqs.uniprot.isin(box1.uniprot)][400:500].iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/85:\n",
      "for i, row in seqs[seqs.uniprot.isin(box1.uniprot)][500:600].iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/86:\n",
      "for i, row in seqs[seqs.uniprot.isin(box1.uniprot)][400:500].iterrows():\n",
      "    print(\">\"+row.uniprot)\n",
      "    print(row.seq)\n",
      "339/87:\n",
      "import os\n",
      "for archivo in os.listdir(\"/home/fernando/Downloads/a600\"):\n",
      "    print(archivo)\n",
      "339/88:\n",
      "import os\n",
      "root = \"/home/fernando/Downloads/a600\"\n",
      "\n",
      "for archivo in os.listdir():\n",
      "    print(pd.read_csv(root + archivo, sep='\\t').head())\n",
      "339/89:\n",
      "import os\n",
      "root = \"/home/fernando/Downloads/a600\"\n",
      "\n",
      "for archivo in os.listdir(root):\n",
      "    print(pd.read_csv(root + archivo, sep='\\t').head())\n",
      "339/90:\n",
      "import os\n",
      "root = \"/home/fernando/Downloads/a600\"\n",
      "\n",
      "for archivo in os.listdir(root):\n",
      "    print(pd.read_csv(root+\"/\" + archivo, sep='\\t').head())\n",
      "339/91:\n",
      "import os\n",
      "root = \"/home/fernando/Downloads/a600\"\n",
      "\n",
      "for archivo in os.listdir(root):\n",
      "    print(pd.read_csv(root+\"/\" + archivo + \"/result\", sep='\\t').head())\n",
      "339/92:\n",
      "import os\n",
      "root = \"/home/fernando/Downloads/a600\"\n",
      "dfs = []\n",
      "for archivo in os.listdir(root):\n",
      "    try:\n",
      "        dfs.append(pd.read_csv(root+\"/\" + archivo + \"/result\", sep='\\t'))\n",
      "    except:\n",
      "        pass\n",
      "339/93: dfs\n",
      "339/94: pd.concat(dfs)\n",
      "339/95: pd.concat(dfs)['PSP?'].value_counts()\n",
      "339/96: dc 0 pd.read_csv('sp_mobidb.tsv', sep='\\t')\n",
      "339/97: dc = pd.read_csv('sp_mobidb.tsv', sep='\\t')\n",
      "339/98: dc\n",
      "339/99: dc.set_index('acc').content_fraction.todict()\n",
      "339/100: dc.set_index('acc').content_fraction.to_dict()\n",
      "339/101:\n",
      "boxes = tablas['human_boxes']\n",
      "boxes[\"dc\"] = boxes.uniprot.map(dc.set_index('acc').content_fraction.to_dict())\n",
      "339/102: boxes\n",
      "339/103: boxes.drop_duplicates(['uniprot', 'box']).boxplot('dc','box')\n",
      "339/104: boxes.sort_values('box').drop_duplicates('uniprot')\n",
      "339/105: boxes.sort_values('box').drop_duplicates('uniprot').box.value_counts()\n",
      "339/106: boxes.sort_values('box').drop_duplicates('uniprot')\n",
      "339/107: boxes.sort_values('box').drop_duplicates('uniprot').set_index('uniprot').dc.to_dict()\n",
      "339/108: dc['box'] = dc.acc.map(boxes.sort_values('box').drop_duplicates('uniprot').set_index('uniprot').dc.to_dict())\n",
      "339/109: dc\n",
      "339/110: dc.box = dc.box.fillna('control')\n",
      "339/111: dc\n",
      "339/112: dc['box'] = dc.acc.map(boxes.sort_values('box').drop_duplicates('uniprot').set_index('uniprot').box.to_dict())\n",
      "339/113: dc.box = dc.box.fillna('control')\n",
      "339/114: dc\n",
      "339/115: dc.boxplot('dc','box')\n",
      "339/116: dc.boxplot('fraction_content','box')\n",
      "339/117: dc\n",
      "339/118: dc.boxplot('content_fraction','box')\n",
      "339/119: dc.query('content_fraction <= 1').boxplot('content_fraction','box')\n",
      "339/120: dc.drop_duplicates([\"acc\",'box']).query('content_fraction <= 1').boxplot('content_fraction','box')\n",
      "339/121:\n",
      "desorden = dc.drop_duplicates([\"acc\",'box']).query('content_fraction <= 1')\n",
      "sns.boxplot(data=desorden, x='box', 'content_fraction')\n",
      "339/122:\n",
      "desorden = dc.drop_duplicates([\"acc\",'box']).query('content_fraction <= 1')\n",
      "sns.boxplot(data=desorden, x='box',y= 'content_fraction')\n",
      "339/123:\n",
      "desorden = dc.drop_duplicates([\"acc\",'box']).query('content_fraction <= 1')\n",
      "sns.boxplot(data=desorden, x='box',y= 'content_fraction', notch=True)\n",
      "339/124: human\n",
      "339/125: human.drop_duplicates([\"uniprot\",'mlo_loc'])\n",
      "339/126: human.drop_duplicates([\"uniprot\",'mlo_loc']).uniprot.value_counts()\n",
      "339/127: human.drop_duplicates([\"uniprot\",'mlo_loc']).uniprot.value_counts().to_dict()\n",
      "339/128: nmlos = human.drop_duplicates([\"uniprot\",'mlo_loc']).uniprot.value_counts().to_dict()\n",
      "339/129: dc['nmlos'] = dc.acc.map(nmlos)\n",
      "339/130: sns.boxplot(data=desorden, x='nmlos',y= 'content_fraction', notch=True)\n",
      "339/131:\n",
      "desorden = dc.drop_duplicates([\"acc\",'box']).query('content_fraction <= 1')\n",
      "sns.boxplot(data=desorden, x='box',y= 'content_fraction', notch=True)\n",
      "339/132: sns.boxplot(data=desorden, x='nmlos',y= 'content_fraction', notch=True)\n",
      "339/133: dc['nmlos3'] = dc.mlos.map(lambda x: x if x < 3 else 3)\n",
      "339/134: dc['nmlos3'] = dc.nmlos.map(lambda x: x if x < 3 else 3)\n",
      "339/135: sns.boxplot(data=desorden, x='nmlos',y= 'content_fraction', notch=True)\n",
      "339/136: sns.boxplot(data=desorden, x='nmlos3',y= 'content_fraction', notch=True)\n",
      "339/137:\n",
      "desorden = dc.drop_duplicates([\"acc\",'box']).query('content_fraction <= 1')\n",
      "sns.boxplot(data=desorden, x='box',y= 'content_fraction', notch=True)\n",
      "339/138: sns.boxplot(data=desorden, x='nmlos3',y= 'content_fraction', notch=True)\n",
      "339/139: desorden.groupby('nmlos3').mean()\n",
      "339/140: desorde.query('nmlos == 3')\n",
      "339/141: desorden.query('nmlos == 3')\n",
      "339/142: sns.boxplot(data=desorden.drop_duplicates([\"uniprot\"]), x='nmlos3',y= 'content_fraction', notch=True)\n",
      "339/143: sns.boxplot(data=desorden.drop_duplicates('acc'), x='nmlos3',y= 'content_fraction', notch=True)\n",
      "339/144: sns.violinplot(data=desorden.drop_duplicates('acc'), x='nmlos3',y= 'content_fraction', notch=True)\n",
      "339/145: uniprot = pd.read_csv('sp_uniprot.tab', sep='\\t')\n",
      "339/146: uniprot\n",
      "339/147: uniprot.loc[0,'Subcellular location[CC]']\n",
      "339/148: uniprot.columns\n",
      "339/149: uniprot.loc[0, 'Subcellular location [CC]]\n",
      "339/150: uniprot.loc[0, 'Subcellular location [CC]']\n",
      "339/151: uniprot.loc[:10, 'Subcellular location [CC]']\n",
      "339/152: uniprot.loc[:10, 'Subcellular location [CC]'].str.replace('SUBCELLULAR LOCATION: ','')\n",
      "339/153: uniprot.loc[:10, 'Subcellular location [CC]'].str.replace('SUBCELLULAR LOCATION: ','').str.split('; ')\n",
      "339/154: uniprot['local'] = uniprot.loc[:, 'Subcellular location [CC]'].str.replace('SUBCELLULAR LOCATION: ','').str.split('; ')\n",
      "339/155: uniprot\n",
      "339/156: uniprot['local'] = uniprot.loc[:, 'Subcellular location [CC]'].str.replace('SUBCELLULAR LOCATION: ','').replace('.','').str.split('; ')\n",
      "339/157: uniprot\n",
      "339/158: uniprot.explode('local')\n",
      "339/159:\n",
      "exp = uniprot.explode('local')\n",
      "exp.local = exp.local.replace('.','')\n",
      "exp.local.value_counts()\n",
      "339/160:\n",
      "exp = uniprot.explode('local')\n",
      "exp.local = exp.local.replace('.','')\n",
      "exp.local.value_counts()[:50]\n",
      "339/161: len(exp)\n",
      "339/162: uniprot = pd.read_csv('swiss_prot.tab', sep='\\t')\n",
      "339/163: uniprot.columns\n",
      "339/164: import localcider\n",
      "339/165: from localcider.sequenceParameters import SequenceParameters\n",
      "339/166: uniprots[['uniprot','seq']]\n",
      "339/167: uniprot[['uniprot','seq']]\n",
      "339/168: uniprot[['acc','Sequence']]\n",
      "339/169: uniprot\n",
      "339/170: uniprot[['Entry','Sequence']]\n",
      "339/171: seqs = uniprot[['Entry','Sequence']]\n",
      "339/172: seqs\n",
      "339/173: seqs['X' in seqs.Sequence]\n",
      "339/174: seqs[seqs.Sequence.contains(\"X\")]\n",
      "339/175: seqs[seqs.Sequence.str.contains(\"X\")]\n",
      "339/176: seqs[seqs.Sequence.str.contains(\"x\")]\n",
      "339/177: seqs\n",
      "339/178: seqs.Sequence.map(lambda x: SequenceParameters(x)).head()\n",
      "339/179: seqs.Sequence.map(lambda x: SequenceParameters(x)).head() if 'U' not in x else None)\n",
      "339/180: seqs.Sequence.map(lambda x: SequenceParameters(x).head() if 'U' not in x else None)\n",
      "339/181: seqs.head().Sequence.map(lambda x: SequenceParameters(x) if 'U' not in x else None)\n",
      "339/182: seqs['lseq'] = seqs.Sequence.map(lambda x: SequenceParameters(x) if 'U' not in x else None)\n",
      "339/183:\n",
      "seqs['fcr'] = seqs.lseq.map( get_FCR())\n",
      "seqs['ip'] = seqs.lseq.map(get_isoelectric_point())\n",
      "seqs['count_neg'] = seqs.lseq.map(get_countNeg())\n",
      "seqs['count_pos'] = seqs.lseq.map(get_countPos())\n",
      "seqs['f_n'] = seqs.lseq.map(get_fraction_negative())\n",
      "seqs['f_p'] = seqs.lseq.map(get_fraction_positive())\n",
      "seqs['faa'] = seqs.lseq.map(get_amino_acid_fractions())\n",
      "seqs['kappa'] = seqs.lseq.map(get_kappa())\n",
      "seqs['omega'] = seqs.lseq.map(get_Omega())\n",
      "seqs['cn'] = seqs.lseq.map(get_mean_net_charge())\n",
      "seqs['delta'] = seqs.lseq.map(get_delta())\n",
      "seqs['delta_max'] = seqs.lseq.map(get_deltaMax())\n",
      "339/184:\n",
      "seqs['fcr'] = seqs.lseq.map( lambda x: x.get_FCR())\n",
      "seqs['ip'] = seqs.lseq.map(lambda x: x.get_isoelectric_point())\n",
      "seqs['count_neg'] = seqs.lseq.map(lambda x: x.get_countNeg())\n",
      "seqs['count_pos'] = seqs.lseq.map(lambda x: x.get_countPos())\n",
      "seqs['f_n'] = seqs.lseq.map(lambda x: x.get_fraction_negative())\n",
      "seqs['f_p'] = seqs.lseq.map(lambda x: x.get_fraction_positive())\n",
      "seqs['faa'] = seqs.lseq.map(lambda x: x.get_amino_acid_fractions())\n",
      "seqs['kappa'] = seqs.lseq.map(lambda x: x.get_kappa())\n",
      "seqs['omega'] = seqs.lseq.map(lambda x: x.get_Omega())\n",
      "seqs['cn'] = seqs.lseq.map(lambda x: x.get_mean_net_charge())\n",
      "seqs['delta'] = seqs.lseq.map(lambda x: x.get_delta())\n",
      "seqs['delta_max'] = seqs.lseq.map(lambda x: x.get_deltaMax())\n",
      "339/185: seqs.lseq == None\n",
      "339/186: (seqs.lseq == None).sum()\n",
      "339/187: seqs\n",
      "339/188: seqs.sort_values('lseq')\n",
      "339/189: seqs.str.contains('U')\n",
      "339/190: seqs.Sequence.str.contains('U')\n",
      "339/191: seqs.Sequence.str.contains('U').sum()\n",
      "339/192: seqs.Sequence.str.contains('X').sum()\n",
      "339/193: seqs = seqs[~seqs.Sequence.str.contains('X')]\n",
      "339/194: seqs['lseq'] = seqs.Sequence.map(lambda x: SequenceParameters(x))\n",
      "339/195:\n",
      "from collections import Counter\n",
      "Counter(\"\".join(seqs.Sequence))\n",
      "339/196: seqs = seqs[~seqs.Sequence.str.contains('U')]\n",
      "339/197: seqs['lseq'] = seqs.Sequence.map(lambda x: SequenceParameters(x))\n",
      "339/198:\n",
      "seqs['fcr'] = seqs.lseq.map( lambda x: x.get_FCR())\n",
      "seqs['ip'] = seqs.lseq.map(lambda x: x.get_isoelectric_point())\n",
      "seqs['count_neg'] = seqs.lseq.map(lambda x: x.get_countNeg())\n",
      "seqs['count_pos'] = seqs.lseq.map(lambda x: x.get_countPos())\n",
      "seqs['f_n'] = seqs.lseq.map(lambda x: x.get_fraction_negative())\n",
      "seqs['f_p'] = seqs.lseq.map(lambda x: x.get_fraction_positive())\n",
      "seqs['faa'] = seqs.lseq.map(lambda x: x.get_amino_acid_fractions())\n",
      "seqs['kappa'] = seqs.lseq.map(lambda x: x.get_kappa())\n",
      "seqs['omega'] = seqs.lseq.map(lambda x: x.get_Omega())\n",
      "seqs['cn'] = seqs.lseq.map(lambda x: x.get_mean_net_charge())\n",
      "seqs['delta'] = seqs.lseq.map(lambda x: x.get_delta())\n",
      "seqs['delta_max'] = seqs.lseq.map(lambda x: x.get_deltaMax())\n",
      "337/1: !pwd\n",
      "337/2: dc = pd.read_csv('FINAL/sp_mobidb.tsv',sep='\\t')\n",
      "337/3:\n",
      "import pandas as pd\n",
      "dc = pd.read_csv('FINAL/sp_mobidb.tsv',sep='\\t')\n",
      "337/4: from collections import Counter\n",
      "337/5: Counter('FERNANDO')\n",
      "337/6: loc = pd.read_csv('FINAL/subcellular_location.tsv',sep='\\t')\n",
      "337/7: loc\n",
      "337/8: mapeo = pd.read_csv('FINAL/map.tab',sep='\\t')\n",
      "337/9: mapeo\n",
      "337/10: mapeo.set_index('Entry').iloc[:,1]\n",
      "337/11: mapeo.set_index('Entry').iloc[:,0]\n",
      "337/12: mapeo.set_index(0)\n",
      "337/13: mapeo\n",
      "337/14: mapeo.iloc[:,:3]\n",
      "337/15:\n",
      "mapeo = mapeo.iloc[:,:3]\n",
      "mapeo.columns = ['ens', '_','uniprot']\n",
      "337/16: mapeo.str.len()\n",
      "337/17: mapeo.ens.str.len()\n",
      "337/18: mapeo.ens.str.len().value_counts()\n",
      "337/19: mapeo.set_index('ens').uniprot.to_dict()\n",
      "337/20: loc['uniprot'] = loc.GENE.map(mapeo.set_index('ens').uniprot.to_dict())\n",
      "337/21: loc['uniprot'] = loc.Gene.map(mapeo.set_index('ens').uniprot.to_dict())\n",
      "337/22: loc\n",
      "337/23: loc.uniprot.isna().sum()\n",
      "337/24: loc['Main location'].str.split(\";\")\n",
      "337/25:\n",
      "loc['loca'] = loc['Main location'].str.split(\";\")\n",
      "loca = loc.explode('loca')\n",
      "337/26: loca\n",
      "337/27: loca.loca.value_counts()\n",
      "337/28: loca[loca.loca.isin([\"Nucleoplasm\",'Cytosol'])]\n",
      "337/29: loca[loca.loca.isin([\"Nucleoplasm\",'Cytosol'])][['uniprot','loca']]\n",
      "337/30: loca[loca.loca.isin([\"Nucleoplasm\",'Cytosol'])][['uniprot','loca']].dropna()\n",
      "337/31: locacn = loca[loca.loca.isin([\"Nucleoplasm\",'Cytosol'])][['uniprot','loca']].dropna()\n",
      "337/32: dc\n",
      "337/33: locacn['dc'] = locacn.uniprot.map(dc.set_index('uniprot').content_fraction.to_dict())\n",
      "337/34: locacn['dc'] = locacn.uniprot.map(dc.set_index('acc').content_fraction.to_dict())\n",
      "337/35: locacn\n",
      "337/36: locacn.boxplot('dc','loca')\n",
      "337/37: sns.boxplot(data = locacn, y ='dc', x='loca')\n",
      "337/38:\n",
      "import seaborn as sns\n",
      "sns.boxplot(data = locacn, y ='dc', x='loca')\n",
      "337/39:\n",
      "import seaborn as sns\n",
      "sns.boxplot(data = locacn, y ='dc', x='loca', notch=True)\n",
      "354/1: Vamos a probar que haya cargado correctamente la clase en Colab\n",
      "354/2: Para eso correr la siguiente celda, si imprime en la pantalla el número 3 sin dar ningun error, cargó correctamente y ya estamos listxs para programar.\n",
      "354/3: print(2 + 1)\n",
      "355/1: import pandas as pd\n",
      "355/2: data = pd.read_csv('/home/fernando/Downloads/pre_curso.csv')\n",
      "355/3: data\n",
      "355/4: data.columns\n",
      "355/5: columnas = ['ts','mail','name','dni','grabar', 'exp_leguajes', 'espera', 'estadistica']\n",
      "355/6: data.columns = columns\n",
      "355/7: data.columns = columnas\n",
      "355/8: data.columns\n",
      "355/9: columnas\n",
      "355/10: for i,e in zip(columnas, data.columns): print(i,e)\n",
      "355/11: len(data.columns)\n",
      "355/12: data.columns.tolist()\n",
      "355/13:\n",
      "['ts',\n",
      " 'mail',\n",
      " 'name',\n",
      " 'dni',\n",
      " 'grabar',\n",
      " 'nivel',\n",
      " 'exp_leguajes',\n",
      " 'espera',\n",
      " 'estadistica']\n",
      "355/14: data.columns = columnas\n",
      "355/15:\n",
      "columns = ['ts',\n",
      " 'mail',\n",
      " 'name',\n",
      " 'dni',\n",
      " 'grabar',\n",
      " 'nivel',\n",
      " 'exp_leguajes',\n",
      " 'espera',\n",
      " 'estadistica']\n",
      "355/16: data.columns = columns\n",
      "355/17: data\n",
      "355/18: data.nivel.value_counts().plot.pie()\n",
      "355/19: encuesta = data.copy()\n",
      "355/20: data = pd.read_csv('/home/fernando/Downloads/inscipcion.csv')\n",
      "355/21: data\n",
      "355/22: data.EMail\n",
      "355/23: data.EMail.dropna()\n",
      "355/24: data.set_index(\"EMail\")\n",
      "355/25: data.set_index(\"EMail\")[\"Inscripción a cursos (seleccionar al menos uno)\"]\n",
      "355/26: data.set_index(\"EMail\")[\"Inscripción a cursos (seleccionar al menos uno)\"].to_dict()\n",
      "355/27: encuesta['curso'] = data.mail.map(data.set_index(\"EMail\")[\"Inscripción a cursos (seleccionar al menos uno)\"].to_dict())\n",
      "355/28: encuesta['curso'] = encuesta.mail.map(data.set_index(\"EMail\")[\"Inscripción a cursos (seleccionar al menos uno)\"].to_dict())\n",
      "355/29: encuesta\n",
      "355/30: encuesta.curso.value_counts()\n",
      "355/31: encuesta.nivel.value_counts().plot.pie()\n",
      "355/32: encuesta[encuesta.srt.contains('Python')].nivel.value_counts().plot.pie()\n",
      "355/33: encuesta.nivel.value_counts().plot.pie()\n",
      "355/34: encuesta[encuesta.srt.curso.contains('Python')].nivel.value_counts().plot.pie()\n",
      "355/35: encuesta.nivel.value_counts().plot.pie()\n",
      "355/36: encuesta[encuesta.curso.str.contains('Python')].nivel.value_counts().plot.pie()\n",
      "355/37: encuesta[encuesta.curso.str.contains('Python')].dropna().nivel.value_counts().plot.pie()\n",
      "355/38: encuesta.dropna()[encuesta.curso.str.contains('Python')].nivel.value_counts().plot.pie()\n",
      "355/39: encuesta.dropna()[encuesta.dropna().curso.str.contains('Python')].nivel.value_counts().plot.pie()\n",
      "355/40: encuesta.nivel.value_counts().plot.pie()\n",
      "355/41: encuesta.nivel.value_counts().plot.pie()\n",
      "355/42: encuesta.curso.value_counts().plot.pie()\n",
      "355/43:\n",
      "print(len(encuesta.dropna()))\n",
      "len(encuesta)\n",
      "355/44: encuesta = encuesta.dropna()\n",
      "355/45: encuesta.nivel.value_counts().plot.pie()\n",
      "355/46: encuesta[encuesta.curso.str.contais(\"Python\")].value_counts().plot.pie()\n",
      "355/47: encuesta[encuesta.curso.str.contanis(\"Python\")].value_counts().plot.pie()\n",
      "355/48: encuesta[encuesta.curso.str.contains(\"Python\")].value_counts().plot.pie()\n",
      "355/49: encuesta[encuesta.curso.str.contains(\"Python\")].nivel.value_counts().plot.pie()\n",
      "355/50: encuesta[~encuesta.curso.str.contains(\"Python\")].nivel.value_counts().plot.pie()\n",
      "355/51: encuesta[~encuesta.curso.str.contains(\"Python\")].exp_lenguaje.value_counts().plot.pie()\n",
      "355/52: encuesta[~encuesta.curso.str.contains(\"Python\")].exp_lenguajes.value_counts().plot.pie()\n",
      "355/53: encuesta\n",
      "355/54: encuesta[~encuesta.curso.str.contains(\"Python\")].exp_leguajes.value_counts().plot.pie()\n",
      "355/55: encuesta[encuesta.curso.str.contains(\"Python\")].exp_leguajes.value_counts().plot.pie()\n",
      "355/56: encuesta\n",
      "355/57: encuesta.curso\n",
      "355/58: encuesta.curso.value_counts()\n",
      "355/59: encuesta.curso.unique().tolist()\n",
      "355/60:\n",
      "encuesta['CURSO'] = encuesta.curso.map({'Introducción al análisis de datos con R': 'R',\n",
      " 'Introducción al lenguaje de programación Python para Biología':'Python',\n",
      " 'Introducción al lenguaje de programación Python para Biología, Introducción al análisis de datos con R': 'Ambos'})\n",
      "355/61: encuesta\n",
      "355/62: encuesta.pivot_table(index='espera', columns='CURSO', aggfunc='size')\n",
      "355/63: encuesta.pivot_table(index='espera', columns='estadistica', aggfunc='size')\n",
      "355/64: encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size')\n",
      "355/65: encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size').fillna(0)\n",
      "355/66: encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size').fillna(0).plot.barh(stacked=True)\n",
      "355/67: encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size').fillna(0).T.plot.barh(stacked=True)\n",
      "355/68: encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size').fillna(0).T.plot.bar(stacked=True)\n",
      "355/69: estadistica = encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size').fillna(0).T#.plot.bar(stacked=True)\n",
      "355/70:\n",
      "for col in estadistica:\n",
      "    estadistica[col] = estadistica[col] / estadistica.sum(1)\n",
      "355/71: estadistica.plot.bar(stacked=True)\n",
      "355/72: estadistica\n",
      "355/73: estadistica = encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size').fillna(0).T#.plot.bar(stacked=True)\n",
      "355/74:\n",
      "for col in estadistica:\n",
      "    estadistica[col] = estadistica[col] / estadistica.sum()\n",
      "355/75: estadistica.plot.bar(stacked=True)\n",
      "355/76: estadistica = encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size').fillna(0).T#.plot.bar(stacked=True)\n",
      "355/77: estadistica\n",
      "355/78: estadistica.sum()\n",
      "355/79: estadistica.sum(1)\n",
      "355/80: for i in [1,2,3,4]: estadistica[i] = estadistica[i] / estadistica.sum(1)\n",
      "355/81: estadistica\n",
      "355/82: estadistica = encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size').fillna(0).T#.plot.bar(stacked=True)\n",
      "355/83: estadistica['all'] = estadistica.sum(1)\n",
      "355/84: estadistica\n",
      "355/85: estadistica = encuesta.pivot_table(index='estadistica', columns='CURSO', aggfunc='size').fillna(0).T#.plot.bar(stacked=True)\n",
      "355/86: estadistica.columns\n",
      "355/87: estadistica[8]\n",
      "355/88: estadistica[2]\n",
      "355/89: estadistica\n",
      "355/90: estadistica.sum(1)\n",
      "355/91: estadistica['all'] = estadistica.sum(1)\n",
      "355/92: estadistica\n",
      "355/93:\n",
      "for col in estadistica.columns:\n",
      "    print(col)\n",
      "355/94:\n",
      "for col in estadistica.columns:\n",
      "    print(estadistica[col])\n",
      "355/95:\n",
      "for col in estadistica.columns:\n",
      "    print(estadistica[col] / estadistica['all'])\n",
      "355/96: estadistica\n",
      "355/97:\n",
      "for col in estadistica.columns:\n",
      "    estadistica[col] = estadistica[col] / estadistica['all']\n",
      "355/98: estadistica\n",
      "355/99: estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "355/100:\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plot.legend('otuer')\n",
      "355/101:\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plot.legend(position='out')\n",
      "355/102:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "355/103:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plt.legend(loc='upper center')\n",
      "355/104:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plt.legend(loc='upper center', bbox_to_anchor=(0.5, -0.05))\n",
      "355/105:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plt.legend(loc='upper rigth', bbox_to_anchor=(0.5, -0.05))\n",
      "355/106:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plt.legend(loc='upper right', bbox_to_anchor=(0.5, -0.05))\n",
      "355/107:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plt.legend(loc='bottom right')\n",
      "355/108:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plt.legend(loc='bottom right',, bbox_to_anchor=(0.5, -0.05))\n",
      "355/109:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plt.legend(loc='bottom right', bbox_to_anchor=(0.5, -0.05))\n",
      "355/110:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True)\n",
      "plt.legend(loc='bottom right', bbox_to_anchor=(0.8, -0.05))\n",
      "355/111:\n",
      "import matplotlib.pyplot as plt\n",
      "estadistica.drop(columns='all').plot.bar(stacked=True, cmap='YlOrRd')\n",
      "plt.legend(loc='bottom right', bbox_to_anchor=(0.8, -0.05))\n",
      "355/112: encuesta\n",
      "355/113: encuesta.nivel\n",
      "355/114: encuesta.nivel.value_counts()\n",
      "355/115: nivel = encuesta.pivot_table(index='nivel', columns='CURSO', aggfunc='size').fillna(0).T#.plot.bar(stacked=True)\n",
      "355/116: nivel['all'] = nivel.sum(1)\n",
      "355/117:\n",
      "for col in nivel.columns:\n",
      "    nivel[col] = nivel[col] / nivel['all']\n",
      "355/118:\n",
      "nivel.drop(columns='all').plot.bar(stacked=True, cmap='YlOrRd')\n",
      "plt.legend(loc='bottom right', bbox_to_anchor=(0.8, -0.05))\n",
      "357/1: import pandas as pd\n",
      "357/2:\n",
      "data = pd.read_csv('/home/fernando/Downloads/inscipcion.csv')\n",
      "encuesta = pd.read_csv('//home/fernando/Downloads/pre_curso.csv')\n",
      "357/3: encuesta\n",
      "357/4: data\n",
      "357/5: data.set_index('EMail')\n",
      "357/6: data\n",
      "357/7: data['py'] = data[\"Inscripción a cursos (seleccionar al menos uno)\"].str.contains('Python')\n",
      "357/8: data\n",
      "357/9: python = data[data.py]\n",
      "357/10: python = data[data.fillna(False).py]\n",
      "357/11: python\n",
      "357/12: encuesta\n",
      "357/13: encuesa[encuesta['Email Address'].isin[python.EMail]]\n",
      "357/14: encuesta[encuesta['Email Address'].isin[python.EMail]]\n",
      "357/15: encuesta[encuesta['Email Address'].isin(python.EMail)]\n",
      "357/16: len(encuesta[encuesta['Email Address'].isin(python.EMail)])\n",
      "357/17: encuesta = encuesta[encuesta['Email Address'].isin(python.EMail)]\n",
      "357/18: encuesta\n",
      "357/19: encuesta['¿Con qué lenguajes de programación tenés experiencia?']\n",
      "357/20: encuesta['¿Con qué lenguajes de programación tenés experiencia?'].value_counts()\n",
      "357/21: encuesta['¿Con qué lenguajes de programación tenés experiencia?'].str.split(', ')\n",
      "357/22: encuesta['¿Con qué lenguajes de programación tenés experiencia?'].str.split(',')\n",
      "357/23: encuesta['¿Con qué lenguajes de programación tenés experiencia?'].str.split(', ')\n",
      "357/24: encuesta['¿Con qué lenguajes de programación tenés experiencia?'].str.split(' ')\n",
      "357/25: encuesta['¿Con qué lenguajes de programación tenés experiencia?'].str.split(', ')\n",
      "357/26: encuesta['lenguajes'] = encuesta['¿Con qué lenguajes de programación tenés experiencia?'].str.split(', ')\n",
      "357/27: encuesta.explode('lenguajes')\n",
      "357/28: encuesta.explode('lenguajes').lenguajes.value_counts()\n",
      "357/29: encuesta['¿Con qué lenguajes de programación tenés experiencia?'].value_counts()\n",
      "357/30: lenguajes = encuesta.explode('lenguajes').lenguajes.value_counts()\n",
      "357/31: lenguajes\n",
      "357/32: lenguajes['Otros'] = lenguajes[lenguajes < 10].sum()\n",
      "357/33: lenguajes\n",
      "357/34: lenguajes['Otros'] = 8\n",
      "357/35: lenguajes[['Python','R']]\n",
      "357/36: lenguajes[['Python','R', 'Ninguno']]\n",
      "357/37: lenguajes[['Python','R', 'Ninguno', 'Otros']]\n",
      "357/38: lenguajes[['Python','R', 'Ninguno', 'Otros']].plot.pie()\n",
      "357/39: lenguajes[['Python','R', 'Ninguno', 'Otros']].plot.barh()\n",
      "357/40: lenguajes[['Python','R', 'Ninguno', 'Otros']].sort_values().plot.barh()\n",
      "357/41: lenguajes[['Python','R', 'Ninguno', 'Otros']].sort_values().plot.pie()\n",
      "357/42:\n",
      "import matplotlib.pyplot as plt\n",
      "lenguajes[['Python','R', 'Ninguno', 'Otros']].sort_values().plot.pie()\n",
      "plt.savifig('lenguajes.svg')\n",
      "357/43:\n",
      "import matplotlib.pyplot as plt\n",
      "lenguajes[['Python','R', 'Ninguno', 'Otros']].sort_values().plot.pie()\n",
      "plt.savefig('lenguajes.svg')\n",
      "357/44:\n",
      "import matplotlib.pyplot as plt\n",
      "lenguajes[['Python','R', 'Ninguno', 'Otros']].sort_values().plot.pie(cmap='Pastel')\n",
      "plt.savefig('lenguajes.svg')\n",
      "357/45:\n",
      "import matplotlib.pyplot as plt\n",
      "lenguajes[['Python','R', 'Ninguno', 'Otros']].sort_values().plot.pie(cmap='Set2')\n",
      "plt.savefig('lenguajes.svg')\n",
      "357/46:\n",
      "import matplotlib.pyplot as plt\n",
      "lenguajes[['Python','R', 'Ninguno', 'Otros']].sort_values().plot.pie(cmap='Set1')\n",
      "plt.savefig('lenguajes.svg')\n",
      "357/47:\n",
      "import matplotlib.pyplot as plt\n",
      "lenguajes[['Python','R', 'Ninguno', 'Otros']].sort_values().plot.pie(cmap='Set3')\n",
      "plt.savefig('lenguajes.svg')\n",
      "357/48: encuesta\n",
      "357/49: encuesta['¿Cuál es tu nivel general de conocimiento de programación?']\n",
      "357/50: encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts()\n",
      "357/51: encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='Set3')\n",
      "357/52:\n",
      "encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='Pastel1')\n",
      "plt.savefig('experiencia.svg')\n",
      "357/53:\n",
      "encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='Pastel2')\n",
      "plt.savefig('experiencia.svg')\n",
      "357/54:\n",
      "encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='Tab10')\n",
      "plt.savefig('experiencia.svg')\n",
      "357/55:\n",
      "encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='tab10')\n",
      "plt.savefig('experiencia.svg')\n",
      "357/56:\n",
      "encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='tab10')\n",
      "plt.legend()\n",
      "plt.savefig('experiencia.svg')\n",
      "357/57:\n",
      "encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='tab10',autopct='%1%%')\n",
      "plt.legend()\n",
      "#plt.savefig('experiencia.svg')\n",
      "357/58:\n",
      "encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='tab10',autopct='%1.1f%%')\n",
      "plt.legend()\n",
      "#plt.savefig('experiencia.svg')\n",
      "357/59:\n",
      "encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='tab10',autopct='%1.f%%')\n",
      "plt.legend()\n",
      "#plt.savefig('experiencia.svg')\n",
      "357/60:\n",
      "encuesta['¿Cuál es tu nivel general de conocimiento de programación?'].value_counts().plot.pie(cmap='tab10',autopct='%1.f%%')\n",
      "plt.legend()\n",
      "plt.savefig('experiencia.svg')\n",
      "357/61: encuesta\n",
      "357/62: encuesta['Del curso, esperás:']\n",
      "357/63: encuesta['Del curso, esperás:'].value_counts()\n",
      "359/1: len('TextoDeXLetras')\n",
      "359/2: len('TextoDe14Letras')\n",
      "359/3: len('TextoDe15Letras')\n",
      "360/1: 2.3\n",
      "360/2: int(\"1e10\")\n",
      "360/3: 1e10\n",
      "360/4: float(\"1e10\")\n",
      "361/1:\n",
      "a = 1.0\n",
      "b = '1'\n",
      "c = '1.1'\n",
      "361/2:\n",
      "a + float(b)\n",
      "float(b) + float(c)\n",
      "a + int(c)\n",
      "a + int(float(c))\n",
      "int(a) + int(float(c))\n",
      "2.0 * b\n",
      "361/3:\n",
      "secuencia = 'ATGcGTTGC'\n",
      "secuencias[3] = \"C\"\n",
      "361/4:\n",
      "secuencia = 'ATGcGTTGC'\n",
      "secuencia[3] = \"C\"\n",
      "361/5:\n",
      "for i in [1,2,3]:\n",
      "    print(i)\n",
      "361/6: i\n",
      "362/1: palabra = 'Murcielago'\n",
      "362/2: palabra[-3]\n",
      "362/3: lu = (\"AAA\" \"AAA\")\n",
      "362/4: lu\n",
      "362/5:\n",
      "def suma(*args):\n",
      "    return suma(args)\n",
      "362/6: suma(2,3,4)\n",
      "362/7:\n",
      "def suma(*args):\n",
      "    return args[0]\n",
      "362/8: suma(2,3,4)\n",
      "362/9:\n",
      "def suma(*args):\n",
      "    return args[2]\n",
      "362/10:\n",
      "def suma(*args):\n",
      "    return sum(args)\n",
      "362/11: suma(2,3,4)\n",
      "362/12: suma(2,3,4,10)\n",
      "367/1:\n",
      "fibo = [0,1,1,2,3,5,8,13,21,34]\n",
      "len(fibo)\n",
      "367/2:\n",
      "fibo = [0,1,1,2,3,5,8,13,21,34]\n",
      "print(sum(fibo))\n",
      "print(max(fibo))\n",
      "367/3:\n",
      "fibo = [0,1,1,2,3,5,8,13,21,34]\n",
      "\n",
      "print(sum(fibo))\n",
      "print(max(fibo))\n",
      "367/4:\n",
      "cuadrados = []\n",
      "for numero in fibonacci:\n",
      "    cuadrados.append( numero ** 2 )\n",
      "367/5:\n",
      "cuadrados = []\n",
      "for numero in fibo:\n",
      "    cuadrados.append( numero ** 2 )\n",
      "367/6: print(cuadrados)\n",
      "367/7:\n",
      "print(fibo)\n",
      "print(cuadrados)\n",
      "367/8:\n",
      "import pandas as pd\n",
      "\n",
      "serie = pd.Series(fibo)\n",
      "serie\n",
      "367/9:\n",
      "import pandas as pd\n",
      "\n",
      "serie = pd.Series(fibo)\n",
      "print(serie)\n",
      "367/10:\n",
      "los objetos Series se encuentras vectorizados, es decir que los métodos y las operaciones se realizan automáticamente para todos los elementos de la serie,\n",
      "por ejemplo, para calcular los cuadrados:\n",
      "367/11: fibo ** 2\n",
      "367/12: serie ** 2\n",
      "367/13: serie ** 3\n",
      "367/14: serie.sum()\n",
      "367/15: serie ** 3 / serie **2\n",
      "367/16: serie + 15\n",
      "367/17: serie ** 3\n",
      "367/18: serie + 15\n",
      "367/19: (serie + 100)\n",
      "367/20: (1 - serie)\n",
      "367/21: (1 - serie) / (serie + 100)\n",
      "367/22: abs((1 - serie) / (serie + 100) )\n",
      "367/23: ((1 - serie) / (serie + 100)).abs()\n",
      "367/24: (1 - serie) / (serie + 100)\n",
      "367/25: Además, las series muchos métodos útiles para trabajar con datos numéricos, por ej:\n",
      "367/26:\n",
      "print('suma: ', serie.sum())\n",
      "print('media: ', serie.mean())\n",
      "print('mediana: ', serie.median())\n",
      "print('SD: ', serie.sd())\n",
      "367/27:\n",
      "print('suma: ', serie.sum())\n",
      "print('media: ', serie.mean())\n",
      "print('mediana: ', serie.median())\n",
      "print('SD: ', serie.std())\n",
      "367/28:\n",
      "print('suma: ', serie.sum())\n",
      "print('media: ', serie.mean())\n",
      "print('mediana: ', serie.median())\n",
      "print('SDT: ', serie.std())\n",
      "367/29: serie[2:]\n",
      "367/30: serie[2: -3]\n",
      "367/31: serie[[1,3,4,-1]]\n",
      "367/32: serie.iloc[[1,3,4,-1]]\n",
      "367/33: serie[2:6]\n",
      "367/34: fibo\n",
      "367/35: print(fibo)\n",
      "368/1:\n",
      "\n",
      "\n",
      "l1 = [1,3,5,6,7,10,12,15,22]\n",
      "l2 = [3,4,6,7,8,9,10,11,13,15]\n",
      "def intersection( a, b) :\n",
      "    pass\n",
      "368/2:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    pass\n",
      "368/3:\n",
      "def contains_all(l1, l2):\n",
      "    pass\n",
      "368/4:\n",
      "datos = {\n",
      "    'juan' : ['Berlin','Barcelona','Buenos Aires','Baradero','Boston'],\n",
      "    'pedro' : ['Asunción','Atenas','Alicante','Almería','Berlin'],\n",
      "    'pepe' : ['Asunción','Buenos Aires','Berlin']\n",
      "}\n",
      "\n",
      "def visitantes( dic, ciudad ):\n",
      "    pass\n",
      "368/5:\n",
      "def reverseDict(dic):\n",
      "    pass\n",
      "368/6: Defina una función que tenga como parametros un a partir de un diccionario de secuencias proteicas y una secuencia de interes, que imprima en pantalla los nombres las proteinas que posean la secuencia de interes, y que devuelva la cantidad de dichas proteinas:\n",
      "368/7:\n",
      "secuencias = {\n",
      "    'bos taurus': \"MEESQAELNVEPPLSQETFSDLWNLLPENNLLSSELSAPVDDLLPYTDVATWLDECPNEA\",\n",
      "    'homo sapiens': \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGP\",\n",
      "    'mus musculus': 'MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEA',\n",
      "    'rattus norvegicus': 'MEDSQSDMSIELPLSQETFSCLWKLLPPDDILPTTATGSPNSMEDLFLPQDVAELLEGPE',\n",
      "    'macaca mulatta': 'MEEPQSDPSIEPPLSQETFSDLWKLLPENNVLSPLPSQAVDDLMLSPDDLAQWLTEDPGP',\n",
      "    'sus scrofa': 'MEESQSELGVEPPLSQETFSDLWKLLPENNLLSSELSLAAVNDLLLSPVTNWLDENPDDA',\n",
      "    \"cricetulus griseus\":'MEEPQSDLSIELPLSQETFSDLWKLLPPNNVLSTLPSSDSIEELFLSENVTGWLEDSGGA',\n",
      "    'canis lupus': \"MEESQSELNIDPPLSQETFSELWNLLPENNVLSSELCPAVDELLLPESVVNWLDEDSDDA\",\n",
      "    'felix catus'; 'MQEPPLELTIEPPLSQETFSELWNLLPENNVLSSELSSAMNELPLSEDVANWLDEAPDDA'\n",
      "    \n",
      "}\n",
      "368/8:\n",
      "secuencias = {\n",
      "    'bos taurus': \"MEESQAELNVEPPLSQETFSDLWNLLPENNLLSSELSAPVDDLLPYTDVATWLDECPNEA\",\n",
      "    'homo sapiens': \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGP\",\n",
      "    'mus musculus': 'MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEA',\n",
      "    'rattus norvegicus': 'MEDSQSDMSIELPLSQETFSCLWKLLPPDDILPTTATGSPNSMEDLFLPQDVAELLEGPE',\n",
      "    'macaca mulatta': 'MEEPQSDPSIEPPLSQETFSDLWKLLPENNVLSPLPSQAVDDLMLSPDDLAQWLTEDPGP',\n",
      "    'sus scrofa': 'MEESQSELGVEPPLSQETFSDLWKLLPENNLLSSELSLAAVNDLLLSPVTNWLDENPDDA',\n",
      "    \"cricetulus griseus\":'MEEPQSDLSIELPLSQETFSDLWKLLPPNNVLSTLPSSDSIEELFLSENVTGWLEDSGGA',\n",
      "    'canis lupus': \"MEESQSELNIDPPLSQETFSELWNLLPENNVLSSELCPAVDELLLPESVVNWLDEDSDDA\",\n",
      "    'felix catus': 'MQEPPLELTIEPPLSQETFSELWNLLPENNVLSSELSSAMNELPLSEDVANWLDEAPDDA'\n",
      "    \n",
      "}\n",
      "368/9:\n",
      "def foo(secuencias, target):\n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "            print(proteina)\n",
      "    return len(hits)\n",
      "368/10: foo(secuencias, 'MEE')\n",
      "368/11: foo(secuencias, 'LSSEL')\n",
      "368/12:\n",
      "def funcion(secuencias, target):\n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "            print(proteina)\n",
      "    return len(hits)\n",
      "368/13: funcion(secuencias, 'LSSEL')\n",
      "368/14:\n",
      "hits = funcion(secuencias, 'LSSEL')\n",
      "print(hits)\n",
      "368/15:\n",
      "secuencias = {\n",
      "    'bos taurus': \"MEESQAELNVEPPLSQETFSDLWNLLPENNLLSSELSAPVDDLLPYTDVATWLDECPNEA\",\n",
      "    'homo sapiens': \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGP\",\n",
      "    'mus musculus': 'MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEA',\n",
      "    'rattus norvegicus': 'MEDSQSDMSIELPLSQETFSCLWKLLPPDDILPTTATGSPNSMEDLFLPQDVAELLEGPE',\n",
      "    'macaca mulatta': 'MEEPQSDPSIEPPLSQETFSDLWKLLPENNVLSPLPSQAVDDLMLSPDDLAQWLTEDPGP',\n",
      "    'sus scrofa': 'MEESQSELGVEPPLSQETFSDLWKLLPENNLLSSELSLAAVNDLLLSPVTNWLDENPDDA',\n",
      "    \"cricetulus griseus\":'MEEPQSDLSIELPLSQETFSDLWKLLPPNNVLSTLPSSDSIEELFLSENVTGWLEDSGGA',\n",
      "    'canis lupus': \"MEESQSELNIDPPLSQETFSELWNLLPENNVLSSELCPAVDELLLPESVVNWLDEDSDDA\",\n",
      "    'felix catus': 'MQEPPLELTIEPPLSQETFSELWNLLPENNVLSSELSSAMNELPLSEDVANWLDEAPDDA'\n",
      "    \n",
      "}\n",
      "\n",
      "def funcion(secuencias, target):\n",
      "    pass\n",
      "368/16:\n",
      "hits = funcion(secuencias, 'LSSEL')\n",
      "print(hits)\n",
      "368/17:\n",
      "def funcion(secuencias, target):\n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "            print(proteina)\n",
      "    return len(hits)\n",
      "368/18:\n",
      "hits = funcion(secuencias, 'LSSEL')\n",
      "print(hits)\n",
      "368/19:\n",
      "def funcion2(secuencias, target):\n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "    if len(hits) > 0:\n",
      "        print(\"Se ha encontrado la secuencia\",target, \"en los siguientes organismos:\")\n",
      "        for organismo in organismos:\n",
      "            print(organismo)\n",
      "    else:\n",
      "        print(\"No se ha encontrado la secuencia de interes en las proteinas\" )\n",
      "    return len(hits)\n",
      "368/20:\n",
      "hits = funcion2('LSSEL')\n",
      "print(hits)\n",
      "368/21:\n",
      "def funcion2(secuencias=secuencias, target):\n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "    if len(hits) > 0:\n",
      "        print(\"Se ha encontrado la secuencia\",target, \"en los siguientes organismos:\")\n",
      "        for organismo in organismos:\n",
      "            print(organismo)\n",
      "    else:\n",
      "        print(\"No se ha encontrado la secuencia de interes en las proteinas\" )\n",
      "    return len(hits)\n",
      "368/22:\n",
      "def funcion2(target,secuencias=secuencias):\n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "    if len(hits) > 0:\n",
      "        print(\"Se ha encontrado la secuencia\",target, \"en los siguientes organismos:\")\n",
      "        for organismo in organismos:\n",
      "            print(organismo)\n",
      "    else:\n",
      "        print(\"No se ha encontrado la secuencia de interes en las proteinas\" )\n",
      "    return len(hits)\n",
      "368/23:\n",
      "hits = funcion2('LSSEL')\n",
      "print(hits)\n",
      "368/24:\n",
      "def funcion2(target,secuencias=secuencias):\n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "    if len(hits) > 0:\n",
      "        print(\"Se ha encontrado la secuencia\",target, \"en los siguientes organismos:\")\n",
      "        for organismo in hits:\n",
      "            print(organismo)\n",
      "    else:\n",
      "        print(\"No se ha encontrado la secuencia de interes en las proteinas\" )\n",
      "    return len(hits)\n",
      "368/25:\n",
      "hits = funcion2('LSSEL')\n",
      "print(hits)\n",
      "368/26:\n",
      "hits = funcion2('LSSELXXXXXX')\n",
      "print(hits)\n",
      "368/27:\n",
      "def reverseDict(dic):\n",
      "    reverse_dict = {}\n",
      "    for clave, valor in dic.items():\n",
      "        reverse_dict[valor] = clave\n",
      "    return reverse_dict\n",
      "368/28: reverseDict({1:'a', 2:'b', 3:'z'})\n",
      "368/29: sorted(mi_lista)\n",
      "368/30: mi_lista = [ 2, 517, 7, 11, 528, 531, 19, 536, 31, 40, 552, 50, 565, 55, 568, 571, 572, 583, 584, 588, 78, 591, 80, 82, 596, 91, 605, 611, 99, 101, 102, 105, 110, 622, 114, 118, 635, 636, 129, 643, 131, 645, 136, 655, 661, 664, 156, 669, 672, 163, 167, 168, 170, 683, 171, 176, 688, 179, 697, 187, 700, 191, 703, 197, 709, 201, 713, 716, 204, 205, 719, 720, 209, 726, 728, 729, 219, 228, 741, 231, 745, 237, 238, 240, 759, 248, 251, 254, 771, 773, 775, 778, 266, 787, 276, 789, 279, 793, 285, 798, 799, 800, 803, 295, 818, 307, 306, 821, 822, 823, 310, 315, 323, 835, 839, 843, 331, 334, 854, 342, 343, 347, 860, 363, 878, 367, 882, 883, 373, 886, 887, 376, 889, 379, 387, 900, 903, 907, 399, 400, 913, 911, 915, 409, 925, 929, 421, 933, 939, 437, 950, 951, 949, 953, 446, 959, 962, 451, 452, 453, 969, 458, 970, 972, 463, 464, 978, 982, 986, 475, 988, 474, 482, 485, 486, 509, 510]\n",
      "368/31: sorted(mi_lista)[-2]\n",
      "368/32: sorted(mi_lista)[-3]\n",
      "368/33:\n",
      "print(sorted(mi_lista)[-3])\n",
      "print(mi_lista.index(950))\n",
      "368/34:\n",
      "cuadrados = []\n",
      "numeros = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
      "for n in lista:\n",
      "    cuadrados.append(n ** 2 )\n",
      "368/35:\n",
      "cuadrados = []\n",
      "numeros = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10] \n",
      "for n in numeros:\n",
      "    cuadrados.append(n ** 2 )\n",
      "368/36: cuadrados\n",
      "370/1:\n",
      "d = {'CYS': 'C', 'ASP': 'D', 'SER': 'S', 'GLN': 'Q', 'LYS': 'K',\n",
      "     'ILE': 'I', 'PRO': 'P', 'THR': 'T', 'PHE': 'F', 'ASN': 'N', \n",
      "     'GLY': 'G', 'HIS': 'H', 'LEU': 'L', 'ARG': 'R', 'TRP': 'W', \n",
      "     'ALA': 'A', 'VAL':'V', 'GLU': 'E', 'TYR': 'Y', 'MET': 'M'}\n",
      "370/2:\n",
      "def convertir_aa(secuencia):\n",
      "    salida = \"\"\n",
      "    s = secuencia.split(\"-\")\n",
      "    for aa in s:\n",
      "        salida = salida + d[s]\n",
      "    return salida\n",
      "370/3: convertir_aa(\"STLE\")\n",
      "370/4:\n",
      "def convertir_aa(secuencia):\n",
      "    salida = \"\"\n",
      "    s = secuencia.split(\"-\")\n",
      "    for aa in s:\n",
      "        salida = salida + d[s]\n",
      "    return salida\n",
      "370/5: convertir_aa(\"SER-THR-LEU-GLU\")\n",
      "370/6:\n",
      "def convertir_aa(secuencia):\n",
      "    salida = \"\"\n",
      "    s = secuencia.split(\"-\")\n",
      "    for aa in s:\n",
      "        salida = salida + d[aa]\n",
      "    return salida\n",
      "370/7: convertir_aa(\"SER-THR-LEU-GLU\")\n",
      "370/8: convertir_aa(\"SER-THR-LEU-GLU-LYS-ASN\")\n",
      "370/9: convertir_aa(\"SER-THR-LEU-GLU-LYS-ASN\")\n",
      "370/10: \"Ser-Thr-Leu-Glu-Lys-Asn\"\n",
      "377/1:\n",
      "def calcular_ep(m, g, h):    # Este es el encabezado de la función\n",
      "    ep =  m * g * h          # Este es el cuerpo de la función\n",
      "    return ep                # Lo que la funcion devuelve o retorna\n",
      "\n",
      "\n",
      "print(\"La Energía Potencial de un cuerpo de masa \", m, 'a una altura ',h ,'es de: ' calcular_ep, \"J\")\n",
      "377/2:\n",
      "def calcular_ep(m, g, h):    # Este es el encabezado de la función\n",
      "    ep =  m * g * h          # Este es el cuerpo de la función\n",
      "    return ep                # Lo que la funcion devuelve o retorna\n",
      "\n",
      "\n",
      "print(\"La Energía Potencial de un cuerpo de masa 2.5 Kg a 10 metros de altura es de: \", calcular_ep(2.5, 9.8, 10), \"J\")\n",
      "377/3:\n",
      "def calcular_ep(m, h):# Este es el encabezado de la función\n",
      "    g = 9.8\n",
      "    ep =  m * g * h          # Este es el cuerpo de la función\n",
      "    return ep                # Lo que la funcion devuelve o retorna\n",
      "\n",
      "\n",
      "print(\"La Energía Potencial de un cuerpo de masa 2.5 Kg a 10 metros de altura es de: \", calcular_ep(2.5, 10), \"J\")\n",
      "377/4:\n",
      "def calcular_ep(m, g, h):    # Este es el encabezado de la función\n",
      "    ep =  m * g * h          # Este es el cuerpo de la función\n",
      "    return ep                # Lo que la funcion devuelve o retorna\n",
      "377/5:\n",
      "def calcular_ep(m, g, h):# Este es el encabezado de la función\n",
      "    \"\"\"Calcula la energia potencial de un cuerpo segun su altura y masa\"\"\"\n",
      "    ep =  m * g * h          # Este es el cuerpo de la función\n",
      "    return ep                # Lo que la funcion devuelve o retorna\n",
      "370/11:\n",
      "def es_vocal(letra):\n",
      "    if letra in 'AEIOUaeiou':\n",
      "        print(letra, \"es una vocal\")\n",
      "    else:\n",
      "        print(letra, \"no es una vocal\")\n",
      "370/12: es_vocal(\"A\")\n",
      "370/13: es_vocal(\"U\")\n",
      "370/14: es_vocal(\"u\")\n",
      "370/15: es_vocal(\"e\")\n",
      "370/16: es_vocal(\"s\")\n",
      "370/17: es_vocal(\"j\")\n",
      "370/18:\n",
      "def es_vocal(letra):\n",
      "    if letra in 'AEIOUaeiou':\n",
      "        print(letra, \"es una vocal\")\n",
      "        return 'VOCAL'\n",
      "    else:\n",
      "        print(letra, \"no es una vocal\")\n",
      "        return 'NO VOCAL'\n",
      "370/19: es_vocal(\"j\")\n",
      "373/1: {'a':1, 'b':3}\n",
      "373/2: a = {'a':1, 'b':3}\n",
      "373/3: a.delitem(\"b\")\n",
      "373/4: del a[\"b\"]\n",
      "373/5: a\n",
      "362/13: %history\n",
      "   1:\n",
      "l1 = [1,3,5,6,7,10,12,15,22]\n",
      "l2 = [3,4,6,7,8,9,10,11,13,15]\n",
      "def intersection( a, b) :\n",
      "    intersec = []\n",
      "    for item in l1:\n",
      "        if item in l2:\n",
      "            intersec.append(item)\n",
      "   2: intersection(l1,l2)\n",
      "   3:\n",
      "l1 = [1,3,5,6,7,10,12,15,22]\n",
      "l2 = [3,4,6,7,8,9,10,11,13,15]\n",
      "def intersection( a, b) :\n",
      "    intersec = []\n",
      "    for item in l1:\n",
      "        if item in l2:\n",
      "            intersec.append(item)\n",
      "    return intersec\n",
      "   4: intersection(l1,l2)\n",
      "   5:\n",
      "l1 = [1,3,5,6,7,10,12,15,22]\n",
      "l2 = [3,4,6,7,8,9,22,10,11,13,15]\n",
      "def intersection( a, b) :\n",
      "    intersec = []\n",
      "    for item in l1:\n",
      "        if item in l2:\n",
      "            intersec.append(item)\n",
      "    return intersec\n",
      "   6: intersection(l1,l2)\n",
      "   7:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = l1 == sorted(l2)\n",
      "    return es_reversa\n",
      "   8: isReverse(l1,l2)\n",
      "   9: isReverse(l1,l2 + [2])\n",
      "  10: isReverse(l1,l2 + [3])\n",
      "  11: l2 + [3]\n",
      "  12: l2 + [3] == l1\n",
      "  13:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = b == sorted(a)\n",
      "    return es_reversa\n",
      "  14: l2 + [3] == l1\n",
      "  15: isReverse(l1,l2 + [3])\n",
      "  16: isReverse(l1,l2)\n",
      "  17:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = b == sorted(a)\n",
      "    print(a)\n",
      "    print(b)\n",
      "    return es_reversa\n",
      "  18: isReverse(l1,l2)\n",
      "  19:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = b == sorted(a)\n",
      "    print(a)\n",
      "    print(sorted(b))\n",
      "    return es_reversa\n",
      "  20: isReverse(l1,l2)\n",
      "  21:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = b == list(sorted(a))\n",
      "    print(a)\n",
      "    print(sorted(b))\n",
      "    return es_reversa\n",
      "  22: isReverse(l1,l2)\n",
      "  23:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = b == list(sorted(a))\n",
      "    print(a)\n",
      "    print(a)\n",
      "    return es_reversa\n",
      "  24: isReverse(l1,l2)\n",
      "  25: [1, 2, 3] == [1, 2, 3]\n",
      "  26: [1, 2, 3] == reversed([1, 2, 3])\n",
      "  27: [1, 2, 3] == reversed([3, 2, 1])\n",
      "  28: [1, 2, 3] == list(reversed([3, 2, 1]))\n",
      "  29:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = b == list(reversed(a))\n",
      "    print(a)\n",
      "    print(a)\n",
      "    return es_reversa\n",
      "  30: isReverse(l1,l2)\n",
      "  31: isReverse(l1,l2)\n",
      "  32: isReverse(l1,l2)\n",
      "  33:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = b == list(reversed(a))\n",
      "\n",
      "    return es_reversa\n",
      "  34: isReverse(l1,l2)\n",
      "  35: isReverse(l1,l2 + [4])\n",
      "  36: isReverse(l1.insert(0, 4),l2 + [4])\n",
      "  37: isReverse([4] l1,l2 + [4])\n",
      "  38: isReverse([4] + l1,l2 + [4])\n",
      "  39: [4] + l1\n",
      "  40:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = b == list(reversed(a))\n",
      "\n",
      "    return es_reversa\n",
      "  41: [4] + l1\n",
      "  42: isReverse([4] + l1,l2 + [4])\n",
      "  43: b.reverse()\n",
      "  44:\n",
      "l1 = [1,2,3]\n",
      "l2 = [3,2,1]\n",
      "\n",
      "def isReverse(a, b) :\n",
      "    es_reversa = b == list(reversed(a))\n",
      "    return es_reversa\n",
      "  45: reversa = []\n",
      "  46:\n",
      "for item in li:\n",
      "    reverse.insert(0, item)\n",
      "  47:\n",
      "for item in l1:\n",
      "    reverse.insert(0, item)\n",
      "  48:\n",
      "for item in l1:\n",
      "    reversa.insert(0, item)\n",
      "  49: reversa\n",
      "  50:\n",
      "for item in range(10):\n",
      "    reversa.insert(0, item)\n",
      "  51: reversa\n",
      "  52: reversa = []\n",
      "  53:\n",
      "for item in range(10):\n",
      "    reversa.insert(0, item)\n",
      "  54: reversa\n",
      "  55:\n",
      "def isReverse(a, b) :\n",
      "    reversa = []\n",
      "    for item in a:\n",
      "        reversa.insert(0, item)\n",
      "    return reversa == item\n",
      "  56: isReverse(l1,l2)\n",
      "  57:\n",
      "def isReverse(a, b) :\n",
      "    reversa = []\n",
      "    for item in a:\n",
      "        reversa.insert(0, item)\n",
      "    print(reversa)\n",
      "    return reversa == item\n",
      "  58: isReverse(l1,l2)\n",
      "  59:\n",
      "def isReverse(a, b) :\n",
      "    reversa = []\n",
      "    for item in a:\n",
      "        reversa.insert(0, item)\n",
      "    print(reversa)\n",
      "    return reversa == b\n",
      "  60: isReverse(l1,l2)\n",
      "  61: isReverse(l1,l2 + [4])\n",
      "  62: isReverse([4] + l1,l2 + [4])\n",
      "  63:\n",
      "def isReverse(a, b) :\n",
      "    reversa = []\n",
      "    for item in a:\n",
      "        reversa.insert(0, item)\n",
      "    print(reversa)\n",
      "    print(b)\n",
      "    return reversa == b\n",
      "  64: isReverse([4] + l1,l2 + [4])\n",
      "  65:\n",
      "def isReverse(a, b) :\n",
      "    reversa = []\n",
      "    for item in a:\n",
      "        reversa.insert(0, item)\n",
      "    print(a)\n",
      "    print(b)\n",
      "    return reversa == b\n",
      "  66: isReverse([4] + l1,l2 + [4])\n",
      "  67:\n",
      "def isReverse(a, b) :\n",
      "    a_reversa = []\n",
      "    for item in a:\n",
      "        a_reversa.insert(0, item)\n",
      "    print(a)\n",
      "    print(b)\n",
      "    return a_reversa == b\n",
      "  68: isReverse([4] + l1,l2 + [4])\n",
      "  69: isReverse([2,3,4,5], [5,4])\n",
      "  70: isReverse([2,3,4,5], [5,4,3])\n",
      "  71: isReverse([2,3,4,5], [5,4,3,2])\n",
      "  72: isReverse([2,3,4,5], [5,4,3,2, 1])\n",
      "  73: isReverse([1,2,3,4,5], [5,4,3,2, 1])\n",
      "  74:\n",
      "def contains_all(l1, l2):\n",
      "    is_in = 0\n",
      "    for elemento in l1:\n",
      "        if elemento in l2:\n",
      "            is_in +=1\n",
      "    return is_in == len(l1)\n",
      "  75: contains_all([2,3], [2,3])\n",
      "  76: contains_all([2,3], [2,3,4])\n",
      "  77: contains_all([2,3,6], [2,3,4])\n",
      "  78: contains_all([2,3,4], [2,3,4])\n",
      "  79: contains_all([2,3,4,4], [2,3,4])\n",
      "  80: contains_all([2,3,4,9], [2,3,4])\n",
      "  81: contains_all([2,3,4,9], [2,3,4])\n",
      "  82: contains_all([2,3,4,9,'a'], [2,3,4])\n",
      "  83: contains_all([2,3,4,9,'a'], [2,3,4,'a'])\n",
      "  84: contains_all([2,3,4,9,'a'], [2,3,4,'9'])\n",
      "  85: contains_all([2,3,4,9,'a'], [2,3,4,'a',9])\n",
      "  86:\n",
      "datos = {\n",
      "    'juan' : ['Berlin','Barcelona','Buenos Aires','Baradero','Boston'],\n",
      "    'pedro' : ['Asunción','Atenas','Alicante','Almería','Berlin'],\n",
      "    'pepe' : ['Asunción','Buenos Aires','Berlin']\n",
      "}\n",
      "\n",
      "def visitantes( dic, ciudad ):\n",
      "    visitantes = []\n",
      "    for nombre in dic:\n",
      "        if ciudad in dic[nombre]:\n",
      "            visitantes.append(nombre)\n",
      "    return visitantes\n",
      "  87: vistitanes(datos, 'Madrid')\n",
      "  88: visitanes(datos, 'Madrid')\n",
      "  89: visitantes(datos, 'Madrid')\n",
      "  90: visitantes(datos, 'Berlin')\n",
      "  91: visitantes(datos, 'Atenas')\n",
      "  92: visitantes(datos, 'Barcelona')\n",
      "  93: visitantes(datos, 'Asuncion')\n",
      "  94: visitantes(datos, 'Asunción')\n",
      "  95:\n",
      "datos = {\n",
      "    'juan' : ['Berlin','Barcelona','Buenos Aires','Baradero','Boston', 'Asunción'],\n",
      "    'pedro' : ['Asunción','Atenas','Alicante','Almería','Berlin'],\n",
      "    'pepe' : ['Asunción','Buenos Aires','Berlin']\n",
      "}\n",
      "\n",
      "def visitantes( dic, ciudad ):\n",
      "    visitantes = []\n",
      "    for nombre in dic:\n",
      "        if ciudad in dic[nombre]:\n",
      "            visitantes.append(nombre)\n",
      "    return visitantes\n",
      "  96: visitantes(datos, 'Asunción')\n",
      "  97:\n",
      "datos = {\n",
      "    'juan' : ['Berlin','Barcelona','Buenos Aires','Baradero','Boston', 'Asunción'],\n",
      "    'pedro' : ['Atenas','Alicante','Almería','Berlin'],\n",
      "    'pepe' : ['Asunción','Buenos Aires','Berlin']\n",
      "}\n",
      "\n",
      "def visitantes( dic, ciudad ):\n",
      "    visitantes = []\n",
      "    for nombre in dic:\n",
      "        if ciudad in dic[nombre]:\n",
      "            visitantes.append(nombre)\n",
      "    return visitantes\n",
      "  98: visitantes(datos, 'Asunción')\n",
      "  99:\n",
      "datos = {\n",
      "    'juan' : ['Berlin','Barcelona','Buenos Aires','Baradero','Boston'],\n",
      "    'pedro' : ['Asunción','Atenas','Alicante','Almería','Berlin'],\n",
      "    'pepe' : ['Asunción','Buenos Aires','Berlin']\n",
      "}\n",
      "\n",
      "def visitantes( dic, ciudad ):\n",
      "    visitantes = []\n",
      "    for nombre in dic:\n",
      "        if ciudad in dic[nombre]:\n",
      "            visitantes.append(nombre)\n",
      "    return visitantes\n",
      " 100: visitantes(datos, 'Asunción')\n",
      " 101:\n",
      "def reverseDict(dic):\n",
      "    reverse_dict = {}\n",
      "    for clave, valor in dic.items():\n",
      "        reverse_dict[valor] = clave\n",
      "    return reverse_dict\n",
      "\n",
      "\n",
      "def reverseDict(dic):\n",
      "    reverse_dict = {}\n",
      "    for clave in dic:\n",
      "        reverse_dict[ dic[clave] ] = clave\n",
      "    return reverse_dict\n",
      " 102:\n",
      "def reverseDict(dic):\n",
      "    reverse_dict = {}\n",
      "    for clave, valor in dic.items():\n",
      "        reverse_dict[valor] = clave\n",
      "    return reverse_dict\n",
      "\n",
      "\n",
      "def reverseDict2(dic):\n",
      "    reverse_dict = {}\n",
      "    for clave in dic:\n",
      "        reverse_dict[ dic[clave] ] = clave\n",
      "    return reverse_dict\n",
      " 103: reverseDict({1:'a', 2:'b', 3:'z'})\n",
      " 104:\n",
      "reverseDict({1:'a', 2:'b', 3:'z'})\n",
      "reverseDict2({1:'a', 2:'b', 3:'z'})\n",
      " 105:\n",
      "print(reverseDict({1:'a', 2:'b', 3:'z'}))\n",
      "reverseDict2({1:'a', 2:'b', 3:'z'})\n",
      " 106:\n",
      "def funcion2(target,sequencias = 'default'):\n",
      "    \n",
      "    if secuencias = 'default':\n",
      "        secuencias = {\n",
      "        'bos taurus': \"MEESQAELNVEPPLSQETFSDLWNLLPENNLLSSELSAPVDDLLPYTDVATWLDECPNEA\",\n",
      "        'homo sapiens': \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGP\",\n",
      "        'mus musculus': 'MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEA',\n",
      "        'rattus norvegicus': 'MEDSQSDMSIELPLSQETFSCLWKLLPPDDILPTTATGSPNSMEDLFLPQDVAELLEGPE',\n",
      "        'macaca mulatta': 'MEEPQSDPSIEPPLSQETFSDLWKLLPENNVLSPLPSQAVDDLMLSPDDLAQWLTEDPGP',\n",
      "        'sus scrofa': 'MEESQSELGVEPPLSQETFSDLWKLLPENNLLSSELSLAAVNDLLLSPVTNWLDENPDDA',\n",
      "        \"cricetulus griseus\":'MEEPQSDLSIELPLSQETFSDLWKLLPPNNVLSTLPSSDSIEELFLSENVTGWLEDSGGA',\n",
      "        'canis lupus': \"MEESQSELNIDPPLSQETFSELWNLLPENNVLSSELCPAVDELLLPESVVNWLDEDSDDA\",\n",
      "        'felix catus': 'MQEPPLELTIEPPLSQETFSELWNLLPENNVLSSELSSAMNELPLSEDVANWLDEAPDDA'\n",
      "        }\n",
      "    \n",
      "    \n",
      "    \n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "    if len(hits) > 0:\n",
      "        print(\"Se ha encontrado la secuencia\",target, \"en los siguientes organismos:\")\n",
      "        for organismo in hits:\n",
      "            print(organismo)\n",
      "    else:\n",
      "        print(\"No se ha encontrado la secuencia de interes en las proteinas\" )\n",
      "    return len(hits)\n",
      " 107:\n",
      "def funcion2(target,sequencias = 'default'):\n",
      "    \n",
      "    if secuencias == 'default':\n",
      "        secuencias = {\n",
      "        'bos taurus': \"MEESQAELNVEPPLSQETFSDLWNLLPENNLLSSELSAPVDDLLPYTDVATWLDECPNEA\",\n",
      "        'homo sapiens': \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGP\",\n",
      "        'mus musculus': 'MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEA',\n",
      "        'rattus norvegicus': 'MEDSQSDMSIELPLSQETFSCLWKLLPPDDILPTTATGSPNSMEDLFLPQDVAELLEGPE',\n",
      "        'macaca mulatta': 'MEEPQSDPSIEPPLSQETFSDLWKLLPENNVLSPLPSQAVDDLMLSPDDLAQWLTEDPGP',\n",
      "        'sus scrofa': 'MEESQSELGVEPPLSQETFSDLWKLLPENNLLSSELSLAAVNDLLLSPVTNWLDENPDDA',\n",
      "        \"cricetulus griseus\":'MEEPQSDLSIELPLSQETFSDLWKLLPPNNVLSTLPSSDSIEELFLSENVTGWLEDSGGA',\n",
      "        'canis lupus': \"MEESQSELNIDPPLSQETFSELWNLLPENNVLSSELCPAVDELLLPESVVNWLDEDSDDA\",\n",
      "        'felix catus': 'MQEPPLELTIEPPLSQETFSELWNLLPENNVLSSELSSAMNELPLSEDVANWLDEAPDDA'\n",
      "        }\n",
      "    \n",
      "    \n",
      "    \n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "    if len(hits) > 0:\n",
      "        print(\"Se ha encontrado la secuencia\",target, \"en los siguientes organismos:\")\n",
      "        for organismo in hits:\n",
      "            print(organismo)\n",
      "    else:\n",
      "        print(\"No se ha encontrado la secuencia de interes en las proteinas\" )\n",
      "    return len(hits)\n",
      " 108:\n",
      "hits = funcion2('LSSELXXXXXX')\n",
      "print(hits)\n",
      " 109:\n",
      "def funcion2(target, secuencias = 'default'):\n",
      "    \n",
      "    if secuencias == 'default':\n",
      "        secuencias = {\n",
      "        'bos taurus': \"MEESQAELNVEPPLSQETFSDLWNLLPENNLLSSELSAPVDDLLPYTDVATWLDECPNEA\",\n",
      "        'homo sapiens': \"MEEPQSDPSVEPPLSQETFSDLWKLLPENNVLSPLPSQAMDDLMLSPDDIEQWFTEDPGP\",\n",
      "        'mus musculus': 'MTAMEESQSDISLELPLSQETFSGLWKLLPPEDILPSPHCMDDLLLPQDVEEFFEGPSEA',\n",
      "        'rattus norvegicus': 'MEDSQSDMSIELPLSQETFSCLWKLLPPDDILPTTATGSPNSMEDLFLPQDVAELLEGPE',\n",
      "        'macaca mulatta': 'MEEPQSDPSIEPPLSQETFSDLWKLLPENNVLSPLPSQAVDDLMLSPDDLAQWLTEDPGP',\n",
      "        'sus scrofa': 'MEESQSELGVEPPLSQETFSDLWKLLPENNLLSSELSLAAVNDLLLSPVTNWLDENPDDA',\n",
      "        \"cricetulus griseus\":'MEEPQSDLSIELPLSQETFSDLWKLLPPNNVLSTLPSSDSIEELFLSENVTGWLEDSGGA',\n",
      "        'canis lupus': \"MEESQSELNIDPPLSQETFSELWNLLPENNVLSSELCPAVDELLLPESVVNWLDEDSDDA\",\n",
      "        'felix catus': 'MQEPPLELTIEPPLSQETFSELWNLLPENNVLSSELSSAMNELPLSEDVANWLDEAPDDA'\n",
      "        }\n",
      "    \n",
      "    \n",
      "    \n",
      "    hits = []\n",
      "    for proteina in secuencias:\n",
      "        if target in secuencias[proteina]:\n",
      "            hits.append(proteina)\n",
      "    if len(hits) > 0:\n",
      "        print(\"Se ha encontrado la secuencia\",target, \"en los siguientes organismos:\")\n",
      "        for organismo in hits:\n",
      "            print(organismo)\n",
      "    else:\n",
      "        print(\"No se ha encontrado la secuencia de interes en las proteinas\" )\n",
      "    return len(hits)\n",
      " 110:\n",
      "hits = funcion2('LSSELXXXXXX')\n",
      "print(hits)\n",
      " 111:\n",
      "hits = funcion2('LSSEL')\n",
      "print(hits)\n",
      " 112:\n",
      "def mayor(a,b):\n",
      "    if a > b:\n",
      "        return a\n",
      "    if b > a:\n",
      "        return b\n",
      "    if a == b:\n",
      "        return 'Los dos son iguales'\n",
      "    if a = 0 and b == 0:\n",
      "        return 'Los dos son 0'\n",
      " 113:\n",
      "def mayor(a,b):\n",
      "    if a > b:\n",
      "        return a\n",
      "    if b > a:\n",
      "        return b\n",
      "    if a == b:\n",
      "        return 'Los dos son iguales'\n",
      "    if a == 0 and b == 0:\n",
      "        return 'Los dos son 0'\n",
      " 114: mayor(10,3)\n",
      " 115: mayor(10,10)\n",
      " 116: mayor(2,10)\n",
      " 117: mayor(2,1)\n",
      " 118: mayor(2,2)\n",
      " 119: mayor(0,0)\n",
      " 120:\n",
      "def mayor(a,b):\n",
      "    if a > b:\n",
      "        return a\n",
      "    if b > a:\n",
      "        return b\n",
      "    if a == 0 and b == 0:\n",
      "        return 'Los dos son 0'\n",
      "    if a == b:\n",
      "        return 'Los dos son iguales'\n",
      " 121: mayor(0,0)\n",
      " 122: range(-10)\n",
      " 123: list(ange(-10))\n",
      " 124: list(range(-10))\n",
      " 125: help(range)\n",
      " 126: range(-10,0)\n",
      " 127: list(range(-10,0))\n",
      " 128: range(10, 0)\n",
      " 129: list(range(10, 0))\n",
      " 130: list(range(10, 0, 1))\n",
      " 131: list(range(10, 0, -1))\n",
      " 132: \"a\".upper()\n",
      "380/1: import pandas as pd\n",
      "380/2: covid = pd.read_csv('https://sisa.msal.gov.ar/datos/descargas/covid-19/files/Covid19Determinaciones.csv')\n",
      "380/3: covid\n",
      "380/4: covid.info()\n",
      "380/5: covid.loc[0]\n",
      "380/6: covid[['fecha', 'provincia','total','positivos']]\n",
      "380/7: covid[['fecha', 'provincia','total','positivos']].tail(20)\n",
      "380/8: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-02\"')\n",
      "380/9: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-02\"').groupby('provincia').sum()\n",
      "380/10: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-01\"').groupby('provincia').sum()\n",
      "380/11: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-01\"')#.groupby('provincia').sum()\n",
      "380/12: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-01\"').query('provincia == \"buenos aires\"')#.groupby('provincia').sum()\n",
      "380/13: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-01\"')#.query('provincia == \"buenos aires\"')#.groupby('provincia').sum()\n",
      "380/14: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-01\"')#.query('provincia == \"Buenos Aires\"')#.groupby('provincia').sum()\n",
      "380/15: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-01\"').query('provincia == \"Buenos Aires\"')#.groupby('provincia').sum()\n",
      "380/16: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-01\"').query('provincia == \"Buenos Aires\"').positivos.sum()#.groupby('provincia').sum()\n",
      "380/17: covid[['fecha', 'provincia','total','positivos']].query('fecha == \"2021-03-02\"').query('provincia == \"Buenos Aires\"').positivos.sum()#.groupby('provincia').sum()\n",
      "380/18: covid = pd.read_csv('https://sisa.msal.gov.ar/datos/descargas/covid-19/files/Covid19Casos.csv')\n",
      "380/19: covid\n",
      "380/20: covid.loc[0]\n",
      "380/21: covid.clasificacion_resumen.value_counts()\n",
      "380/22: covid.query('residencia_provincia_nombre == \"Buenos Aires\"')\n",
      "380/23: covid.query('residencia_provincia_nombre == \"Buenos Aires\"').residencia_departamento.nombre.value_counts()\n",
      "380/24: covid.query('residencia_provincia_nombre == \"Buenos Aires\"').residencia_departamento_nombre.value_counts()\n",
      "380/25: covid.query('residencia_provincia_nombre == \"Buenos Aires\"').query('residencia_departamento_nombre == \"General San Martin\"')\n",
      "380/26: covid.query('residencia_provincia_nombre == \"Buenos Aires\"').query('residencia_departamento_nombre == \"San Martin\"')\n",
      "380/27: covid.query('residencia_provincia_nombre == \"Buenos Aires\"').residencia_departamento_nombre.value_counts()\n",
      "380/28: covid.query('residencia_provincia_nombre == \"Buenos Aires\"').residencia_departamento_nombre.value_counts().head()\n",
      "380/29: covid.query('residencia_provincia_nombre == \"Buenos Aires\"').residencia_departamento_nombre.value_counts().head(10)\n",
      "380/30: covid.query('residencia_provincia_nombre == \"Buenos Aires\"').query('residencia_departamento_nombre == \"General San Martín\"')\n",
      "380/31: covid_san_martin = covid.query('residencia_provincia_nombre == \"Buenos Aires\"').query('residencia_departamento_nombre == \"General San Martín\"')\n",
      "380/32: covid_san_martin\n",
      "380/33: covid_san_martin.columns\n",
      "380/34: covid_san_martin.loc\n",
      "380/35: covid_san_martin.loc[0]\n",
      "380/36: covid_san_martin.iloc[0]\n",
      "380/37: covid_san_martin.sexo.value_counts()\n",
      "380/38: covid_san_martin\n",
      "380/39: covid_san_martin.carga_provincia_nombre.value_counts()\n",
      "380/40: covid_san_martin#.carga_provincia_nombre.value_counts()\n",
      "380/41: covid_san_martin.residencia_provincia_nombre.value_counts()\n",
      "380/42: columnas = ['edad', 'sexo', 'fecha_inicio_sintomas', ]\n",
      "380/43: covid_san_martin\n",
      "380/44: covid.groupby('residencia_provincia_nombre')\n",
      "380/45: covid.groupby('residencia_provincia_nombre').agg.sum()\n",
      "380/46: covid.groupby('residencia_provincia_nombre').sum()\n",
      "380/47: covid.pivot_table(index='residencia_provincia_nombre', columns='clasificacion_resumen')\n",
      "380/48: covid.pivot_table(index='residencia_provincia_nombre', columns='clasificacion_resumen', aggfunc='sum')\n",
      "380/49: covid.pivot_table(index='residencia_provincia_nombre', columns='clasificacion_resumen', aggfunc='size')\n",
      "380/50: covid.pivot_table(index='residencia_provincia_nombre', columns='clasificacion_resumen', aggfunc='size').Confirmado.sum()\n",
      "380/51: pivot = covid.pivot_table(index='residencia_provincia_nombre', columns='clasificacion_resumen', aggfunc='size')\n",
      "380/52: pivot\n",
      "380/53: pivot.sum()\n",
      "380/54: pivot.sum(1)\n",
      "380/55: pivot['test'] = pivot.sum(1)\n",
      "380/56: pivot\n",
      "380/57: pivot = covid.pivot_table(index='residencia_provincia_nombre', columns='clasificacion_resumen', aggfunc='size')\n",
      "380/58:\n",
      "del pivot['Sospechoso']\n",
      "pivot['test'] = pivot.sum(1)\n",
      "380/59: pivot\n",
      "380/60: pivot[\"positividad\"] = pivot['Confirmado'] / pivot['test']\n",
      "380/61: pivot\n",
      "380/62: pd.read_csv('https://www.worldometers.info/coronavirus/#countries')\n",
      "380/63: web = pd.read_html('https://www.worldometers.info/coronavirus/#countries')\n",
      "380/64: web = pd.read_html('https://www.worldometers.info/coronavirus/')\n",
      "380/65: web = pd.read_clipboard()\n",
      "380/66: data = pd.read_clipboard()\n",
      "380/67: data\n",
      "380/68: data = pd.read_clipboard()\n",
      "380/69: data.columns = ['pais','total_casos', 'total_fallecidos', 'recuperados', 'casos_activos', 'condicion_crítica', 'tests_realizados', 'poblacion']\n",
      "380/70: data\n",
      "380/71: mercorsur = ['Argentina','Brasil','Paraguay', 'Uruguay']\n",
      "380/72: data.set_index('pais')\n",
      "380/73: data[mercosur]\n",
      "380/74: mercosur = ['Argentina','Brasil','Paraguay', 'Uruguay']\n",
      "380/75: data[mercosur]\n",
      "380/76: data[mercosur,:]\n",
      "380/77: data.loc[mercosur,:]\n",
      "380/78: data.index\n",
      "380/79: data.get_index()\n",
      "380/80: data.index()\n",
      "380/81: list(data.index)\n",
      "380/82: data.set_index('pais', inplace=True)\n",
      "380/83: data.loc[mercosur,:]\n",
      "380/84: data\n",
      "380/85: data.info()\n",
      "380/86: data.shape\n",
      "380/87: data.info()\n",
      "380/88: data\n",
      "380/89: data.loc['Argentina']\n",
      "380/90: data.loc['Argentina', 'total_casos']\n",
      "380/91: data.columns = ['pais','casos_totales', 'fallecidos', 'recuperados', 'casos_activos', 'condicion_crítica', 'tests_realizados', 'poblacion']\n",
      "380/92: data = pd.read_clipboard()\n",
      "380/93: data.columns = ['pais','casos_totales', 'fallecidos', 'recuperados', 'casos_activos', 'condicion_crítica', 'tests_realizados', 'poblacion']\n",
      "380/94: data.set_index('pais', inplace=True)\n",
      "380/95: data.info()\n",
      "380/96: data.loc['Argentina', 'casos_totales']\n",
      "380/97: data\n",
      "380/98: data[['Argentina','Uruguay']]\n",
      "380/99: data['Argentina']\n",
      "380/100: data\n",
      "380/101: data.keys()\n",
      "380/102: data.loc['Argentina']\n",
      "380/103: data.loc[['Argentina','Uruguay']]\n",
      "380/104: data.loc[['Argentina','Uruguay','Brasil']]\n",
      "380/105: data.loc[['Argentina','Uruguay','Brasil','Paraguay']]\n",
      "380/106: data[mercosur]\n",
      "380/107: data.loc[mercosur]\n",
      "380/108: data.loc[mercosur , 'casos_totales', 'fallecidos', 'poblacion']\n",
      "380/109: covid = data\n",
      "380/110: covid\n",
      "380/111: covid.head()\n",
      "380/112: covid.head(inplace=True)\n",
      "380/113: covid = covid.head(10)\n",
      "380/114: covid\n",
      "380/115: covid = covid.head(5)\n",
      "380/116: covid\n",
      "380/117: data\n",
      "380/118: covid[\"casos_totales\"] = 0\n",
      "380/119: data\n",
      "380/120: data = pd.read_clipboard()\n",
      "380/121: data\n",
      "380/122: data.columns = ['pais','casos_totales', 'fallecidos', 'recuperados', 'casos_activos', 'condicion_crítica', 'tests_realizados', 'poblacion']\n",
      "380/123: data.set_index('pais', inplace=True)\n",
      "380/124:\n",
      "!ls\n",
      "#data.to_csv('covid_la.csv')\n",
      "380/125: data.to_csv('covid_la.csv')\n",
      "380/126: dir(list(data.index))\n",
      "380/127: list(data.index).__len__\n",
      "380/128: list(list(data.index).__len__)\n",
      "380/129: str(list(data.index).__len__)\n",
      "380/130: dir({})\n",
      "380/131: data.loc[~['Argentina','Uruguay','Brasil','Paraguay']]\n",
      "380/132: data.loc[-['Argentina','Uruguay','Brasil','Paraguay']]\n",
      "380/133: data.loc[[-'Argentina','Uruguay','Brasil','Paraguay']]\n",
      "380/134: data.loc[['Argentina','Uruguay','Brasil','Paraguay']]\n",
      "380/135: data.loc[mercosur , ['casos_totales', 'fallecidos', 'poblacion']]\n",
      "380/136: data.drop(['Argentina','Brasil'])\n",
      "380/137: data.drop(['Argentina','Brasil'], columns='condicion_critica')\n",
      "380/138: data.drop(['Argentina','Brasil'], columns='condicion_crítica')\n",
      "380/139: data.columns = ['pais','casos_totales', 'fallecidos', 'recuperados', 'casos_activos', 'condicion_critica', 'tests_realizados', 'poblacion']\n",
      "380/140: data = pd.read_clipboard()\n",
      "380/141: data.columns = ['pais','casos_totales', 'fallecidos', 'recuperados', 'casos_activos', 'condicion_critica', 'tests_realizados', 'poblacion']\n",
      "380/142: data.set_index('pais', inplace=True)\n",
      "380/143: data.to_csv('covid_la.csv')\n",
      "380/144: data.drop(['Argentina','Brasil'], columns='condicion_critica')\n",
      "380/145: data.drop(['Argentina','Brasil'], columns=['condicion_critica'])\n",
      "380/146: data.drop(['Argentina','Brasil']).drop(columns='condicion_critica')\n",
      "380/147: covid = data.copy()\n",
      "380/148: covid\n",
      "380/149: covid.loc[[pais for pais in covid.index if 'u' in pais]]\n",
      "380/150: covid.loc[[pais for pais in covid.index if 'u' in pais]]\n",
      "380/151: covid[covid.index.str.contains('u')]\n",
      "380/152: covid.loc[covid.index.str.contains('u')]\n",
      "380/153: covid.loc[covid.index.str.contains('P')]\n",
      "380/154: covid.loc[covid.index.str.icontains('P')]\n",
      "380/155: covid.loc[covid.index.str.contains('P')]\n",
      "380/156: covid.index.str.contains('P')\n",
      "380/157:\n",
      "covid.loc[[False, False, False,  True, False, False, False,  True, False,\n",
      "       False, False, False, False, False]]\n",
      "380/158:\n",
      "covid.loc[[False, False, False,  True, False, False, False,  True, False,\n",
      "       False, False, False, False, False], [True, False, False,  True, False, False, False]]\n",
      "380/159:\n",
      "covid.loc[[False, False, False,  True, False, False, False,  True, False,\n",
      "       False, False, False, False, False], [True, True, False,  True, False, False, False]]\n",
      "380/160:\n",
      "covid.loc[[False, False, False,  True, False, False, False,  True, False,\n",
      "       False, False, False, False, False], [True, True, True,  True, False, False, False]]\n",
      "380/161:\n",
      "covid.loc[[False, False, False,  True, False, False, False,  True, False,\n",
      "       False, False, False, False, False], [True, True, True,  False, False, False, False]]\n",
      "380/162:\n",
      "covid.loc[[False, False, False,  True, False, False, False,  True, False,\n",
      "       False, False, False, False, False]]\n",
      "380/163: dict(zip([1,2,3],['a','b','c']))\n",
      "380/164: l = list(range(10))\n",
      "380/165: l\n",
      "380/166: map(l, lambda x: x ** 2)\n",
      "380/167: map(lambda x: x ** 2, l)\n",
      "380/168: list(map(lambda x: x ** 2, l))\n",
      "380/169: help(str.split)\n",
      "380/170: [palabra + \"_\" for palabra in \"Las_palabras_todas_juntas\".split('_')]\n",
      "380/171: nam = pd.read_clipboard()\n",
      "380/172: nam.columns = columnas\n",
      "380/173: nam.columns = columns\n",
      "380/174: nam.columns = ['pais','casos_totales', 'fallecidos', 'recuperados', 'casos_activos', 'condicion_critica', 'tests_realizados', 'poblacion']\n",
      "380/175: nam\n",
      "380/176: nam.set_index('pais')\n",
      "380/177: pd.concat([nam.set_index('pais'), data])\n",
      "380/178: covid = pd.concat([nam.set_index('pais'), data])\n",
      "380/179: covid.sort_values('casos_totales', ascending=False)\n",
      "380/180: covid = covid.sort_values('casos_totales', ascending=False)\n",
      "380/181: covid\n",
      "380/182: covid.to_csv('america.csv')\n",
      "380/183: covid\n",
      "380/184: covid.query('casos_totales > 10000').to_csv('america.csv')\n",
      "380/185: covid\n",
      "380/186: covid = covid.query('casos_totales > 10000')\n",
      "380/187: covid\n",
      "380/188: covid.rese_index()\n",
      "380/189: covid.reset_index()\n",
      "380/190: covid['subcontinent'] = 'Central America'\n",
      "380/191: covid\n",
      "380/192: covid.loc['Brasil','Colombia','Argentina', 'Peru', 'Chile', 'Bolivia', 'Paraguay','Venezuela', 'Uruguay']\n",
      "380/193: covid.loc[['Brasil','Colombia','Argentina', 'Peru', 'Chile', 'Bolivia', 'Paraguay','Venezuela', 'Uruguay']]\n",
      "380/194: covid.loc[['Brasil','Colombia','Argentina', 'Peru', 'Chile', 'Bolivia', 'Paraguay','Venezuela', 'Uruguay'], 'subcontinent']\n",
      "380/195: covid['subcontinent'] = 'C.A.'\n",
      "380/196: covid.loc[['Brasil','Colombia','Argentina', 'Peru', 'Chile', 'Bolivia', 'Paraguay','Venezuela', 'Uruguay'], 'subcontinent'] = 'S.A.'\n",
      "380/197: covid\n",
      "380/198: covid.loc[['USA','Mexico', 'Canada'], 'subcontinent'] = 'N.A.'\n",
      "380/199: covid.columns\n",
      "380/200:\n",
      "covid[['casos_totales', 'subcontinent', 'fallecidos', 'recuperados', 'casos_activos',\n",
      "       'condicion_critica', 'tests_realizados', 'poblacion']]\n",
      "380/201:\n",
      "covid = covid[['casos_totales', 'subcontinent', 'fallecidos', 'recuperados', 'casos_activos',\n",
      "       'condicion_critica', 'tests_realizados', 'poblacion']]\n",
      "380/202: covid\n",
      "380/203: covid.rename(columns='subcontinet', 'region')\n",
      "380/204: covid.rename(columns=['subcontinet', 'region'])\n",
      "380/205: covid.rename(columns={'subcontinet': 'region'})\n",
      "380/206: covid.rename(columns={'subcontinent': 'region'})\n",
      "380/207: covid = covid.rename(columns={'subcontinent': 'region'})\n",
      "380/208: covid\n",
      "380/209: covid.region.map({'N.A.':'NA', 'C.A.':'CA', 'S.A.':'SA'})\n",
      "380/210: covid.region = covid covid.region.map({'N.A.':'N', 'C.A.':'C', 'S.A.':'S'})\n",
      "380/211: covid.region = covid.region.map({'N.A.':'N', 'C.A.':'C', 'S.A.':'S'})\n",
      "380/212: covid\n",
      "380/213: covid.region.value_counts()\n",
      "380/214: covid#.region.value_counts()\n",
      "380/215: covid.to_csv('america.csv')#.region.value_counts()\n",
      "380/216: covid\n",
      "376/1: df = pd.read_csv('data/america.csv')\n",
      "376/2: import pandas as pd\n",
      "376/3: df = pd.read_csv('data/america.csv')\n",
      "376/4: df = pd.read_csv('../data/america.csv')\n",
      "376/5: df\n",
      "376/6: df.head()\n",
      "376/7: df.set_index('pais')\n",
      "376/8: df.set_index('pais').to_csv('../data/america.csv')\n",
      "376/9: df = pd.read_csv('../data/america.csv')\n",
      "376/10: df.head()\n",
      "376/11: df.set_index('pais')\n",
      "376/12: df.set_index('pais').to_csv('../data/df1.csv')\n",
      "376/13: df = pd.read_csv('../data/df1.csv')\n",
      "376/14: df.head()\n",
      "376/15: df = pd.read_csv('../data/df1.csv').set_index(\"pais\")\n",
      "376/16: df.head()\n",
      "376/17: df.to_csv('../data/df1.csv')\n",
      "376/18: df = pd.read_csv('../data/df1.csv')#.set_index(\"pais\")\n",
      "376/19: df\n",
      "376/20: df = pd.read_csv('../data/df1.csv').set_index(\"pais\")\n",
      "376/21: df\n",
      "376/22: df.to_csv('../data/df1.csv')\n",
      "376/23: df = pd.read_csv('../data/df1.csv')#.set_index(\"pais\")\n",
      "376/24: df\n",
      "376/25: df\n",
      "376/26: df = pd.read_csv('../data/df1.csv')#.set_index(\"pais\")\n",
      "376/27: df\n",
      "376/28: df = pd.read_csv('../data/df1.csv', index='pais')#.set_index(\"pais\")\n",
      "376/29: df = pd.read_csv('../data/df1.csv', index_col='pais')#.set_index(\"pais\")\n",
      "376/30: df = pd.read_csv('../data/america.csv', index_col='pais')#.set_index(\"pais\")\n",
      "376/31: df\n",
      "376/32: df.head()\n",
      "376/33: Para conocer las dimensiones de la tabla utilizamos el atributo ```.shape```\n",
      "376/34:\n",
      "dims = df.shape\n",
      "print('dimensiones:', dims)\n",
      "nrows = df.shape[0]\n",
      "ncols = df.shape[1]\n",
      "print('La tabla tiene {} filas y {} columnas'.format(nrows, ncols))\n",
      "376/35: df.info()\n",
      "376/36: df = pd.read_csv('../data/america.csv', index_col='pais')\n",
      "376/37: df.head()\n",
      "376/38:\n",
      "dims = df.shape\n",
      "print('dimensiones:', dims)\n",
      "nrows = df.shape[0]\n",
      "ncols = df.shape[1]\n",
      "print('La tabla tiene {} filas y {} columnas'.format(nrows, ncols))\n",
      "376/39: df.info()\n",
      "376/40: si al contrario, queremos ver las ultimas filas utilizamos ```.tail()```\n",
      "376/41: df.tail()\n",
      "376/42: df\n",
      "376/43: df\n",
      "376/44: df.sort_values('fallecidos')\n",
      "376/45: df.sort_values('fallecidos', ascending=False)\n",
      "376/46: df.describe()\n",
      "376/47:\n",
      "filas_del_subset = ['Argentina', 'Brasil', 'Paraguay', 'Uruguay']\n",
      "columnas_del_subset = ['casos_totales', 'fallecidos']\n",
      "376/48: subset = df.loc[filas_del_subset, columnas_del_subset]\n",
      "376/49:\n",
      "subset = df.loc[filas_del_subset, columnas_del_subset]\n",
      "subset\n",
      "376/50:\n",
      "subset = df.loc[filas_del_subset, columnas_del_subset]\n",
      "subset.sort_values('casos_totales', ascending=False)\n",
      "376/51: df.loc[ filas_del_subset , : ]\n",
      "376/52: df.loc[ : , columnas_del_subset ] # Todas las filas\n",
      "376/53: df.loc[ filas_del_subset , : ] # Todas las columnas\n",
      "376/54: df.loc[:,:]\n",
      "376/55: Hasta ahora hicimos un subset con varias filas y varias columnas, que pasa si lo quiero hacer sobre solo una fila o solo una columna?\n",
      "376/56: df\n",
      "376/57: df.head()\n",
      "376/58: df.loc['casos_totales', :]\n",
      "376/59: df.loc[:, 'casos_totales']\n",
      "376/60: df.loc[filas_del_subset, 'casos_totales']\n",
      "376/61: df.loc[\"USA\", columnas_del_subset]\n",
      "376/62: df\n",
      "376/63: df.loc[df['casos_totales'] > 100000]\n",
      "376/64: df.loc[df['casos_totales'] > 100000, :]\n",
      "376/65: df['casos_totales'] > 100000\n",
      "376/66: df.casos_totales.plot.bar()\n",
      "376/67: subset\n",
      "376/68: mascara = [True, False, True, True]\n",
      "376/69: subset.loc[mascara, :]\n",
      "376/70: subset.loc[mascara, [True, False]]\n",
      "376/71: subset.loc[mascara, [True, True]]\n",
      "376/72: subset.loc[mascara, [False, True]]\n",
      "376/73: subset.loc[mascara, [False, False]]\n",
      "376/74: subset.loc[mascara, : ]\n",
      "376/75:\n",
      "casos_totales = df['casos_totales']\n",
      "casos_totales\n",
      "376/76:\n",
      "casos_totales = df.casos_totales\n",
      "casos_totales\n",
      "376/77:\n",
      "casos_y_fallecidos = df['casos_totales','fallecidos']\n",
      "casos_y_fallecidos\n",
      "376/78:\n",
      "casos_y_fallecidos = df['casos_totales','fallecidos']]\n",
      "casos_y_fallecidos\n",
      "376/79:\n",
      "casos_y_fallecidos = df[[]'casos_totales','fallecidos']]\n",
      "casos_y_fallecidos\n",
      "376/80:\n",
      "casos_y_fallecidos = df[['casos_totales','fallecidos']]\n",
      "casos_y_fallecidos\n",
      "376/81:\n",
      "casos_y_fallecidos = df[['casos_totales','fallecidos']]\n",
      "casos_y_fallecidos.head()\n",
      "376/82:\n",
      "usa = df.loc['USA']\n",
      "usa\n",
      "376/83: df.head()\n",
      "376/84:\n",
      "mercosur_df = df.loc[mercosur]\n",
      "mercosur_df\n",
      "376/85:\n",
      "mercosur_df = df.loc[filas_del_subset]\n",
      "mercosur_df\n",
      "376/86: Generalmente no le vamos a pasar una lista de booleanos creada por nosotrxs sino una lista o serie de booleanos proveniente de una operación de comparación:\n",
      "376/87: 1e6\n",
      "376/88: 1_000_000\n",
      "376/89: 1_456_000\n",
      "376/90: 1_456_10\n",
      "376/91: 100_000_000_000\n",
      "376/92:\n",
      "casos_totales = df['casos_totales']\n",
      "mas_1M = casos_totales > 1_000_000\n",
      "mas_1M\n",
      "376/93: casos_totales\n",
      "376/94: casos_totales\n",
      "376/95: casos_totales.loc[mas_1M]\n",
      "376/96: casos_totales[mas_1M]\n",
      "376/97: casos_totales[casos_totales > 1_000_000]\n",
      "376/98: casos_totales[casos_totales  % 2 == 0]\n",
      "376/99: casos_totales[casos_totales  % 2 != 0]\n",
      "376/100: casos_totales[casos_totales  % 5 != 0]\n",
      "376/101: casos_totales[casos_totales  % 5 == 0]\n",
      "376/102: df.loc[mas_1M, :]\n",
      "376/103: df.loc[mas_1M]\n",
      "376/104: df[mas_1M]\n",
      "376/105: df[casos_totales  % 5 == 0]\n",
      "376/106: casos_totales  % 5 == 0\n",
      "376/107: df[[True, False]]\n",
      "376/108: df[columnas]\n",
      "376/109: df[columnas_del_subset]\n",
      "376/110: df.loc[mas_1M]\n",
      "376/111: df.loc[mas_1M, ['casos_totales', 'fallecidos']]\n",
      "376/112: df.iloc[:5, :]\n",
      "376/113: df.iloc[3:8, 3:-2]\n",
      "376/114: df.iloc[3:8, 3:-1]\n",
      "376/115: df.iloc[3:8, 3:-2]   # De la cuarta fila a la octava, de la cuarta columna a la antpeneultima\n",
      "376/116: df.iloc[0]\n",
      "376/117:\n",
      "# solo los paises de sudamerica\n",
      "\n",
      "sudamerica = df.loc[df['region'] == 'S', ['fallecidos', 'condicion_critica', 'recuperados']]\n",
      "\n",
      "sudamerica = sudamerica.sort_values('fallecidos', ascending=False)\n",
      "\n",
      "sudamerica\n",
      "376/118:\n",
      "def a_miles(n):\n",
      "    round(n/1000)*1000\n",
      "376/119: a_miles(1231235)\n",
      "376/120:\n",
      "def a_miles(n):\n",
      "    return round(n/1000)*1000\n",
      "376/121: a_miles(1231235)\n",
      "376/122:\n",
      "# solo los paises de sudamerica\n",
      "\n",
      "sudamerica = df.loc[df['region'] == 'S', ['casos_totales','fallecidos', 'condicion_critica', 'recuperados']]\n",
      "\n",
      "sudamerica = sudamerica.sort_values('fallecidos', ascending=False)\n",
      "\n",
      "sudamerica\n",
      "376/123:\n",
      "def a_miles(n):\n",
      "    return round(n/1000)\n",
      "376/124: a_miles(1231235)\n",
      "376/125: sudamerica['miles_casos_totales'] = sudamerica['casos_totales'] / 1000\n",
      "376/126: sudamerica\n",
      "376/127:\n",
      "# solo los paises de sudamerica\n",
      "\n",
      "sudamerica = df.loc[df['region'] == 'S', ['casos_totales','fallecidos', 'condicion_critica', 'recuperados']]\n",
      "\n",
      "sudamerica = sudamerica.sort_values('fallecidos', ascending=False)\n",
      "\n",
      "sudamerica\n",
      "376/128: df.query('casos_totales > 1_000_000')\n",
      "376/129: sudamerica_1m = sudamerica.query('casos_totales > 1_000_000')\n",
      "376/130: sudamerica_1m\n",
      "376/131: sudamerica = sudamerica.query('casos_totales > 1_000_000')\n",
      "376/132: sudamerica\n",
      "376/133: sudamerica['miles_casos_totales'] = sudamerica['casos_totales'] / 1_000_000\n",
      "376/134: sudamerica\n",
      "376/135: sudamerica.round()\n",
      "376/136: sudamerica['miles_casos_totales'].round()\n",
      "376/137: sudamerica['miles_casos_totales'].round(1)\n",
      "376/138: sudamerica['miles_casos_totales'].round(2)\n",
      "376/139: sudamerica['miles_casos_totales'] = sudamerica['miles_casos_totales'].round(2)\n",
      "376/140: sudamerica\n",
      "376/141: sudamerica['millon_casos_totales'] = sudamerica['millon_casos_totales'].round(2)\n",
      "376/142: sudamerica['millon_casos_totales'] = sudamerica['casos_totales'] / 1_000_000\n",
      "376/143: sudamerica['millon_casos_totales'] = sudamerica['millon_casos_totales'].round(2)\n",
      "376/144: sudamerica.loc[:, 'millon_de_casos'] = sudamerica['casos_totales'] / 1_000_000\n",
      "376/145: sudamerica\n",
      "376/146: sudamerica = sudamerica.query('casos_totales > 1_000_000')\n",
      "376/147: sudamerica\n",
      "376/148:\n",
      "# solo los paises de sudamerica\n",
      "\n",
      "sudamerica = df.loc[df['region'] == 'S', ['casos_totales','fallecidos', 'condicion_critica', 'recuperados']]\n",
      "\n",
      "sudamerica = sudamerica.sort_values('fallecidos', ascending=False)\n",
      "\n",
      "sudamerica\n",
      "376/149: sudamerica = sudamerica.query('casos_totales > 1_000_000')\n",
      "376/150: sudamerica\n",
      "376/151: sudamerica['millon_casos_totales'] = sudamerica['casos_totales'] / 1_000_000\n",
      "376/152: sudamerica['millon_casos_totales'] = sudamerica['millon_casos_totales'].round(2)\n",
      "376/153: sudamerica\n",
      "376/154:\n",
      "# solo los paises de sudamerica\n",
      "\n",
      "sudamerica = df.loc[df['region'] == 'S', ['casos_totales','fallecidos', 'condicion_critica', 'recuperados']]\n",
      "\n",
      "sudamerica = sudamerica.sort_values('fallecidos', ascending=False)\n",
      "\n",
      "sudamerica\n",
      "376/155: sudamerica = sudamerica.query('casos_totales > 1_000_000')\n",
      "376/156: sudamerica\n",
      "376/157: sudamerica['millon_de_casos'] = sudamerica['casos_totales'] / 1_000_000\n",
      "376/158: sudamerica['millon_de_casos'] = sudamerica['millon_casos_totales'].round(2)\n",
      "376/159: sudamerica['millon_de_casos'] = sudamerica['millon_de_casos'].round(2)\n",
      "376/160: sudamerica\n",
      "376/161: sudamerica['casos_totales']\n",
      "376/162: sudamerica['casos_totales'] ** 2\n",
      "376/163: sudamerica['casos_totales'] % 10\n",
      "376/164: sudamerica['casos_totales'] * 2 7 3\n",
      "376/165: sudamerica['casos_totales'] * 2 / 3\n",
      "376/166: sudamerica['millon_de_casos']\n",
      "376/167: sudamerica['casos_totales'] / 1_000_000\n",
      "376/168: sudamerica['fallecidos'] / sudamerica['casos_totales']\n",
      "376/169: sudamerica['fallecidos'] / sudamerica['casos_totales'] * 1000\n",
      "376/170: sudamerica['fallecidos'] / sudamerica['casos_totales'] * 100\n",
      "376/171: sudamerica['letalidad'] = sudamerica['fallecidos'] / sudamerica['casos_totales'] * 100\n",
      "376/172: sudamerica\n",
      "376/173: df\n",
      "376/174: df['poblacion']\n",
      "376/175: df['poblacion'].to_dict()\n",
      "376/176: poblacion = df['poblacion'].to_dict()\n",
      "376/177: poblacion\n",
      "376/178: sudamerica['tasa_recuperados'] = sudamerica['recuperados'] / sudamerica['casos_totales']\n",
      "376/179: sudamerica\n",
      "376/180:\n",
      "def porciento(numero):\n",
      "    redondeo = round(numero * 100)\n",
      "    return '{} %'.format(redondeo)\n",
      "376/181: sudamerica.tasa_recuperados.map(porciento) # la funcion no lleva los parentesis\n",
      "376/182:\n",
      "def porciento(numero):\n",
      "    redondeo = round(numero * 100)\n",
      "    return '{}%'.format(redondeo)\n",
      "376/183: sudamerica.tasa_recuperados.map(porciento) # la funcion no lleva los parentesis\n",
      "376/184: sudamerica['porcentaje_recuperados'] = sudamerica.tasa_recuperados.map(porciento) # la funcion no lleva los parentesis\n",
      "376/185: sudamerica\n",
      "376/186: sudamerica.info()\n",
      "376/187: sudamerica#.info()\n",
      "376/188: sudamerica.index\n",
      "376/189: sudamerica.index\n",
      "376/190: list(sudamerica.index)\n",
      "376/191: sudamerica.index\n",
      "376/192: df.poblacion.to_dict()\n",
      "376/193: {'USA': 332299143, 'Brasil': 213568068, 'Argentina': 45472346, 'Canada': 37962879, 'Peru': 33277411,'Colombia': 51244298}\n",
      "376/194: poblaciones = {'USA': 332299143, 'Brasil': 213568068, 'Argentina': 45472346, 'Canada': 37962879, 'Peru': 33277411,'Colombia': 51244298}\n",
      "376/195: sudamerica\n",
      "376/196: sudamerica.index\n",
      "376/197: sudamerica.index.map(poblaciones)\n",
      "376/198: sudamerica['poblacion'] = sudamerica.index.map(poblaciones)\n",
      "376/199: sudamerica\n",
      "376/200: sudamerica['casos_por_millon'] = sudamerica['casos_totales'] / sudamerica['millon_de_casos']\n",
      "376/201: sudamerica['casos_por_millon'] = sudamerica['casos_totales'] / sudamerica['poblacion'] * 1_000_000\n",
      "376/202: sudamerica['fallecidos_por_millon'] = sudamerica.fallecidos / sudamerica.poblacion * 100_000\n",
      "376/203: sudamerica\n",
      "376/204: sudamerica.fallecidos_por_millon.plot('bar')\n",
      "376/205: sudamerica.fallecidos_por_millon.plot(kind='bar')\n",
      "376/206:\n",
      "import pandas as pd\n",
      "pd.options.mode.chained_assignment = None  # default='warn'\n",
      "376/207: pd.options.mode.chained_assignment = None  # default='warn'\n",
      "376/208:\n",
      "# solo los paises de sudamerica\n",
      "\n",
      "sudamerica = df.loc[df['region'] == 'S', ['casos_totales','fallecidos', 'condicion_critica', 'recuperados']]\n",
      "\n",
      "sudamerica = sudamerica.sort_values('fallecidos', ascending=False)\n",
      "\n",
      "sudamerica\n",
      "376/209: sudamerica = sudamerica.query('casos_totales > 1_000_000')\n",
      "376/210: sudamerica\n",
      "376/211: pd.options.mode.chained_assignment = None  # default='warn'\n",
      "376/212: sudamerica['millon_de_casos'] = sudamerica['casos_totales'] / 1_000_000\n",
      "376/213: sudamerica['millon_de_casos'] = sudamerica['millon_de_casos'].round(2)\n",
      "376/214: sudamerica['letalidad'] = sudamerica['fallecidos'] / sudamerica['casos_totales'] * 100\n",
      "376/215: sudamerica['tasa_recuperados'] = sudamerica['recuperados'] / sudamerica['casos_totales']\n",
      "376/216:\n",
      "# solo los paises de sudamerica\n",
      "\n",
      "sudamerica = df.loc[df['region'] == 'S', ['casos_totales','fallecidos', 'condicion_critica', 'recuperados']]\n",
      "\n",
      "sudamerica = sudamerica.sort_values('fallecidos', ascending=False)\n",
      "\n",
      "sudamerica\n",
      "376/217: sudamerica = sudamerica.query('casos_totales > 1_000_000')\n",
      "376/218: sudamerica\n",
      "376/219: pd.options.mode.chained_assignment = None  # default='warn'\n",
      "376/220:\n",
      "sudamerica['millon_de_casos'] = sudamerica['casos_totales'] / 1_000_000\n",
      "sudamerica\n",
      "376/221:\n",
      "sudamerica['millon_de_casos'] = sudamerica['millon_de_casos'].round(2)\n",
      "sudamerica\n",
      "376/222:\n",
      "sudamerica['letalidad'] = sudamerica['fallecidos'] / sudamerica['casos_totales'] * 100\n",
      "sudamerica\n",
      "376/223:\n",
      "sudamerica['tasa_recuperados'] = sudamerica['recuperados'] / sudamerica['casos_totales']\n",
      "sudamerica\n",
      "376/224: sudamerica['millon_de_casos'] = sudamerica['casos_totales'] / 1_000_000\n",
      "376/225: sudamerica['millon_de_casos'] = sudamerica['millon_de_casos'].round(2)\n",
      "376/226: sudamerica['letalidad'] = sudamerica['fallecidos'] / sudamerica['casos_totales'] * 100\n",
      "376/227: sudamerica['tasa_recuperados'] = sudamerica['recuperados'] / sudamerica['casos_totales']\n",
      "376/228: Podemos también aplicar los métodos de una serie para cambiarla sus valores o para agregarla en otra columna\n",
      "376/229: También nos permite realizar operaciones matemáticas combinando series y números\n",
      "376/230: sudamerica['letalidad'] = sudamerica['fallecidos'] / sudamerica['casos_totales'] * 100\n",
      "376/231: sudamerica['tasa_recuperados'] = sudamerica['recuperados'] / sudamerica['casos_totales']\n",
      "376/232: porciento(0,30)\n",
      "376/233: porciento(0.30)\n",
      "376/234:\n",
      "for numero in [0,0.2, 0.75, 1]:\n",
      "    print(porciento(numero))\n",
      "376/235: sudamerica['porcentaje_recuperados'] = sudamerica.tasa_recuperados.map(porciento) # la funcion no lleva los parentesis\n",
      "376/236: sudamerica#.info()\n",
      "376/237: sudamerica.index\n",
      "376/238: poblaciones = {'USA': 332299143, 'Brasil': 213568068, 'Argentina': 45472346, 'Canada': 37962879, 'Peru': 33277411,'Colombia': 51244298}\n",
      "376/239: sudamerica['poblacion'] = sudamerica.index.map(poblaciones)\n",
      "376/240: sudamerica\n",
      "376/241: sudamerica['casos_por_millon'] = sudamerica['casos_totales'] / sudamerica['poblacion'] * 1_000_000\n",
      "376/242: sudamerica['fallecidos_por_millon'] = sudamerica.fallecidos / sudamerica.poblacion * 100_000\n",
      "376/243: sudamerica\n",
      "376/244: sudamerica['fallecidos_por_cienmil'] = sudamerica.fallecidos / sudamerica.poblacion * 100_000\n",
      "376/245: sudamerica\n",
      "376/246:\n",
      "for fila in df.iterrows():\n",
      "    print(fila)\n",
      "376/247: pi = 3,14\n",
      "376/248:\n",
      "pi = 3,14\n",
      "pi\n",
      "376/249:\n",
      "n1, n2 = 3,14\n",
      "print(n1)\n",
      "print(n2)\n",
      "376/250:\n",
      "for indice, fila in df.iterrows():\n",
      "    print(fila)\n",
      "376/251:\n",
      "for indice, fila in df.iterrows():\n",
      "    print('Serie:')\n",
      "    print(fila)\n",
      "    print()\n",
      "376/252:\n",
      "for indice, fila in df.iterrows():\n",
      "    print('Pais:', indice)\n",
      "    print(fila)\n",
      "    print()\n",
      "376/253:\n",
      "for pais, fila in df.iterrows():\n",
      "    print('Pais:', pais)\n",
      "    print(fila)\n",
      "    print()\n",
      "376/254:\n",
      "for pais, fila in df.iterrows():\n",
      "    #print('Pais:', pais)\n",
      "    #print(fila)\n",
      "    print(type(fila))\n",
      "    print()\n",
      "376/255:\n",
      "for pais, fila in df.iterrows():\n",
      "    print('Pais:', pais)\n",
      "    print(fila[\"casos_totales\"])\n",
      "    \n",
      "    print()\n",
      "376/256:\n",
      "for pais, fila in df.iterrows():\n",
      "    print(pais)\n",
      "    print('Casos:', fila[\"casos_totales\"])\n",
      "    print('Fallecidos:', fila['fallecidos'])\n",
      "    \n",
      "    print()\n",
      "376/257:\n",
      "casos = {}\n",
      "for pais, fila in df.iterrows():\n",
      "    print(pais)\n",
      "    print('Casos:', fila[\"casos_totales\"])\n",
      "    print('Fallecidos:', fila['fallecidos'])\n",
      "    casos[pais] = fila['casos_totales']\n",
      "    \n",
      "    print()\n",
      "376/258:\n",
      "casos = {}\n",
      "for pais, fila in df.iterrows():\n",
      "    #print(pais)\n",
      "    #print('Casos:', fila[\"casos_totales\"])\n",
      "    print('Fallecidos:', fila['fallecidos'])\n",
      "    casos[pais] = fila['casos_totales']\n",
      "    \n",
      "    #print()\n",
      "376/259:\n",
      "casos = {}\n",
      "for pais, fila in df.iterrows():\n",
      "    #print(pais)\n",
      "    #print('Casos:', fila[\"casos_totales\"])\n",
      "    #print('Fallecidos:', fila['fallecidos'])\n",
      "    casos[pais] = fila['casos_totales']\n",
      "    \n",
      "    #print()\n",
      "376/260: casos\n",
      "376/261: df\n",
      "376/262: sudamerica\n",
      "376/263: df\n",
      "376/264:\n",
      "for pais, fila in df.iterrows():\n",
      "    print('Paises de Norteamerica:')\n",
      "    if fila['region'] == 'N':\n",
      "        print('En', pais, 'existen',fila['casos_activos'],'de casos actualmente activos')\n",
      "376/265:\n",
      "print('Paises de Norteamerica:')\n",
      "for pais, fila in df.iterrows():\n",
      "    if fila['region'] == 'N':\n",
      "        print('En', pais, 'existen',fila['casos_activos'],'de casos actualmente activos')\n",
      "376/266:\n",
      "print('Paises de Norteamerica:')\n",
      "for pais, fila in df.iterrows():\n",
      "    if fila['region'] == 'N':\n",
      "        print('En', pais, 'existen',fila['casos_activos'],'casos actualmente activos')\n",
      "376/267:\n",
      "casos = {}\n",
      "for fila in df.iterrows():\n",
      "    print(fila)\n",
      "376/268: sudamerica['casos_totales'] / 1_000_000 # Podemos separar los numeros con _ para poder leerlos mejor, Python va a ignorar los _\n",
      "376/269: df.sort_index()\n",
      "376/270: df = df.sort_index()\n",
      "376/271: df\n",
      "376/272: df.sort_values('region')\n",
      "376/273: df.sort_values('region', ascending=False)\n",
      "376/274: df.sort_values(['region','casos_totales'], ascending=False)\n",
      "376/275: list('abcd')\n",
      "376/276: list('abcdef')\n",
      "376/277: pd.Series(list('abcdef'))\n",
      "376/278: pd.Series(list('abcdef'))[2:4]\n",
      "376/279: pd.Series(list('abcdef'))\n",
      "376/280:\n",
      "letras = list('abcdef')\n",
      "pd.Series(letras, names=letras)\n",
      "376/281:\n",
      "letras = list('abcdef')\n",
      "pd.Series(letras, index=letras)\n",
      "376/282:\n",
      "letras = list('abcdef')\n",
      "pd.Series(letras.upper(), index=letras)\n",
      "376/283:\n",
      "letras = list('abcdef')\n",
      "pd.Series([i.upper() for i in letras], index=letras)\n",
      "376/284:\n",
      "letras = list('abcdef')\n",
      "pd.Series([i.upper() for i in letras], index=letras)[\"b\"]\n",
      "376/285: sudamerica\n",
      "376/286: sudamerica.index.tolist()\n",
      "376/287: paises = df.index.tolist()\n",
      "376/288: casos_covid = df.casos_totales.tolist()\n",
      "376/289: casos_covid\n",
      "376/290: *casos_covid\n",
      "376/291: *casos_covid,\n",
      "376/292:  df.casos_totales.tolist()\n",
      "376/293: casos_covid = [2118676, 12320, 250557, 10647845, 872747, 832512, 2259599, 205514, 51587, 286725, 60491, 10149, 175411, 16627, 12531, 171758, 23838, 2097194, 342019, 161530, 1338297, 240201, 29370705, 59171, 139934]\n",
      "376/294: paises\n",
      "376/295: paises = ['Argentina', 'Belize', 'Bolivia', 'Brasil', 'Canada', 'Chile', 'Colombia', 'Costa Rica', 'Cuba', 'Ecuador', 'El Salvador', 'Guadalupe', 'Guatemala', 'Guyana Francesa', 'Haiti', 'Honduras', 'Jamaica', 'Mexico', 'Panama', 'Paraguay', 'Peru', 'Republica Dominicana', 'USA', 'Uruguay', 'Venezuela']\n",
      "376/296:\n",
      "casos_covid = [2118676, 12320, 250557, 10647845, 872747, 832512, 2259599, 205514, 51587, 286725, 60491, 10149, 175411,\n",
      "               16627, 12531, 171758, 23838, 2097194, 342019, 161530, 1338297, 240201, 29370705, 59171, 139934]\n",
      "\n",
      "paises = ['Argentina', 'Belize', 'Bolivia', 'Brasil', 'Canada', 'Chile', 'Colombia', 'Costa Rica', 'Cuba', 'Ecuador',\n",
      "          'El Salvador', 'Guadalupe', 'Guatemala', 'Guyana Francesa', 'Haiti', 'Honduras', 'Jamaica', 'Mexico', 'Panama',\n",
      "          'Paraguay', 'Peru', 'Republica Dominicana', 'USA', 'Uruguay', 'Venezuela']\n",
      "376/297:\n",
      "paises = ['Argentina', 'Belize', 'Bolivia', 'Brasil', 'Canada', 'Chile', 'Colombia', 'Costa Rica', 'Cuba', 'Ecuador',\n",
      "          'El Salvador', 'Guadalupe', 'Guatemala', 'Guyana Francesa', 'Haiti', 'Honduras', 'Jamaica', 'Mexico', 'Panama',\n",
      "          'Paraguay', 'Peru', 'Republica Dominicana', 'USA', 'Uruguay', 'Venezuela']\n",
      "\n",
      "casos_covid = [2118676, 12320, 250557, 10647845, 872747, 832512, 2259599, 205514, 51587, 286725, 60491, 10149, 175411,\n",
      "               16627, 12531, 171758, 23838, 2097194, 342019, 161530, 1338297, 240201, 29370705, 59171, 139934]\n",
      "376/298:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    print(pais, ncasos, sep='\\t')\n",
      "376/299:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    print(pais, ncasos, sep='\\t\\t')\n",
      "376/300:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    print(pais, ncasos, sep='\\t\\t\\t')\n",
      "376/301:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    print(\"{}\\t\\t{}\".format(pais, ncasos))\n",
      "376/302:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    print(\"{}\\t{}\".expandtabs().format(pais, ncasos))\n",
      "376/303:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    print(\"{}{}\".expandtabs().format(pais, ncasos))\n",
      "376/304:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    print(\"{}{}\".expandtabs(3).format(pais, ncasos))\n",
      "376/305:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    print(\"{}\\t{}\".expandtabs(3).format(pais, ncasos))\n",
      "376/306:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    if ncasos > 10000:\n",
      "        print(pais, ncasos)\n",
      "376/307:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    if ncasos > 100000:\n",
      "        print(pais, ncasos)\n",
      "376/308:\n",
      "for pais,ncasos in zip(paises, casos_covid):\n",
      "    if ncasos > 1000000:\n",
      "        print(pais, ncasos)\n",
      "376/309:\n",
      "casos_por_pais = dict(zip)\n",
      "casos_por_pais\n",
      "376/310:\n",
      "casos_por_pais = dict(zip(paises, casos_covid))\n",
      "casos_por_pais\n",
      "376/311:\n",
      "casos = dict(zip(paises, casos_covid))\n",
      "casos\n",
      "376/312:\n",
      "for pais in casos:\n",
      "    if casos[pais] > 1000000:\n",
      "        print(pais, casos)\n",
      "376/313:\n",
      "for pais in casos:\n",
      "    if casos[pais] > 1000000:\n",
      "        print(pais, casos[pais])\n",
      "376/314:\n",
      "casos = dict(zip(paises, casos_covid)) # Convierto a un diccionario, las claves son la primer lista y los valores la segunda\n",
      "\n",
      "for pais in casos:\n",
      "    if casos[pais] > 1000000:\n",
      "        print(pais, casos[pais])\n",
      "376/315: casos\n",
      "376/316:\n",
      "serie = pd.Series(casos)\n",
      "serie\n",
      "376/317:\n",
      "serie = pd.Series(casos_covid)\n",
      "serie\n",
      "376/318: serie = pd.Series(casos_covid)\n",
      "376/319: serie[1]\n",
      "376/320: serie[0]\n",
      "376/321: serie\n",
      "376/322: serie ** 2\n",
      "376/323: serie + serie\n",
      "376/324: serie + serie * 5\n",
      "376/325: serie * 100\n",
      "376/326: serie * 10000\n",
      "376/327: serie\n",
      "376/328: serie ** 2 # Me devuele otra serie, con el resultado de elevar cada numero de la serie al cuadrado\n",
      "376/329:\n",
      "Si recuerdan el ejercicio de energia potencial =\n",
      "\n",
      "#### Cargar tablas\n",
      "376/330: serie ** 2 # Me devuele otra serie, con el resultado de elevar cada numero de la serie al cuadrado\n",
      "376/331:\n",
      "# m * g * h\n",
      "5 * 9.8 * serie\n",
      "376/332:\n",
      "amino = [\n",
      "        'I', 'L', 'K', 'M', 'F',\n",
      "        'T', 'W', 'V', 'R', 'H',\n",
      "        'A', 'N', 'D', 'C', 'E',\n",
      "        'Q', 'G', 'P', 'S', 'Y'\n",
      "        ]\n",
      "pm = [\n",
      "        131.1736, 131.1736, 146.1882, 149.2124, 165.1900,\n",
      "        119.1197, 204.2262, 117.1469, 174.2017, 155.1552,\n",
      "        89.0935,  132.1184, 133.1032, 121.1590, 147.1299,\n",
      "        146.1451, 75.0669,  115.1310, 105.0930, 181.1894\n",
      "    ]\n",
      "376/333:\n",
      "amino = [\n",
      "        'I', 'L', 'K', 'M', 'F',\n",
      "        'T', 'W', 'V', 'R', 'H',\n",
      "        'A', 'N', 'D', 'C', 'E',\n",
      "        'Q', 'G', 'P', 'S', 'Y'\n",
      "        ]\n",
      "pm = [\n",
      "        131.1736, 131.1736, 146.1882, 149.2124, 165.1900,\n",
      "        119.1197, 204.2262, 117.1469, 174.2017, 155.1552,\n",
      "        89.0935,  132.1184, 133.1032, 121.1590, 147.1299,\n",
      "        146.1451, 75.0669,  115.1310, 105.0930, 181.1894\n",
      "    ]\n",
      "376/334:\n",
      "for aa,peso in zip(amino, pm):\n",
      "    if pm > 30:\n",
      "        print(a, pm)\n",
      "376/335:\n",
      "for aa,peso in zip(amino, pm):\n",
      "    if peso > 30:\n",
      "        print(a, pm)\n",
      "376/336:\n",
      "for aa,peso in zip(amino, pm):\n",
      "    if peso > 30:\n",
      "        print(a, peso)\n",
      "376/337:\n",
      "for aa,peso in zip(amino, pm):\n",
      "    if peso > 30:\n",
      "        print(aa, peso)\n",
      "376/338:\n",
      "for aa,peso in zip(amino, pm):\n",
      "    if peso > 130:\n",
      "        print(aa, peso)\n",
      "376/339:\n",
      "for aa,peso in zip(amino, pm):\n",
      "    if peso > 150:\n",
      "        print(aa, peso)\n",
      "376/340:\n",
      "pesos_moleculares = dict(zip(amino, pm)) # Convierto a un diccionario, las claves son la primer lista y los valores la segunda\n",
      "\n",
      "for aa in pesos_moleculares:\n",
      "    if pesos_moleculares[aa] > 130:\n",
      "        print(aa, pesos_moleculares[a])\n",
      "376/341:\n",
      "pesos_moleculares = dict(zip(amino, pm)) # Convierto a un diccionario, las claves son la primer lista y los valores la segunda\n",
      "\n",
      "for aa in pesos_moleculares:\n",
      "    if pesos_moleculares[aa] > 130:\n",
      "        print(aa, pesos_moleculares[aa])\n",
      "376/342:\n",
      "pesos_moleculares = dict(zip(amino, pm)) # Convierto a un diccionario, las claves son la primer lista y los valores la segunda\n",
      "\n",
      "for aa in pesos_moleculares:\n",
      "    if pesos_moleculares[aa] > 150:\n",
      "        print(aa, pesos_moleculares[aa])\n",
      "376/343: serie = pd.Series(pm)\n",
      "376/344: serie\n",
      "376/345: serie[3]\n",
      "376/346: serie#[3]\n",
      "376/347: serie[3:10]\n",
      "376/348: serie\n",
      "376/349: serie ** 2\n",
      "376/350: serie\n",
      "376/351: serie  / 10\n",
      "376/352: serie  / 100\n",
      "376/353: serie  * 100\n",
      "376/354: serie  * 100 - 90\n",
      "376/355: serie > 150\n",
      "376/356: serie\n",
      "376/357: serie[serie > 150]\n",
      "376/358: serie = pd.Series(pm, index=aa)\n",
      "376/359: serie = pd.Series(pm, index=amino)\n",
      "376/360: serie[serie > 150]\n",
      "376/361:\n",
      "serie = pd.Series(pm, index=amino)\n",
      "serie\n",
      "376/362: serie[serie > 150]\n",
      "376/363: serie.sum()\n",
      "376/364: serie.median()\n",
      "376/365: serie.mean()\n",
      "376/366: serie.std()\n",
      "376/367: (serie - serie.mean()) / serie.std()\n",
      "376/368: centrada = (serie - serie.mean()) / serie.std()\n",
      "376/369: centrada[centrada > 0]\n",
      "376/370:\n",
      "serie = pd.Series(pm)\n",
      "serie\n",
      "376/371: serie[serie > 150]\n",
      "376/372:\n",
      "serie = pd.Series(pm, index=amino)\n",
      "serie\n",
      "376/373: serie[serie > 150]\n",
      "376/374:\n",
      "serie = pd.Series(pm) # A partir de la lista de pesos moleculares construimos una Serie\n",
      "serie\n",
      "376/375: serie ** 2\n",
      "376/376: serie ** 2\n",
      "376/377: serie > 150\n",
      "376/378:\n",
      "# m * g * h\n",
      "5 * 9.8 * serie\n",
      "376/379:\n",
      "h = 50\n",
      "g = 9.8\n",
      "serie * g * h\n",
      "376/380:\n",
      "h = 50\n",
      "g = 9.8\n",
      "print('EP de cada aminoacido a {} m y g = 9.8')\n",
      "serie * g * h\n",
      "376/381:\n",
      "h = 50\n",
      "g = 9.8\n",
      "print('EP de cada aminoacido a {} m y g ={}'.format(h,g))\n",
      "serie * g * h\n",
      "376/382:\n",
      "h = 50\n",
      "g = 9.8\n",
      "print('EP de cada aminoacido a {} m y g de {}'.format(h,g))\n",
      "serie * g * h\n",
      "376/383:\n",
      "h = 50\n",
      "g = 9.8\n",
      "print('EP de cada aminoacido a {}m y g de {}'.format(h,g))\n",
      "serie * g * h\n",
      "376/384:\n",
      "h = 50\n",
      "g = 9.8\n",
      "print('\"EP\" de cada aminoacido a {}m y g de {}'.format(h,g))\n",
      "serie * g * h\n",
      "376/385:\n",
      "h = 50\n",
      "g = 9.8\n",
      "print('\"EP\" a {}m y g de {}'.format(h,g))\n",
      "serie * g * h\n",
      " 133: %hisotry\n",
      " 134: %history\n",
      " 135: %history -g\n"
     ]
    }
   ],
   "source": [
    "%history -g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
